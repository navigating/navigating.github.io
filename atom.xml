<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[On The Open Way]]></title>
  <subtitle><![CDATA[自信人生二百年，会当水击三千里！]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://navigating.github.io//"/>
  <updated>2015-08-01T01:47:08.711Z</updated>
  <id>http://navigating.github.io//</id>
  
  <author>
    <name><![CDATA[Steven Xu]]></name>
    <email><![CDATA[xxx@qq.com]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[BIGDATA Update 201507]]></title>
    <link href="http://navigating.github.io/2015/BIGDATA-Update-201507/"/>
    <id>http://navigating.github.io/2015/BIGDATA-Update-201507/</id>
    <published>2015-07-30T08:22:01.000Z</published>
    <updated>2015-08-01T01:47:08.711Z</updated>
    <content type="html"><![CDATA[<p>Hortonworks<br>HDP 2.3发布：<br>HDP 2.3新增加组件Apache Atlas、Apache Calcite<br><a href="http://hortonworks.com/blog/available-now-hdp-2-3/" target="_blank" rel="external">http://hortonworks.com/blog/available-now-hdp-2-3/</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-2/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-2/</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/</a><br>Spark 1.2开始支持ORC(Columnar Formats)<br><a href="http://hortonworks.com/blog/bringing-orc-support-into-apache-spark/" target="_blank" rel="external">http://hortonworks.com/blog/bringing-orc-support-into-apache-spark/</a><br>Spark in HDInsight新特性一览<br><a href="http://hortonworks.com/blog/spark-in-hdinsight/" target="_blank" rel="external">http://hortonworks.com/blog/spark-in-hdinsight/</a> </p>
<p>Cloudera<br>HBase 1.0 开始支持Thrift客户端鉴权<br><a href="http://blog.cloudera.com/blog/2015/07/thrift-client-authentication-support-in-apache-hbase-1-0/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/thrift-client-authentication-support-in-apache-hbase-1-0/</a><br>Pig on MR优化<br><a href="http://blog.cloudera.com/blog/2015/07/how-to-tune-mapreduce-parallelism-in-apache-pig-jobs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/how-to-tune-mapreduce-parallelism-in-apache-pig-jobs/</a><br>Apache Zeppelin on CDH<br><a href="http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/</a><br>大数据欺诈检测架构<br><a href="http://blog.cloudera.com/blog/2015/07/designing-fraud-detection-architecture-that-works-like-your-brain-does/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/designing-fraud-detection-architecture-that-works-like-your-brain-does/</a> </p>
<p>MapR<br>YARN资源管理实践<br><a href="https://www.mapr.com/blog/best-practices-yarn-resource-management" target="_blank" rel="external">https://www.mapr.com/blog/best-practices-yarn-resource-management</a><br>Hive 1.0对Transaction的支持<br><a href="https://www.mapr.com/blog/hive-transaction-feature-hive-10" target="_blank" rel="external">https://www.mapr.com/blog/hive-transaction-feature-hive-10</a> </p>
<p>Databricks<br>Spark Streaming执行模型<br><a href="https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html</a><br>Spark 1.4 MLP新特性<br><a href="https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html</a><br>从Spark 1.2开始支持ORC<br><a href="https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html</a><br>从Spark 1.4开始支持窗口函数<br><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br>从Spark 1.4开始新的Web UI<br><a href="https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html</a> </p>
<p>Phoenix对join的支持，TPC in Apache Phoenix<br><a href="https://blogs.apache.org/phoenix/entry/tpc_in_apache_phoenix" target="_blank" rel="external">https://blogs.apache.org/phoenix/entry/tpc_in_apache_phoenix</a> </p>
<p>Cassandra<br><a href="http://cassandra.apache.org/" target="_blank" rel="external">http://cassandra.apache.org/</a> </p>
<p>mongoDB<br><a href="https://www.mongodb.org/" target="_blank" rel="external">https://www.mongodb.org/</a> </p>
<p>Confluent<br>基于Kafka的实时流处理<br><a href="http://www.confluent.io/" target="_blank" rel="external">http://www.confluent.io/</a><br>大数据生态系统之Kafka价值<br><a href="http://www.confluent.io/blog/the-value-of-apache-kafka-in-big-data-ecosystem/" target="_blank" rel="external">http://www.confluent.io/blog/the-value-of-apache-kafka-in-big-data-ecosystem/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hortonworks<br>HDP 2.3发布：<br>HDP 2.3新增加组件Apache Atlas、Apache Calcite<br><a href="http://hortonworks.com/blog/available-now-hdp-2-3/" targ]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Cassandra" scheme="http://navigating.github.io/tags/Cassandra/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="mongoDB" scheme="http://navigating.github.io/tags/mongoDB/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用Hexo搭建Github静态博客]]></title>
    <link href="http://navigating.github.io/2015/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BAGithub%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2/"/>
    <id>http://navigating.github.io/2015/使用Hexo搭建Github静态博客/</id>
    <published>2015-07-28T09:20:22.000Z</published>
    <updated>2015-08-01T01:33:22.223Z</updated>
    <content type="html"><![CDATA[<p>环境：</p>
<pre><code><span class="bullet">1. </span>Windows XP
<span class="bullet">2. </span>Git
</code></pre><p>步骤：</p>
<pre><code><span class="bullet">1. </span>安装Node.js
<span class="bullet">2. </span>安装Hexo
<span class="bullet">3. </span>创建博客(初始化Hexo)
<span class="bullet">4. </span>创建文章本地调试
<span class="bullet">5. </span>配置Github
<span class="bullet">6. </span>远程发布
<span class="bullet">7. </span>支持sitemap和feed
<span class="bullet">8. </span>支持百度统计
<span class="bullet">9. </span>支持图片
<span class="bullet">10. </span>参考资源
</code></pre><h2 id="安装Node-js">安装Node.js</h2><p>下载并安装，<a href="https://nodejs.org/" target="_blank" rel="external">https://nodejs.org/</a></p>
<h2 id="安装Hexo">安装Hexo</h2><p>npm install -g hexo<br>D:\git\navigating.github.io&gt;npm install -g hexo</p>
<pre><code>npm WARN optional dep failed, continuing fsevents<span class="variable">@0</span>.3.6
npm WARN optional dep failed, continuing fsevents<span class="variable">@0</span>.3.6
-


&gt; dtrace-provider<span class="variable">@0</span>.5.0 install C:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo\node_modules\bunyan\node_modules\dtrace-provider
&gt; node scripts/install.js

C:\Users\stevenxu\AppData\Roaming\npm\hexo -&gt; C:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo\bin\hexo
hexo<span class="variable">@3</span>.1.1 C:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo
├── pretty-hrtime<span class="variable">@1</span>.0.0
├── hexo-front-matter<span class="variable">@0</span>.2.2
├── abbrev<span class="variable">@1</span>.0.7
├── titlecase<span class="variable">@1</span>.0.2
├── archy<span class="variable">@1</span>.0.0
├── <span class="keyword">text</span>-table<span class="variable">@0</span>.2.0
├── tildify<span class="variable">@1</span>.1.0 (os-homedir<span class="variable">@1</span>.0.1)
├── <span class="keyword">strip</span>-indent<span class="variable">@1</span>.0.1 (get-stdin<span class="variable">@4</span>.0.1)
├── hexo-i18n<span class="variable">@0</span>.2.1 (sprintf-js<span class="variable">@1</span>.0.3)
├── chalk<span class="variable">@1</span>.1.0 (escape-<span class="keyword">string</span>-regexp<span class="variable">@1</span>.0.3, supports-<span class="keyword">color</span><span class="variable">@2</span>.0.0, ansi-styles<span class="variable">@2</span>.1.0, <span class="keyword">strip</span>-ansi<span class="variable">@3</span>.0.0, has-ansi<span class="variable">@2</span>.0.0)
├── bluebird<span class="variable">@2</span>.9.34
├── minimatch<span class="variable">@2</span>.0.10 (brace-expansion<span class="variable">@1</span>.1.0)
├── through2<span class="variable">@1</span>.1.1 (xtend<span class="variable">@4</span>.0.0, readable-stream<span class="variable">@1</span>.1.13)
├── swig-extras<span class="variable">@0</span>.0.1 (markdown<span class="variable">@0</span>.5.0)
├── hexo-fs<span class="variable">@0</span>.1.3 (escape-<span class="keyword">string</span>-regexp<span class="variable">@1</span>.0.3, graceful-fs<span class="variable">@3</span>.0.8, chokidar<span class="variable">@0</span>.12.6)
├── js-yaml<span class="variable">@3</span>.3.1 (esprima<span class="variable">@2</span>.2.0, argparse<span class="variable">@1</span>.0.2)
├── nunjucks<span class="variable">@1</span>.3.4 (optimist<span class="variable">@0</span>.6.1, chokidar<span class="variable">@0</span>.12.6)
├── warehouse<span class="variable">@1</span>.0.2 (graceful-fs<span class="variable">@3</span>.0.8, cuid<span class="variable">@1</span>.2.5, JSONStream<span class="variable">@0</span>.10.0)
├── cheerio<span class="variable">@0</span>.19.0 (entities<span class="variable">@1</span>.1.1, dom-serializer<span class="variable">@0</span>.1.0, css-<span class="keyword">select</span><span class="variable">@1</span>.0.0, htmlparser2<span class="variable">@3</span>.8.3)
├── bunyan<span class="variable">@1</span>.4.0 (safe-json-stringify<span class="variable">@1</span>.0.3, dtrace-provider<span class="variable">@0</span>.5.0, mv<span class="variable">@2</span>.1.1)

├── hexo-cli<span class="variable">@0</span>.1.7 (minimist<span class="variable">@1</span>.1.2)
├── moment-timezone<span class="variable">@0</span>.3.1
├── moment<span class="variable">@2</span>.10.3
├── hexo-util<span class="variable">@0</span>.1.7 (ent<span class="variable">@2</span>.2.0, highlight.js<span class="variable">@8</span>.6.0)
├── swig<span class="variable">@1</span>.4.2 (optimist<span class="variable">@0</span>.6.1, uglify-js<span class="variable">@2</span>.4.24)
└── lodash<span class="variable">@3</span>.10.0

D:\git\hexo&gt;
</code></pre><h2 id="创建博客(初始化hexo)">创建博客(初始化hexo)</h2><p>创建博客站点的本地目录，然后在文件夹下执行命令：<br>$ hexo init<br>[info] Copying data<br>[info] You are almost done! Don’t forget to run <code>npm install</code> before you start b<br>logging with Hexo!</p>
<p>Hexo会自动在目标文件夹下建立网站所需要的文件。然后按照提示，安装node_modules，执行如下命令：<br>$ hexo install</p>
<h2 id="创建文章本地调试">创建文章本地调试</h2><p>预览本地调试模式，执行如下命令：<br>$ hexo server<br>[info] Hexo is running at <a href="http://localhost:4000/" target="_blank" rel="external">http://localhost:4000/</a>. Press Ctrl+C to stop.</p>
<p>关键命令简介：<br>hexo n     #创建新的文章<br>hexo g     #重新生成站点<br>hexo s     #启动本地服务<br>hexo d     #发布到github</p>
<p>创建文章<br>$ hexo new “使用Hexo搭建Github静态博客”<br>在Hexo工作文件夹下source_posts发现新创建的md文件 使用Hexo搭建Github静态博客.md 。</p>
<h2 id="配置Github">配置Github</h2><p>部署到Github需要修改配置文件_config.yml文件，在Hexo工作目录之下：</p>
<pre><code># Deployment
## <span class="string">Docs:</span> <span class="string">http:</span><span class="comment">//hexo.io/docs/deployment.html</span>
<span class="label">
deploy:</span>
<span class="label">    type:</span> git
<span class="label">    repository:</span> git<span class="annotation">@github</span>.<span class="string">com:</span>&lt;Your Github Username&gt;/&lt;Your github.io url&gt;
<span class="label">    branch:</span> master
</code></pre><p>注意，当前type为git，而不是github</p>
<p>测试Github是否好用<br>ssh -T git@github.com</p>
<h2 id="远程发布">远程发布</h2><p>远程部署到Github，通过执行如下命令：<br>$ hexi deploy</p>
<p>Troubleshooting<br>出现错误：Error: spawn git ENOENT<br>解决方案：<br><a href="http://blog.csdn.net/rainloving/article/details/46595559" target="_blank" rel="external">http://blog.csdn.net/rainloving/article/details/46595559</a> </p>
<p>使用github出现：fatal: unable to access: Failed connect to github.com:8080: No error<br>解决方案：<br><a href="http://www.zhihu.com/question/26954892" target="_blank" rel="external">http://www.zhihu.com/question/26954892</a> </p>
<p>使用github出现：ssh:connect to host github.com port 22: Bad file number<br>解决方案：<br><a href="http://www.xnbing.org/?p=759" target="_blank" rel="external">http://www.xnbing.org/?p=759</a><br><a href="http://blog.csdn.net/temotemo/article/details/7641883" target="_blank" rel="external">http://blog.csdn.net/temotemo/article/details/7641883</a> </p>
<h2 id="添加sitemap和feed">添加sitemap和feed</h2><p>首先安装sitemap和feed插件<br>$ npm install hexo-generator-sitemap<br>$ npm install hexo-generator-feed</p>
<p>修改配置，在文件 _config.yml 增加以下内容</p>
<pre><code><span class="preprocessor"># Extensions</span>
<span class="label">Plugins:</span>
- hexo-generator-feed
- hexo-generator-sitemap

<span class="preprocessor">#Feed Atom</span>
<span class="label">feed:</span>
    type: atom
    path: atom.xml
    limit: <span class="number">20</span>

<span class="preprocessor">#sitemap</span>
<span class="label">sitemap:</span>
    path: sitemap.xml
</code></pre><p>在 themes\landscape_config.yml 中添加：</p>
<pre><code><span class="attribute">menu</span>:
    <span class="attribute">Home</span>: /
    <span class="attribute">Archives</span>: /archives
    <span class="attribute">Sitemap</span>: /sitemap.xml
<span class="attribute">rss</span>: /atom.xml
</code></pre><h2 id="支持百度统计">支持百度统计</h2><p>在 <a href="http://tongji.baidu.com" target="_blank" rel="external">http://tongji.baidu.com</a> 注册帐号，添加网站，生成统计功能的 JS 代码。</p>
<p>在 themes\landscape_config.yml 中新添加一行：</p>
<pre><code><span class="keyword">baidu_t</span>ongji: <span class="keyword">true</span>
</code></pre><p>在 themes\landscape\layout_partial\head.ejs 中head的结束标签  之前新添加一行代码</p>
<pre><code>&lt;<span class="preprocessor">%</span>- partial<span class="comment">('baidu_tongji')</span> <span class="preprocessor">%</span>&gt;
</code></pre><p>在 themes\landscape\layout_partial 中新创建一个文件 baidu_tongji.ejs 并添加如下内容：</p>
<pre><code><span class="xml"></span>&lt;%<span class="ruby"> <span class="keyword">if</span> (theme.baidu_tongji){ </span>%&gt;<span class="xml">
<span class="tag">&lt;<span class="title">script</span> <span class="attribute">type</span>=<span class="value">"text/javascript"</span>&gt;</span><span class="apache">
    <span class="tag">&lt;百度统计的 JS 代码&gt;</span>
</span><span class="tag">&lt;/<span class="title">script</span>&gt;</span>
</span>&lt;%<span class="ruby"> } </span>%&gt;<span class="xml"></span>
</code></pre><p>添加统计，参考：<br><a href="http://ibruce.info/2013/11/22/hexo-your-blog/" target="_blank" rel="external">http://ibruce.info/2013/11/22/hexo-your-blog/</a><br><a href="http://www.cnblogs.com/zhcncn/p/4097881.html" target="_blank" rel="external">http://www.cnblogs.com/zhcncn/p/4097881.html</a> </p>
<h2 id="支持图片">支持图片</h2><p>在source目录下创建images目录，然后将图片放在其中。</p>
<h2 id="添加robots-txt">添加robots.txt</h2><p><a href="http://blog.lmintlcx.com/post/blog-with-hexo.html" target="_blank" rel="external">http://blog.lmintlcx.com/post/blog-with-hexo.html</a> </p>
<h2 id="参考资源">参考资源</h2><p><a href="http://blog.lmintlcx.com/post/blog-with-hexo.html" target="_blank" rel="external">http://blog.lmintlcx.com/post/blog-with-hexo.html</a><br><a href="https://github.com/bruce-sha" target="_blank" rel="external">https://github.com/bruce-sha</a><br><a href="http://zipperary.com/2013/05/28/hexo-guide-2/" target="_blank" rel="external">http://zipperary.com/2013/05/28/hexo-guide-2/</a><br><a href="http://zipperary.com/2013/05/29/hexo-guide-3/" target="_blank" rel="external">http://zipperary.com/2013/05/29/hexo-guide-3/</a><br><a href="http://zipperary.com/2013/05/30/hexo-guide-4/" target="_blank" rel="external">http://zipperary.com/2013/05/30/hexo-guide-4/</a><br><a href="http://cnfeat.com/2014/05/10/2014-05-11-how-to-build-a-blog/" target="_blank" rel="external">http://cnfeat.com/2014/05/10/2014-05-11-how-to-build-a-blog/</a><br><a href="http://www.cnblogs.com/zhcncn/p/4097881.html" target="_blank" rel="external">http://www.cnblogs.com/zhcncn/p/4097881.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>环境：</p>
<pre><code><span class="bullet">1. </span>Windows XP
<span class="bullet">2. </span>Git
</code></pre><p>步骤：</p>
<pre><code><span ]]>
    </summary>
    
      <category term="blog" scheme="http://navigating.github.io/tags/blog/"/>
    
      <category term="github" scheme="http://navigating.github.io/tags/github/"/>
    
      <category term="hexo" scheme="http://navigating.github.io/tags/hexo/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://navigating.github.io/2015/hello-world/"/>
    <id>http://navigating.github.io/2015/hello-world/</id>
    <published>2015-07-27T09:20:22.000Z</published>
    <updated>2015-07-28T09:21:58.301Z</updated>
    <content type="html"><![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io]]>
    </summary>
    
      <category term="hexo" scheme="http://navigating.github.io/tags/hexo/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop 2.7.1 发布]]></title>
    <link href="http://navigating.github.io/2015/Hadoop-2-7-1-%E5%8F%91%E5%B8%83/"/>
    <id>http://navigating.github.io/2015/Hadoop-2-7-1-发布/</id>
    <published>2015-07-09T13:49:30.000Z</published>
    <updated>2015-07-30T13:50:50.764Z</updated>
    <content type="html"><![CDATA[<p>2015年7月6日，Apache Hadoop的稳定版本 2.7.1 正式发布。<br><a href="http://hadoop.apache.org/releases.html#Release+Notes" target="_blank" rel="external">http://hadoop.apache.org/releases.html#Release+Notes</a> </p>
<p>Hadoop 2.7的一个小版本发布了，本版本属于稳定版本。<br>修复了2.7.0中存在的131个bug。<br>这是2.7.x第一个稳定版本，增强的功能列表请通过2.7.0版本部分查看。<br>按着计划，下一个2.7.x的小版本是2.7.2.</p>
<p>原文：<br>06 July, 2015: Release 2.7.1 (stable) availableA point release for the 2.7 line. This release is now considered stable.<br>Please see the Hadoop 2.7.1 Release Notes for the list of 131 bug fixes and patches since the previous release 2.7.0. Please look at the 2.7.0 section below for the list of enhancements enabled by this first stable release of 2.7.x.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>2015年7月6日，Apache Hadoop的稳定版本 2.7.1 正式发布。<br><a href="http://hadoop.apache.org/releases.html#Release+Notes" target="_blank" rel="external"]]>
    </summary>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[读《Deploying Apache Kafka: A Practical FAQ》]]></title>
    <link href="http://navigating.github.io/2015/%E8%AF%BB%E3%80%8ADeploying-Apache-Kafka-A-Practical-FAQ%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/读《Deploying-Apache-Kafka-A-Practical-FAQ》/</id>
    <published>2015-07-02T14:57:45.000Z</published>
    <updated>2015-07-30T15:01:55.553Z</updated>
    <content type="html"><![CDATA[<p>Cloudera发布了Kafka的好文，《Deploying Apache Kafka: A Practical FAQ》，参见：<a href="http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-practical-faq" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-practical-faq</a></p>
<p>是否应当为Kafka Broker使用 固态硬盘 (SSD)<br>实际上使用SSD盘并不能显著地改善 Kafka 的性能，主要有两个原因：</p>
<pre><code>* Kafka写磁盘是异步的，不是同步的。就是说，除了启动、停止之外，Kafka的任何操作都不会去等待磁盘同步（sync）完成；而磁盘同步(disk syncs)总是在后台完成的。这就是为什么Kafka消息至少复制到三个副本是至关重要的，因为一旦单个副本崩溃，这个副本就会丢失数据无法同步写到磁盘。
* 每一个Kafka <span class="keyword">Partition</span>被存储为一个串行的WAL（<span class="keyword">Write</span> Ahead <span class="keyword">Log</span>）日志文件。因此，除了极少数的数据查询，Kafka中的磁盘读写都是串行的。现代的操作系统已经对串行读写做了大量的优化工作。
</code></pre><p>如何对Kafka Broker上持久化的数据进行加密<br>目前，Kafka不提供任何机制对Broker上持久化的数据进行加密。用户可以自己对写入到Kafka的数据进行加密，即是，生产者(Producers)在写Kafka之前加密数据，消费者(Consumers)能解密收到的消息。这就要求生产者(Producers)把加密协议(protocols)和密钥(keys)分享给消费者(Consumers)。<br>另外一种选择，就是使用软件提供的文件系统级别的加密，例如Cloudera Navigator Encrypt。Cloudera Navigator Encrypt是Cloudera企业版(Cloudera Enterprise)的一部分，在应用程序和文件系统之间提供了一个透明的加密层。<br>Apache Zookeeper正成为Kafka集群的一个痛点(pain point)，真的吗？<br>Kafka高级消费者(high-level consumer)的早期版本(0.8.1或更早)使用Zookeeper来维护读的偏移量(offsets，主要是Topic的每个Partition的读偏移量)。如果有大量生产者(consumers)同时从Kafka中读数据，对Kafka的读写负载可能就会超出它的容量，Zookeeper就变成一个瓶颈(bottleneck)。当然，这仅仅出现在一些很极端的案例中(extreme cases)，即有成百上千个消费者(consumers)在使用同一个Zookeeper集群来管理偏移量(offset)。<br>不过，这个问题已经在Kafka当前的版本(0.8.2)中解决。从版本0.8.2开始，高级消费者(high-level consumer)能够使用Kafka自己来管理偏移量(offsets)。本质上讲，它使用一个单独的Kafka Topic来管理最近的读偏移量(read offsets)，因此偏移量管理(offset management)不再要求Zookeeper必须存在。然后，用户将不得不面临选择是用Kafka还是Zookeeper来管理偏移量(offsets)，由消费者(consumer)配置参数 offsets.storage 决定。<br>Cloudera强烈推荐使用Kafka来存储偏移量。当然，为了保证向后兼容性，你可以继续选择使用Zookeeper存储偏移量。(例如，你可能有一个监控平台需要从Zookeeper中读取偏移量信息。) 假如你不得不使用Zookeeper进行偏移量(offset)管理，我们推荐你为Kafka集群使用一个专用的Zookeeper集群。假如一个专用的Zookeeper集群仍然有性能瓶颈，你依然可以通过在Zookeeper节点上使用固态硬盘(SSD)来解决问题。<br>Kafka是否支持跨数据中心的可用性<br>Kafka跨数据中心可用性的推荐解决方案是使用MirrorMaker(<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330" target="_blank" rel="external">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330</a> ) 。在你的每一个数据中心都搭建一个Kafka集群，在Kafka集群之间使用MirrorMaker来完成近实时的数据复制。<br>使用MirrorMaker的架构模式是为每一个”逻辑”的topic在每一个数据中心创建一个topic：例如，在逻辑上你有一个”clicks”的topic，那么你实际上有”DC1.clicks”和“DC2.clicks”两个topic(DC1和DC2指得是你的数据中心)。DC1向DC1.clicks中写数据，DC2向DC2.clicks中写数据。MirrorMaker将复制所有的DC1 topics到DC2，并且复制所有的DC2 topics到DC1。现在每个DC上的应用程序都能够访问写入到两个DC的事件。这个应用程序能够合并信息和处理相应的冲突。<br>另一种更复杂的模式是在每一个DC都搭建本地和聚合Kafka集群。这个模式已经被Linkedin使用，Linkedin Kafka运维团队已经在这篇Blog(<a href="https://engineering.linkedin.com/kafka/running-kafka-scale" target="_blank" rel="external">https://engineering.linkedin.com/kafka/running-kafka-scale</a> )中有详细的描述(参见“Tiers and Aggregation”)。<br>Kafka支持哪些类型的数据转换(data transformation)<br>数据流过的Kafka的时候，Kafka并不能进行数据转换。为了处理数据转换，我们推荐如下方法：</p>
<pre><code>* 对于简单事件处理，使用<span class="constant">Flume Kafka </span>integration(<span class="symbol">http:</span>/<span class="regexp">/blog.cloudera.com/blog</span><span class="regexp">/2014/</span><span class="number">11</span>/flafka-apache-flume-meets-apache-kafka-<span class="keyword">for</span>-event-processing )，并且写一个简单的<span class="constant">Apache Flume Interceptor。</span>
* 对于复杂(事件)处理，使用<span class="constant">Apache Spark Streaming从Kafka中</span>读数据和处理数据。
</code></pre><p>在这两种情况下，被转换或者处理的数据可被写会到新的Kafka Topic中，或者直接传送到数据的最终消费者(Consumer)那里。<br>对于实时事件处理模式更全面的描述，看看这篇文章(<a href="http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/</a> )。<br>如何通过Kafka发送大消息或者超大负荷量？<br>Cloudera的性能测试表明Kafka达到最大吞吐量的消息大小为10K左右。更大的消息将导致吞吐量下降。然后，在一些情况下，用户需要发送比10K大的多的消息。<br>如果消息负荷大小是每100s处理MB级别，我们推荐探索以下选择：</p>
<pre><code><span class="bullet">* </span>如果可以使用共享存储(HDFS、S3、NAS)，那么将超负载放在共享存储上，仅用Kafka发送负载数据位置的消息。
<span class="bullet">* </span>对于大消息，在写入Kafka之前将消息拆分成更小的部分，使用消息Key确保所有的拆分部分都写入到同一个partition中，以便于它们能被同一个消息着(Consumer)消费的到，在消费的时候将拆分部分重新组装成一个大消息。
</code></pre><p>在通过Kafka发送大消息时，请记住以下几点：<br>压缩配置</p>
<pre><code><span class="keyword">*</span> Kafka生产者(Producers)能够压缩消息。通过配置参数compression.codec确保压缩已经开启。有效的选项为<span class="string">"gzip"</span>和<span class="string">"snappy"</span>。
</code></pre><p>Broker配置</p>
<pre><code>* message.<span class="built_in">max</span>.<span class="keyword">bytes</span> (default: <span class="number">1000000</span>): Broker能够接受的最大消息。增加这个值以便于匹配你的最大消息。
* <span class="built_in">log</span>.<span class="keyword">segment</span>.<span class="keyword">bytes</span> (default: <span class="number">1</span>GB): Kafka数据文件的大小。确保它至少大于一条消息。默认情况下已经够用，一般最大的消息不会超过<span class="number">1</span>G大小。
* replica.fetch.<span class="built_in">max</span>.<span class="keyword">bytes</span> (default: <span class="number">1</span>MB): Broker间复制的最大的数据大小。这个值必须大于message.<span class="built_in">max</span>.<span class="keyword">bytes</span>，否则一个Broker接受到消息但是会复制失败，从而导致潜在的数据丢失。
</code></pre><p>Consumer配置</p>
<pre><code>* <span class="tag">fetch</span><span class="class">.message</span><span class="class">.max</span><span class="class">.bytes</span> (<span class="rule"><span class="attribute">default</span>:<span class="value"> <span class="number">1</span>MB): Consumer所读消息的最大大小。这个值应该大于或者等于Broker配置的message.max.bytes的值。</span></span>
</code></pre><p>其他方面的考虑：</p>
<pre><code>* <span class="tag">Broker</span>需要针对复制为每一个<span class="tag">partition</span>分配一个<span class="tag">replica</span><span class="class">.fetch</span><span class="class">.max</span><span class="class">.bytes</span>大小的缓存区。需要计算确认( <span class="tag">partition</span>的数量 * 最大消息的大小 )不会超过可用的内存，否则就会引发<span class="tag">OOMs</span>（内存溢出异常）。
* <span class="tag">Consumers</span>有同样的问题，因子参数为 <span class="tag">fetch</span><span class="class">.message</span><span class="class">.max</span><span class="class">.bytes</span> ：确认每一个<span class="tag">partition</span>的消费者针对最大的消息有足够可用的内存。
* 大消息可能引发更长时间的垃圾回收停顿(<span class="tag">garbage</span> <span class="tag">collection</span> <span class="tag">pauses</span>)(<span class="tag">brokers</span>需要申请更大块的内存)。注意观察<span class="tag">GC</span>日志和服务器日志。假如发现长时间的<span class="tag">GC</span>停顿导致<span class="tag">Kafka</span>丢失了<span class="tag">Zookeeper</span> <span class="tag">session</span>，你可能需要为<span class="tag">zookeeper</span><span class="class">.session</span><span class="class">.timeout</span><span class="class">.ms</span>配置更长的<span class="tag">timeout</span>值。
</code></pre><p>Kafka是否支持MQTT或JMS协议<br>目前，Kafka针对上述协议不提供直接支持。但是，用户可以自己编写Adaptors从MQTT或者JMS中读取数据，然后写入到Kafka中。</p>
<p>更多关于在CDH中使用Kafka的信息，下载Deployment Guide(<a href="http://www.cloudera.com/content/cloudera/en/resources/library/datasheet/kafka-reference-architecture.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/resources/library/datasheet/kafka-reference-architecture.html</a> ) 或者 观看webinar “Bringing Real-Time Data to Hadoop”(<a href="http://www.cloudera.com/content/cloudera/en/resources/library/recordedwebinar/kafka-webinar-recording.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/resources/library/recordedwebinar/kafka-webinar-recording.html</a> )。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Cloudera发布了Kafka的好文，《Deploying Apache Kafka: A Practical FAQ》，参见：<a href="http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-]]>
    </summary>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="Kafka" scheme="http://navigating.github.io/tags/Kafka/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[BIGDATA Update 201506]]></title>
    <link href="http://navigating.github.io/2015/BIGDATA-Update-201506/"/>
    <id>http://navigating.github.io/2015/BIGDATA-Update-201506/</id>
    <published>2015-06-09T13:52:23.000Z</published>
    <updated>2015-07-30T14:09:27.343Z</updated>
    <content type="html"><![CDATA[<p>Pinot：LinkedIn的实时数据分析系统<br><a href="http://www.infoq.com/cn/news/2014/10/linkdln" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/10/linkdln</a><br><a href="https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot" target="_blank" rel="external">https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot</a></p>
<p>Twitter Heron：Twitter发布新的大数据实时分析系统Heron<br><a href="http://geek.csdn.net/news/detail/33750" target="_blank" rel="external">http://geek.csdn.net/news/detail/33750</a><br><a href="http://www.longda.us/?p=529" target="_blank" rel="external">http://www.longda.us/?p=529</a> </p>
<p>Cloudera<br>HBase对MOBs( Moderate Objects, 主要是大小100K到10M的对象存储 )的支持<br><a href="http://blog.cloudera.com/blog/2015/06/inside-apache-hbases-new-support-for-mobs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/inside-apache-hbases-new-support-for-mobs/</a><br>准实时计算架构模式<br><a href="http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/</a><br>(翻译：<a href="http://zhuanlan.zhihu.com/donglaoshi/20082628" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20082628</a> )<br>CDH 5.4 新功能：敏感数据处理(Sensitive Data Redaction)<br><a href="http://blog.cloudera.com/blog/2015/06/new-in-cdh-5-4-sensitive-data-redaction/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/new-in-cdh-5-4-sensitive-data-redaction/</a> </p>
<p>Hortonworks<br>YARN的CapacityScheduler对Resource-preemption的支持<br><a href="http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/" target="_blank" rel="external">http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/</a><br>Hadoop集群对Multihoming的支持<br><a href="http://hortonworks.com/blog/multihoming-on-hadoop-yarn-clusters/" target="_blank" rel="external">http://hortonworks.com/blog/multihoming-on-hadoop-yarn-clusters/</a><br>HDP 2.3企业级HDFS数据加密<br><a href="http://hortonworks.com/blog/new-in-hdp-2-3-enterprise-grade-hdfs-data-at-rest-encryption/" target="_blank" rel="external">http://hortonworks.com/blog/new-in-hdp-2-3-enterprise-grade-hdfs-data-at-rest-encryption/</a><br>Apache Slider 0.80.0版本发布<br><a href="http://hortonworks.com/blog/announcing-apache-slider-0-80-0/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-apache-slider-0-80-0/</a><br>Apache Spark 1.3.1 on HDP 2.2<br><a href="http://hortonworks.com/blog/apache-spark-on-hdp-learn-try-and-do/" target="_blank" rel="external">http://hortonworks.com/blog/apache-spark-on-hdp-learn-try-and-do/</a><br><a href="http://hortonworks.com/hadoop-tutorial/using-apache-spark-technical-preview-with-hdp-2-2/" target="_blank" rel="external">http://hortonworks.com/hadoop-tutorial/using-apache-spark-technical-preview-with-hdp-2-2/</a><br>Ambari 2.0.1 和 HDP 2.2.6 发布<br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html</a><br><a href="http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_releasenotes_ambari_2.0.1.0/content/ch_relnotes-ambari-2.0.1.0.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_releasenotes_ambari_2.0.1.0/content/ch_relnotes-ambari-2.0.1.0.html</a></p>
<p>其他：<br>Graphite的百万Metrics实践之路<br><a href="http://calvin1978.blogcn.com/articles/graphite.html" target="_blank" rel="external">http://calvin1978.blogcn.com/articles/graphite.html</a><br>HBaseCon 2015 大会幻灯片 &amp; 视频<br><a href="http://hbasecon.com/archive.html" target="_blank" rel="external">http://hbasecon.com/archive.html</a><br>HBase在腾讯大数据的应用实践<br><a href="http://www.d1net.com/bigdata/news/353500.html" target="_blank" rel="external">http://www.d1net.com/bigdata/news/353500.html</a><br>从Spark到Hadoop的架构实践<br><a href="http://www.csdn.net/article/2015-06-08/2824889" target="_blank" rel="external">http://www.csdn.net/article/2015-06-08/2824889</a><br>56网大数据<br><a href="http://share.csdn.net/slides/10903" target="_blank" rel="external">http://share.csdn.net/slides/10903</a><br>七牛技术总监陈超：记Spark Summit China 2015<br><a href="http://www.csdn.net/article/2015-04-30/2824594-spark-summit-china-2015" target="_blank" rel="external">http://www.csdn.net/article/2015-04-30/2824594-spark-summit-china-2015</a><br>唯品会美研中心郭安琪：2015 Hadoop Summit见闻<br><a href="http://zhuanlan.zhihu.com/donglaoshi/20072576" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20072576</a><br>华为叶琪：论Spark Streaming的数据可靠性和一致性<br><a href="http://www.csdn.net/article/2015-06-12/2824938" target="_blank" rel="external">http://www.csdn.net/article/2015-06-12/2824938</a><br>Hadoop Summit 2015<br><a href="http://2015.hadoopsummit.org/san-jose/agenda/" target="_blank" rel="external">http://2015.hadoopsummit.org/san-jose/agenda/</a><br>Spark Summit 2015<br><a href="https://spark-summit.org/2015/" target="_blank" rel="external">https://spark-summit.org/2015/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Pinot：LinkedIn的实时数据分析系统<br><a href="http://www.infoq.com/cn/news/2014/10/linkdln" target="_blank" rel="external">http://www.infoq.com/cn/]]>
    </summary>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[BIGDATA Update 201502]]></title>
    <link href="http://navigating.github.io/2015/BIGDATA-Update-201502/"/>
    <id>http://navigating.github.io/2015/BIGDATA-Update-201502/</id>
    <published>2015-03-24T14:10:07.000Z</published>
    <updated>2015-07-30T14:12:04.632Z</updated>
    <content type="html"><![CDATA[<p>本月Hadoop技术动态：<br>1.经过6年的孵化，Hive 1.0 发布了。<br>2.经过7年的孵化，HBase 1.0 发布了。<br>3.Cloudera 开始提供 Hive-on-Spark Beta版的下载。</p>
<p>HBase 1.0 需要特别关注的特性：<br>1.API的重新组织和变更；<br>2.读的高可用；<br>3.在线配置变更；</p>
<p>HDP 2.2 发布有一段时间：<br><a href="http://hortonworks.com/blog/announcing-hive-1-0-stable-moment-time/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-hive-1-0-stable-moment-time/</a><br><a href="http://hortonworks.com/blog/start-new-era-apache-hbase-1-0/" target="_blank" rel="external">http://hortonworks.com/blog/start-new-era-apache-hbase-1-0/</a><br><a href="http://blog.cloudera.com/blog/2015/02/apache-hbase-1-0-is-released/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/apache-hbase-1-0-is-released/</a><br><a href="http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/</a><br><a href="https://issues.apache.org/jira/secure/attachment/12652517/Hive-on-Spark.pdf" target="_blank" rel="external">https://issues.apache.org/jira/secure/attachment/12652517/Hive-on-Spark.pdf</a></p>
<p>Cluster Manager Framework:<br>1.YARN<br>2.Apache Helix</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>本月Hadoop技术动态：<br>1.经过6年的孵化，Hive 1.0 发布了。<br>2.经过7年的孵化，HBase 1.0 发布了。<br>3.Cloudera 开始提供 Hive-on-Spark Beta版的下载。</p>
<p>HBase 1.0 需要特别关注的特]]>
    </summary>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[XP极限编程]]></title>
    <link href="http://navigating.github.io/2004/XP%E6%9E%81%E9%99%90%E7%BC%96%E7%A8%8B/"/>
    <id>http://navigating.github.io/2004/XP极限编程/</id>
    <published>2004-10-22T15:32:55.000Z</published>
    <updated>2015-07-28T15:36:31.222Z</updated>
    <content type="html"><![CDATA[<ol>
<li>模式</li>
<li>重构</li>
<li>测试</li>
<li>增量交付</li>
<li>频繁构建</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<ol>
<li>模式</li>
<li>重构</li>
<li>测试</li>
<li>增量交付</li>
<li>频繁构建</li>
</ol>
]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[设计模式之分层实现(Layering Implements)]]></title>
    <link href="http://navigating.github.io/2004/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%88%86%E5%B1%82%E5%AE%9E%E7%8E%B0(Layering%20Implements)/"/>
    <id>http://navigating.github.io/2004/设计模式之分层实现(Layering Implements)/</id>
    <published>2004-10-22T14:10:50.000Z</published>
    <updated>2015-07-28T15:10:23.097Z</updated>
    <content type="html"><![CDATA[<p>原文第一次发布：<a href="http://www.blogbus.com/navigating-logs/455608.html" target="_blank" rel="external">http://www.blogbus.com/navigating-logs/455608.html</a><br>今天去听课，老师讲到persistence layer的时候，提到dao的出现主要应付数据库的移植，可能他出现的最初的原型确实如此，仔细想一下，真实的enterprise application的数据源有多少在移植呢，就我们一般而言，一般相对都是很固定，在软件的最初设计阶段就已经选型了。（在OO中讲，如果一个变化可能要等三五年，那么你就得去考虑现在考虑这个变化是不是有意义？）</p>
<p>现在在设计模式的基础上，提出了企业架构应用的诸多模式，这么多模式都是在依赖于设计模式来实现的。那么企业架构应用的诸多模式的价值何在？就persistence layer而言，抽象出数据持久化的一般问题解决方案，看下面的图：</p>
<p>在与数据源(一般都是大型数据库)交互的时候，对于网络的依赖是必不可少的。基本的原则：minimize distributed communication(今天老师刚讲的)。在持久层，既是DAO还需要做一定的数据缓冲，减少网络访问，对于本地persistence object的管理才是比较重要的。对于我们的工作中，不一定要做数据缓冲，但是应该考虑对于网络访问是否最小化了。</p>
<p>为什么要分层?</p>
<ol>
<li>can understand a single layer as a coherent whole without knowing much about the other layers。就如现在我们在设计dao的时候，很少去考虑什么business logic，单纯的去研究数据的访问。</li>
</ol>
<ol>
<li>can substitute layers with alternative implementations of the same basic services.这个问题就跟接口有关了，还要讨论的。很好的满足了DIP(依赖倒置原则);</li>
</ol>
<ol>
<li>minimize dependencies between the layers.对象之间的应用不再是杂乱五章的，大家都依赖于抽象。</li>
</ol>
<ol>
<li>Layers make good places for standardization.现在datasource layer(persistence layer),domain layer,presentation lay.经过大家的群策群力，每一个层次都有了模式，加快了解决问题的速度，使得开发人员集中于一些必要的挑战。最近有提出了service layer.(今天老师讲了The Business Delegate Design Pattern,但是老师却讲到了field和object的对比，这个方式我们早已讲过了，这次SUN单独提出来，它的意义远不再这里，我理解和service layer有一定的关系。)</li>
</ol>
<ol>
<li>Once you have a layer built you can use it for many higher level services.这也是分离出层的原型吧。</li>
</ol>
<p>我觉得Pattern of Enterprise Application Architecture讲分层的第一句话特别好：</p>
<p>Layering is one of the most common techniques that software designers use to break apart a complicated software system.</p>
<p>依然有了分层，就产生了层，有了层，就有了层与层之间的关系，那么就面对这些 层关系的设计和实现问题。</p>
<p>在企业应用架构有三层（也有四层的分法），这些层之间必然会有一些依赖，我们都知道依赖有一个原则：依赖倒置原则（DIP）</p>
<ol>
<li>高层模块不应该依赖于底层模块。二者都应该依赖于抽象。</li>
</ol>
<ol>
<li>抽象不应该依赖于细节。细节应该以来于抽象。</li>
</ol>
<p>对应于分层，较“高”层包含了什么，包含了策略选择和业务模型，而这些真是应用程序的价值所在，如果“高”层以来于“低”层，那么“低”层的变化就会影响到“高”层。</p>
<p>更不可能去让“低”层依赖于“高”层。</p>
<p>现在只有一种可能，“高”层和“低”层是独立的。这不也正是我们使用framework的价值所在吗？</p>
<p>这样层与层之间的依赖就必须是抽象，而不是实现和细节。</p>
<p>Service layer的出现，是为了面对客户层接入集成了一系列高效的操作集。主要目的提高数据存取和业务逻辑的利用率，减少重复调用。</p>
<p>Serverice layer是一个边缘层（boundary）。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>原文第一次发布：<a href="http://www.blogbus.com/navigating-logs/455608.html" target="_blank" rel="external">http://www.blogbus.com/navigating-log]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[转：《敏捷软件开发：原则、模式与实践》中文版序]]></title>
    <link href="http://navigating.github.io/2004/%E8%BD%AC%EF%BC%9A%E3%80%8A%E6%95%8F%E6%8D%B7%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%EF%BC%9A%E5%8E%9F%E5%88%99%E3%80%81%E6%A8%A1%E5%BC%8F%E4%B8%8E%E5%AE%9E%E8%B7%B5%E3%80%8B%E4%B8%AD%E6%96%87%E7%89%88%E5%BA%8F/"/>
    <id>http://navigating.github.io/2004/转：《敏捷软件开发：原则、模式与实践》中文版序/</id>
    <published>2004-10-14T14:10:50.000Z</published>
    <updated>2015-07-28T15:07:24.258Z</updated>
    <content type="html"><![CDATA[<p>“最好的软件开发人员都知道一个秘密：美的东西比丑的东西创建起来更廉价，也更快捷。构建、维护一个美的软件系统所花费的时间、金钱都要少于丑的系统。软件开发新手往往不理解这一点。他们认为做每件事情都必须要快，他们认为美是不实用的。错！由于事情做得过快，他们造成的混乱致使软件僵化，难以理解。美的系统是灵活、易于理解的，构建、维护它们就是一种快乐。丑陋的系统才是不实用的。丑陋会降低你的开发速度，使你的软件昂贵而又脆弱。构建、维护美的系统所花费的代价最少，交付起来也最快。”</p>
<p>——摘自“Robert C. Martin《敏捷软件开发：原则、模式与实践》中文版序”</p>
<p>Robert C. Martin《敏捷软件开发：原则、模式与实践》中文版序</p>
<p>　　除了我的家庭，软件是我的挚爱。通过它，我可以创造出美的东西。软件之美在于它的功能，在于它的内部结构，还在于团队创建它的过程。对用户来说，通过直观、简单的界面呈现出恰当特性的程序就是美的。对软件设计者来说，被简单、直观地分割，并具有最小内部耦合的内部结构就是美的。对开发人员和管理者来说，每周都会取得重大进展，并且生产出无缺陷代码的具有活力的团队就是美的。美存在于所有这些层次之中，它们都是本书内容的一部分。<br>　　软件开发人员如何学到创造美的知识呢？在本书中，我讲授了一些原则、模式以及实践，它们可以帮助软件开发人员在追求美的程序、设计以及团队的道路上迈出第一步。其中，我们探索了基本的设计原则，软件设计结构的通用模式以及有助于团队融为一个有机整体的一系列实践。由于本书是关于软件开发的，所以包含了许多代码。仔细研究这些代码是学习本书所教授的原则、模式以及实践的最有效方法。<br>　　人们需要软件—需要许多的软件。50年前，软件还只是运行在少量大型、昂贵的机器之上。30年前，软件可以运行在大多数公司和工业环境之中。现在，移动电话、手表、电器、汽车、玩具以及工具中都运行有软件，并且对更新、更好软件的需求永远不会停止。随着人类文明的发展和壮大，随着发展中国家不断构建它们的基础设施，随着发达国家努力追求更高的效率，就需要越来越多的软件。如果在所有这些软件之中，都没有美存在，这将会是一个很大的遗憾。<br>　　我们知道软件可能会是丑陋的。我们知道软件可能会难以使用、不可靠并且是粗制滥造的；我们知道有一些软件系统，其混乱、粗糙的内部结构使得对它们的更改既昂贵又困难；我们还见过那些通过笨拙、难以使用的界面展现其特性的软件系统；我们同样也见过那些易崩溃且行为不当的软件系统。这些都是丑陋的系统。糟糕的是，作为一种职业，软件开发人员所创建出来的美的东西却往往少于丑的东西。如果你正在阅读这本书，那么你也许就是那个想去创造美而不是丑的人。<br>　　最好的软件开发人员都知道一个秘密：美的东西比丑的东西创建起来更廉价，也更快捷。构建、维护一个美的软件系统所花费的时间、金钱都要少于丑的系统。软件开发新手往往不理解这一点。他们认为做每件事情都必须要快，他们认为美是不实用的。错！由于事情做得过快，他们造成的混乱致使软件僵化，难以理解。美的系统是灵活、易于理解的，构建、维护它们就是一种快乐。丑陋的系统才是不实用的。丑陋会降低你的开发速度，使你的软件昂贵而又脆弱。构建、维护美的系统所花费的代价最少，交付起来也最快。<br>　　我希望你能喜爱这本书。我希望你能像我一样学着以创建美的软件而骄傲，并享受其中的快乐。如果你从本书中略微看到了这种快乐，如果本书使你开始感受到了这种骄傲，如果本书点燃了你内心欣赏这种美的火花，那么就远超过我的目标了。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>“最好的软件开发人员都知道一个秘密：美的东西比丑的东西创建起来更廉价，也更快捷。构建、维护一个美的软件系统所花费的时间、金钱都要少于丑的系统。软件开发新手往往不理解这一点。他们认为做每件事情都必须要快，他们认为美是不实用的。错！由于事情做得过快，他们造成的混乱致使软件僵化，]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[创建对象的两种方式]]></title>
    <link href="http://navigating.github.io/2004/%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
    <id>http://navigating.github.io/2004/创建对象的两种方式/</id>
    <published>2004-10-11T14:10:50.000Z</published>
    <updated>2015-07-28T15:00:30.879Z</updated>
    <content type="html"><![CDATA[<p>One is to create a object with a rich constructor so that its at least created with all<br>least created with all its mandatory data.<br>The other is to create o object an empty object and then populate it with the mandatory<br>data.<br>The former could have a well-formed object from the start.This aslo meants that, if you<br>have an immutable field, you can enforce it by not providing any method to change its<br>value.<br>The problem with a rich constructor is that you have to be aware of cyclic references.<br>Avoiding this requires special case code, often usingg lazy load.<br>You can do this creating an empty object.Use a non-arg constructor to create a blank object<br>and insert that empty object immediately into other object.The way,if you have a cycle,<br>other object will return an object to stop the recursive loading that two objects reference<br>each other.<br>Using an empty object like this means you may need some setters for values that are truely<br>immutalbe when the object is loaded.A combination of a naming convention and perhaps some<br>status-checking guards can fix this.You can alse use reflection for loading data.</p>
<p>(Spring supports Setter Injection very nice.Sometimes using rich constructor to create a object<br> is preferable after you evaluating.)</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>One is to create a object with a rich constructor so that its at least created with all<br>least created with all its mandatory data.<br>]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Eclipse之Workspace路径修改]]></title>
    <link href="http://navigating.github.io/2004/Eclipse%E4%B9%8BWorkspace%E8%B7%AF%E5%BE%84%E4%BF%AE%E6%94%B9/"/>
    <id>http://navigating.github.io/2004/Eclipse之Workspace路径修改/</id>
    <published>2004-10-11T12:23:15.000Z</published>
    <updated>2015-07-28T15:29:40.778Z</updated>
    <content type="html"><![CDATA[<p>eclipse的worspace的路径放置在根目录：<br>\configuration\org.eclipse.ui.ide\recentWorkspaces.xml</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>eclipse的worspace的路径放置在根目录：<br>\configuration\org.eclipse.ui.ide\recentWorkspaces.xml</p>
]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Statement标准关闭方法]]></title>
    <link href="http://navigating.github.io/2004/Statement%E6%A0%87%E5%87%86%E5%85%B3%E9%97%AD%E6%96%B9%E6%B3%95/"/>
    <id>http://navigating.github.io/2004/Statement标准关闭方法/</id>
    <published>2004-09-29T14:10:50.000Z</published>
    <updated>2015-07-28T14:55:39.496Z</updated>
    <content type="html"><![CDATA[<p>在O/R Mapper的代码中，Statement的关闭模式：</p>
<p>clearup的实现： </p>
<p>  /**</p>
<pre><code> * Close database resource.

 *

 * <span class="annotation">@param</span> conn

 * <span class="annotation">@param</span> pstmt

 */

<span class="keyword">protected</span> <span class="function"><span class="keyword">void</span> <span class="title">clearup</span><span class="params">(Connection conn, PreparedStatement pstmt)</span>

</span>{

    <span class="keyword">if</span> (pstmt != <span class="keyword">null</span>)

    {

        <span class="keyword">try</span>

        {

            pstmt.close();

        }

        <span class="keyword">catch</span> (SQLException se)

        {

            se.printStackTrace();

        }

        pstmt = <span class="keyword">null</span>;

    }



    <span class="keyword">if</span> (conn != <span class="keyword">null</span>)

    {

        <span class="keyword">try</span>

        {

            conn.close();

        }

        <span class="keyword">catch</span> (SQLException se)

        {

            se.printStackTrace();

        }

        conn = <span class="keyword">null</span>;

    }

}
</code></pre><p>1.一般模式</p>
<pre><code>    Connection conn = null<span class="comment">;</span>

PreparedStatement pstmt = null<span class="comment">;</span>

try

{

    conn = getConnection()<span class="comment">;</span>

    pstmt = conn.prepareStatement(SQL)<span class="comment">;</span>

    pstmt.setString(1, id)<span class="comment">;</span>

    pstmt.setString(2, type)<span class="comment">;</span>

    pstmt.executeUpdate()<span class="comment">;          </span>

} catch (SQLException se)

{         
</code></pre><p>throw new ApplicationException(se);</p>
<pre><code>} <span class="keyword">finally</span>

{

    clearup(conn, pstmt);<span class="comment">//有效的close了Connection;只有一个pstmt,相当于只打开了一个光标，在这里有效的close了pstmt.</span>

}
</code></pre><p>1．在使用jdbc进行批处理的时候，注意每一次创建一个Statement，一旦无须使用，立刻关闭，因为每一次都是打开了Oracle的一个cursor.(Oracle默认最大一次打开300个，最多也</p>
<p>只支持1000多个)</p>
<p>Connection conn = null;</p>
<pre><code>PreparedStatement pstmt = null;



try

{

    conn = getConnection<span class="params">()</span>;

    pstmt = conn.prepareStatement<span class="params">(SQL1)</span>;

    pstmt.setString<span class="params">(<span class="number">1</span>, id)</span>;

    pstmt.setString<span class="params">(<span class="number">2</span>, type)</span>;

    pstmt.executeUpdate<span class="params">()</span>;

    pstmt.close<span class="params">()</span>; <span class="comment">// 请立刻close当前pstmt的光标。</span>

    pstmt = null;  <span class="comment">// 必须使用</span>



    pstmt = conn.propareStatement<span class="params">(SQL2)</span>;

   pstmt.setString<span class="params">(<span class="number">1</span>, id)</span>;

    pstmt.setString<span class="params">(<span class="number">2</span>, type)</span>;

    pstmt.executeUpdate<span class="params">()</span>;

    pstmt.close<span class="params">()</span>; <span class="comment">// 请立刻close当前pstmt的光标。</span>


    pstmt = null;  <span class="comment">// 必须使用</span>

  … …

} catch <span class="params">(SQLException se)</span>

{         
</code></pre><p>throw new ApplicationException(se);</p>
<pre><code>   } <span class="keyword">finally</span>

   {

       clearup(conn, pstmt); <span class="comment">//有效的close了Connection;只有出现exception才能有效close pstmt;</span>

}
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>在O/R Mapper的代码中，Statement的关闭模式：</p>
<p>clearup的实现： </p>
<p>  /**</p>
<pre><code> * Close database resource.

 *

 * <span class="annotati]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java编程之log4j日志]]></title>
    <link href="http://navigating.github.io/2004/Java%E7%BC%96%E7%A8%8B%E4%B9%8Blog4j%E6%97%A5%E5%BF%97/"/>
    <id>http://navigating.github.io/2004/Java编程之log4j日志/</id>
    <published>2004-09-28T14:09:50.000Z</published>
    <updated>2015-07-28T14:49:32.515Z</updated>
    <content type="html"><![CDATA[<p>一直以来，如何去写日志都萦绕我胸怀已久；直到最近又要开始新的project,关于系统中日志如何设计又一次让我踌躇，发散了思绪，化成了淡淡的笔迹，拿出来给自己，也给大家分享。</p>
<p>在系统中分为两种日志：一种是流程日志，要包含用户最关心的信息，便于问题查找和跟踪；包含系统运行的状态信息和当前步骤；。。。。。。另外一种是调试信息，这有包含系统release debug信息和代码开发测试debug信息两种，release debug信息是希望在系统运行中可能运用到的日至类别；develop&amp;test debug message是在系统开发测试阶段实用，而在code release之后，这些部分需要被删除的（代码冗余,性能考虑,代码净化）。</p>
<p>可以说是三种类型的日志，在code开发过程中都需要在代码中实现的，但是可能在release版本中会有所处理的；这就联系到处理机制。</p>
<p>java一般使用apache的log4j来写日志，可能我们会在log4j的基础之上构建我们系统的日志服务组件，姑且不论哪一种.顺便可以提及log4j的一些feature:日志级别，日志patterns</p>
<p>流程日志是系统运行的必须日志，因此它的级别至少是info级别的。eg:<br> if(log.isInfoEnabled()){<br>            log.info(“ Message! “);<br>        }<br> 或者<br>        {<br>            log.info(“ Message! “);<br>        }</p>
<p>在这个日志信息中要包含：系统流程信息（有必要输入的），用户关心的信息（以优先级最高或者较高的为主），我们有必要在日志输出中关心的消息（以优先级最高为主）；。。。。。。（欢迎补充）</p>
<p>该日志输入对于记录系统运行状态（定时查看系统是否运行良好，可能是一个守护进程），记录流程运行阶段性的信息（当前阶段系统中各个关注点的情况，比方用户的一些信息，或者用户关心的一些信息,以便于系统出现异常是问题跟踪使用，系统维护必须的一些信息）。。。。。。这些日志在系统任何时段都能够准确的输出，因此没有必要对他何种情况输入等进行考虑和限制。对于log.info级别的日志最好能对于条件进行以下判断会更好。</p>
<p>系统release debug日志，是为了便于系统真正运行起来以后，可能进行debug的时候才需要进行日志输入，之所以对他们进行限制，主要是出于性能等方面的考虑。对日志输入的条件进行严格的限制，<br>        if(log.isDebugEnabled()){<br>            log.debug(“ Message! “);<br>        }</p>
<p>尽管程序员是对代码充满自信（证明足够的健壮性），我们还需要在一些不确定，或者可能出现的，对于明天的事情做一些预测，今天对于问题进行防卫，（这些都是软件工程所反对的），我们都需要处理 跟踪 校验 排查等的区域，通过日志预先放置一些”监听器”.这种日志和系统的可维护性息息相关。这里面的很多日志信息都是“流程日志”的强大备份，也是系统通过test以后日志仍然需要变动的地方。也是和develop debug很难区分的一个区域。</p>
<p>前两种日志都是可能会对系统的性能构成影响，或者产生冗余或者日志不完全。尽管编码没有什么难度，但是都需要一些清晰的规范支持(至少是team一级的)，主要是日志的可读性，覆盖的面积，可维护性。  </p>
<p>develop log基本上上适用于代码调试阶段和单元测试阶段使用的log信息。根据程序员个人喜好和习惯来定制，但是这些log message在正式版本release以前都是要被删除的。需要一些灵活的机制和pattern实现等来支持。eg:<br>      DebugManager{<br>          public static boolean debug = true;<br>      }</p>
<pre><code><span class="keyword">if</span><span class="params">(DebugManager.debug)</span>{
    <span class="keyword">if</span><span class="params">(log.isDebugEnabled<span class="params">()</span>)</span>{
        <span class="built_in">log</span>.debug<span class="params">(<span class="string">" Message! "</span>)</span>;
    }
}
</code></pre><p>在编译期间中对于这种情况进行优化。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>一直以来，如何去写日志都萦绕我胸怀已久；直到最近又要开始新的project,关于系统中日志如何设计又一次让我踌躇，发散了思绪，化成了淡淡的笔迹，拿出来给自己，也给大家分享。</p>
<p>在系统中分为两种日志：一种是流程日志，要包含用户最关心的信息，便于问题查找和跟踪；包含]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Web开发起始篇]]></title>
    <link href="http://navigating.github.io/2004/Web%E5%BC%80%E5%8F%91%E8%B5%B7%E5%A7%8B%E7%AF%87/"/>
    <id>http://navigating.github.io/2004/Web开发起始篇/</id>
    <published>2004-09-28T13:08:55.000Z</published>
    <updated>2015-07-28T14:49:26.615Z</updated>
    <content type="html"><![CDATA[<p>首先说一些题外话，做web页面开发，也是j2ee吧。</p>
<p>现在学习mvc，应用struts的兄弟姐妹们多多，最近struts也release了1.2.2版本了。</p>
<p>最近一个同事问我如何让一个新人快速进入到struts的开发中去。我理了理头绪，“往事一幕幕”。</p>
<p>从个人职业规划来看，主要有几个思想的方向。前几天谈论理论与实践的距离和有效性，作为coder,我们是实践者，我们也是工作人，我们需要一些方法论来指导我们的思想，我们通过我们思想的脉络来把握技术的发展。</p>
<p>主要有两个方向：一个是project management;一个是developer.</p>
<p>每一个程序员都有把握整个项目的欲望和冲动；动力之后是方法论了。</p>
<p>想到今天我也对别人怎么去coding指手画脚，:)。</p>
<p>1．了解java的代码规范.刚开始是十分重要的。就像人生的第一步似的。</p>
<p>3.了解一下jdk的版本，和在项目中jdk的控制。</p>
<p>2.开发环境ide的版本，eclipse等，如果是eclipse，还有plugins的了解。(怎么配置代码的style.)</p>
<p>3.对于tomcat的了解和配置；读读tomcat的源码也是不错的野味呢./sun one</p>
<p>4.了解mvc的结构和发展。</p>
<p>5.了解测试驱动开发和junit.</p>
<p>6.了解jdbc,更好能了解一些关于数据连接的封装。</p>
<p>7.了解presentation—business—persistence等layer结构；有时间了解framework更好。</p>
<p>8.了解一些java pattern。</p>
<p>9.有时间了解spring的bean管理想法，顺便就学到了依赖注入的思想了，看看martin fowler的文章，了解各个注入方法的特点。</p>
<p>10.了解jsp的语法。</p>
<p>11.了解javascript的语法。</p>
<p>12.了解struts自带的taglib（JSTL）。apache有一个专门的taglib项目，我们所需要的所有的taglib在里面都有。通过我接触的一些开源的项目，taglib使用的十分广泛。<a href="http://jakarta.apache.org/taglibs/index.html" target="_blank" rel="external">http://jakarta.apache.org/taglibs/index.html</a> 一个tagunit网站<a href="http://www.tagunit.org/tagunit/index.jsp" target="_blank" rel="external">http://www.tagunit.org/tagunit/index.jsp</a></p>
<p>13.有时间可以读struts的源码，我还没有读过，有点儿遗憾，如果能看一下servlet跟好了。</p>
<p>14.了解servlet,filter,listener的用法。特别是filter,链职责模式也可以学到了。</p>
<p>15.了解commons-logging,log4j.如何写log,什么时候写log.    <a href="http://logging.apache.org" target="_blank" rel="external">http://logging.apache.org</a></p>
<p>16.servlet api javadoc:    <a href="http://java.sun.com/products/servlet/2.2/javadoc/" target="_blank" rel="external">http://java.sun.com/products/servlet/2.2/javadoc/</a></p>
<p>17.apache struts:    <a href="http://struts.apache.org/" target="_blank" rel="external">http://struts.apache.org/</a>  <a href="http://struts.apache.org/api/index.html" target="_blank" rel="external">http://struts.apache.org/api/index.html</a> struts已经release了1.2.2版本，应该有不少新特性。</p>
<p>18.junit site:    <a href="http://www.junit.org/junit/javadoc/index.htm" target="_blank" rel="external">http://www.junit.org/junit/javadoc/index.htm</a></p>
<p>19.我们一般画图用的open source 的api也是apache的POI:    <a href="http://jakarta.apache.org/poi" target="_blank" rel="external">http://jakarta.apache.org/poi</a>     <a href="http://jakarta.apache.org/poi/apidocs/index.html" target="_blank" rel="external">http://jakarta.apache.org/poi/apidocs/index.html</a></p>
<p>   如果一开始，我们还是自己写jdbc比较好吧，慢慢的可以接触or mapping的东西。</p>
<p>1.jdbc怎么写可以看 java核心技术.里面还有怎么用java写存储过程的例子。</p>
<p>2.过一段时间可以看看or mapping的东西（Object Relation Mapping）;现在比较热的开源的是：ibatis和hibernate.</p>
<p>在学习过程中，xml和uml都会跑进来的。enjoy it!</p>
<p>关于presentation layer，更多的涉及到jsp,javascript和struts.</p>
<p>1.struts taglib的运用。</p>
<p>2.struts里面有一个validation.xml和validator-rules.xml两个需要理解。</p>
<p>3.struts的formbean的机制，执行顺序。</p>
<p>4.actionmapping的原理。</p>
<p>5.struts-config.xml的配置。（建议：在项目中将struts-config.xml分离，team中每一个成员维护自己的。）</p>
<p>6.中文化的一些问题。unicode的编写。建议使用java提供的命令行：native2ascii -encoding gb2312 sourceFile.properties destFile.properties</p>
<p>7.说明:在struts中，有一个c—control的概念，在我们应用层编码，是看不见它的存在的。它体现在struts中应该是一个org.apache.struts.action.RequestProcessor的类，我也记得不太清楚。</p>
<p>8.了解struts开发中一个baseform的模式。(只有一个baseform的模式)</p>
<p>9.在jpetstore4.0.5中，采取了只使用一个action的模式，这样做是为了将business和action分离，在action层进行了一个公用提取，利于business logic的公用和维护，也提高了可测试性。。。。。。</p>
<p>10.那就看看jpetstore的源代码吧。注：jpetstore4.0.5版使用了ibatis的sql mapping和dao framework。</p>
<p>有时间可以了解一下project management的一些工具，学习写ant和maven的脚本，这都是很重要的。</p>
<ol>
<li><p>学习cvs,了解版本控制。</p>
</li>
<li><p>了解exception机制，原理和发展始末以及方向。</p>
</li>
<li><p>学习xp开发方式。</p>
</li>
</ol>
<p>4.开始学习分析设计了，掌握uml了，了解各个层次的pattern的时机到来了。</p>
<p>还有一些书要看了。经常上网看别人对于现在的开发方向的讨论。。。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>首先说一些题外话，做web页面开发，也是j2ee吧。</p>
<p>现在学习mvc，应用struts的兄弟姐妹们多多，最近struts也release了1.2.2版本了。</p>
<p>最近一个同事问我如何让一个新人快速进入到struts的开发中去。我理了理头绪，“往事一幕]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Eclipse之eclipse3.1M1]]></title>
    <link href="http://navigating.github.io/2004/Eclipse%E4%B9%8Beclipse3.1M1/"/>
    <id>http://navigating.github.io/2004/Eclipse之eclipse3.1M1/</id>
    <published>2004-09-28T12:08:50.000Z</published>
    <updated>2015-07-28T14:17:58.923Z</updated>
    <content type="html"><![CDATA[<p>原文参见：<a href="http://www.blogbus.com/navigating-logs/413323.html" target="_blank" rel="external">http://www.blogbus.com/navigating-logs/413323.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>原文参见：<a href="http://www.blogbus.com/navigating-logs/413323.html" target="_blank" rel="external">http://www.blogbus.com/navigating-logs/4]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
</feed>