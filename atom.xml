<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[On The Open Way]]></title>
  <subtitle><![CDATA[自信人生二百年，会当水击三千里！]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://navigating.github.io//"/>
  <updated>2015-09-22T02:26:21.433Z</updated>
  <id>http://navigating.github.io//</id>
  
  <author>
    <name><![CDATA[Steven Xu]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Hortonworks HDP 2.3.0]]></title>
    <link href="http://navigating.github.io/2015/Hortonworks-HDP-2-3-0/"/>
    <id>http://navigating.github.io/2015/Hortonworks-HDP-2-3-0/</id>
    <published>2015-09-22T02:23:39.000Z</published>
    <updated>2015-09-22T02:26:21.433Z</updated>
    <content type="html"><![CDATA[<p>HDP 2.3 相对于 HDP 2.2.6</p>
<ol>
<li>HBase版本变化比较大，HBase 1.1.1；</li>
<li>新增加了一些组件；</li>
<li>Ambari在配置、监控等方面有较大的升级；</li>
</ol>
<p>新增加的组件：</p>
<pre><code>1. <span class="tag">Apache</span> <span class="tag">Atlas</span> 0<span class="class">.5</span><span class="class">.0</span>
2. <span class="tag">Apache</span> <span class="tag">Calcite</span> 1<span class="class">.2</span><span class="class">.0</span>
3. <span class="tag">Apache</span> <span class="tag">Solr</span> 5<span class="class">.2</span><span class="class">.1</span>
4. <span class="tag">Cascading</span> 3<span class="class">.0</span><span class="class">.1</span>
5. <span class="tag">Cloudbreak</span> 1<span class="class">.0</span>
6. <span class="tag">SmartSense</span>
</code></pre><p>版本升级的组件：</p>
<pre><code>1. <span class="tag">Apache</span> <span class="tag">Ambari</span> 2<span class="class">.1</span>
2. <span class="tag">Apache</span> <span class="tag">Hadoop</span> 2<span class="class">.7</span><span class="class">.1</span>
3. <span class="tag">Apache</span> <span class="tag">HBase</span> 1<span class="class">.1</span><span class="class">.1</span>
4. <span class="tag">Apache</span> <span class="tag">Spark</span> 1<span class="class">.3</span><span class="class">.1</span>
5. <span class="tag">Apache</span> <span class="tag">Hive</span> 1<span class="class">.2</span><span class="class">.1</span>
6. <span class="tag">Apache</span> <span class="tag">Kafka</span> 0<span class="class">.8</span><span class="class">.2</span>
7. <span class="tag">Apache</span> <span class="tag">Phoenix</span> 4<span class="class">.4</span><span class="class">.0</span>
8. <span class="tag">Apache</span> <span class="tag">Pig</span> 0<span class="class">.15</span><span class="class">.0</span>
9. <span class="tag">Apache</span> <span class="tag">Sqoop</span> 1<span class="class">.4</span><span class="class">.6</span>
10. <span class="tag">Apache</span> <span class="tag">Oozie</span> 4<span class="class">.2</span><span class="class">.0</span>
11. <span class="tag">Apache</span> <span class="tag">Knox</span> 0<span class="class">.6</span><span class="class">.0</span>
12. <span class="tag">Apache</span> <span class="tag">Ranger</span> 0<span class="class">.5</span><span class="class">.0</span>
13. <span class="tag">Apache</span> <span class="tag">Falcon</span> 0<span class="class">.6</span><span class="class">.1</span>
14. <span class="tag">Slider</span> 0<span class="class">.80</span><span class="class">.0</span>
15. <span class="tag">Tez</span> 0<span class="class">.7</span><span class="class">.0</span>
16. <span class="tag">Storm</span> 0<span class="class">.10</span><span class="class">.0</span>
</code></pre><p>Ambari 2.1新特性：</p>
<pre><code><span class="bullet">1. </span>通过Ambari参数配置UI优化；
<span class="bullet">2. </span>各项服务Metrics可以进行监控指标添加，通过添加Widgets就可以；
<span class="bullet">3. </span>支持Hive, Pig, Files, Capacity Scheduler的User Views界面，可以通过UI执行Hive、Pig语句；
<span class="bullet">4. </span>配置 Capacity Scheduler 支持图形化了，更加方便；
<span class="bullet">5. </span>支持 SQL User View；
<span class="bullet">6. </span>支持机架感应(Rack Awareness)配置：通过Ambari来配置；
<span class="bullet">7. </span>支持 Cloudbreak；
<span class="bullet">8. </span>支持 SmartSense；
</code></pre><p>系统要求：</p>
<pre><code>1. <span class="tag">Red</span> <span class="tag">Hat</span> <span class="tag">Enterprise</span> <span class="tag">Linux</span> (<span class="tag">RHEL</span>) <span class="tag">v6</span><span class="class">.x</span> 或者 <span class="tag">Red</span> <span class="tag">Hat</span> <span class="tag">Enterprise</span> <span class="tag">Linux</span> (<span class="tag">RHEL</span>) <span class="tag">v7</span><span class="class">.x</span>
2. <span class="tag">Oracle</span> <span class="tag">JDK</span> 1<span class="class">.8</span> 64<span class="tag">-bit</span> (<span class="tag">minimum</span> <span class="tag">JDK</span> 1<span class="class">.8_40</span>) (<span class="tag">default</span>) 或者 <span class="tag">Oracle</span> <span class="tag">JDK</span> 1<span class="class">.7</span> 64<span class="tag">-bit</span> (<span class="tag">minimum</span> <span class="tag">JDK</span> 1<span class="class">.7_67</span>)
</code></pre><p>参考：<br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_RelNotes/content/ch_relnotes_v230.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_RelNotes/content/ch_relnotes_v230.html</a><br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>HDP 2.3 相对于 HDP 2.2.6</p>
<ol>
<li>HBase版本变化比较大，HBase 1.1.1；</li>
<li>新增加了一些组件；</li>
<li>Ambari在配置、监控等方面有较大的升级；</li>
</ol>
<p>新增加的组件：</p>]]>
    </summary>
    
      <category term="Ambari" scheme="http://navigating.github.io/tags/Ambari/"/>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Oozie：入门概述]]></title>
    <link href="http://navigating.github.io/2015/Oozie%EF%BC%9A%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0/"/>
    <id>http://navigating.github.io/2015/Oozie：入门概述/</id>
    <published>2015-09-18T15:24:54.000Z</published>
    <updated>2015-09-18T15:40:18.791Z</updated>
    <content type="html"><![CDATA[<h3 id="Oozie能做什么(What_Oozie_Does)">Oozie能做什么(What Oozie Does)</h3><p>Oozie是一个Java Web应用，用于Apache Hadoop的任务(jobs)调度。Oozie顺序的合并多个任务(jobs)成为一个可工作的逻辑单元。其主要是集成了Hadoop技术栈，包括YARN等，支持Apache MapReduce, Apache Pig, Apache Hive, Apache Sqoop等。Oozie使得用户能够通过Java应用或者Shell脚本的方式调度任务。<br>Oozie任务有两种基本类型</p>
<pre><code>* Oozie Workflow jobs：这种任务是一个有向无环图(DAG, <span class="keyword">Direct</span> Acyclical Graph)，并按着规则顺序的执行，即上一个<span class="keyword">Action</span>运行完成后才能运行下一个<span class="keyword">Action</span>。所以其经常不得不等待。
* Oozie Coordinator jobs：这种任务是重复性的工作流，一般被时间或者数据达到可用会被触发。
</code></pre><p>Oozie Bundle提供一个复合的方式，将多个Workflow jobs和Coordinator jobs打包合并在一起并能对它们的生命周期进行管理。</p>
<h3 id="Oozie如何工作(How_Oozie_works)">Oozie如何工作(How Oozie works)</h3><p>一个Oozie Workflow是一系列编排成有向无环图（DAG）的Action集合。控制节点定义job时间,设置开始和结束workflow的规则(rules)。这样，Oozie通过decision，fork和join节点控制工作流的执行路径。Action节点触发任务的执行。<br>Oozie触发工作流的action操作，实际上由Hadoop MapReduce去执行。Oozie利用Hadoop技术栈来均衡负载和处理失败。<br>Oozie通过回调(callback)和轮询(polling)来检测任务的是否完成。当Oozie开始一个任务(task)，它的提供了一个唯一的可以回调的HTTP URL，当这个任务完成的时候就通知这个URL。如果任务失败就调用回调URL，Oozie可以设置任务完成。<br>经常有这种需求，在规则的时间间隔内运行Oozie workflow，处理那些无法预期的有效数据或者时间。在这些情况下，Oozie Coordinator允许你根据、时间或者事件的条件对工作流触发的时机进行建模。在这些条件得到满足之后，工作流任务就就开始启动。<br>Oozie Coordinator也可以管理多个工作流，是依赖于子工作流的输出结果。子工作流程的输出将会成为下一个工作流的输入。这条链被称为“数据应用管道”(data application pipeline)。</p>
<h3 id="工作流定义">工作流定义</h3><p>定义一个Oozie工作流，两个配置文件是必须的，job.properties和workflow.xml。<br>job.properties的环境变量如下：<br>nameNode                        hdfs://mycluster:8020                HDFS地址<br>jobTracker                        localhost:8034                        jobTracker地址<br>queueName                        default                                Oozie队列<br>examplesRoot                    examples                            全局目录<br>oozie.usr.system.libpath        true                                是否加载用户的lib库<br>oozie.libpath                    share/lib/user                        用户lib库<br>oozie.wf.application.path        ${nameNode}/user/${user.name}/        Oozie流程所在的HDFS地址</p>
<p>workflow.xml示例如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span><br><span class="line">  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">  distributed with this work for additional information</span><br><span class="line">  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">  to you under the Apache License, Version 2.0 (the</span><br><span class="line">  "License"); you may not use this file except in compliance</span><br><span class="line">  with the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">       http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an "AS IS" BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License.</span><br><span class="line">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">workflow-app</span> <span class="attribute">xmlns</span>=<span class="value">"uri:oozie:workflow:0.2"</span> <span class="attribute">name</span>=<span class="value">"map-reduce-wf"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">start</span> <span class="attribute">to</span>=<span class="value">"mr-node"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">action</span> <span class="attribute">name</span>=<span class="value">"mr-node"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">map-reduce</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="title">job-tracker</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="title">name-node</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">prepare</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">delete</span> <span class="attribute">path</span>=<span class="value">"$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">prepare</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.mapper.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.oozie.example.SampleMapper<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.reducer.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.oozie.example.SampleReducer<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.map.tasks<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.input.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/text<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.output.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">map-reduce</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">ok</span> <span class="attribute">to</span>=<span class="value">"end"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">error</span> <span class="attribute">to</span>=<span class="value">"fail"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">action</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">kill</span> <span class="attribute">name</span>=<span class="value">"fail"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">message</span>&gt;</span>Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="title">message</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">kill</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">end</span> <span class="attribute">name</span>=<span class="value">"end"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="实战命令">实战命令</h3><p>上传example目录到hdfs用户oozie根目录(/user/oozie)下：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su - oozie</span><br><span class="line">cd <span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>oozie-server/doc</span><br><span class="line">hdfs dfs -put example example</span><br></pre></td></tr></table></figure></p>
<p>启动任务命令：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie <span class="string">http:</span><span class="comment">//localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run</span></span><br></pre></td></tr></table></figure></p>
<p>停止任务命令：<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie http://localhost:11000/oozie -kill <span class="number">0000002-150</span><span class="number">914143759473</span>-oozie-oozi-W</span><br></pre></td></tr></table></figure></p>
<h3 id="Oozie_Web_UI效果图：">Oozie Web UI效果图：</h3><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_001.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_002.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_003.JPG" alt="这是一张图片"></p>
<p>参考：<br><a href="http://hortonworks.com/hadoop/oozie/" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/</a><br><a href="http://oozie.apache.org/" target="_blank" rel="external">http://oozie.apache.org/</a><br><a href="http://hortonworks.com/hadoop/oozie/#blog" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/#blog</a><br><a href="http://hortonworks.com/hadoop/oozie/#forums" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/#forums</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/</a><br><a href="https://github.com/yahoo/oozie" target="_blank" rel="external">https://github.com/yahoo/oozie</a><br>书籍：<br>《Apache Oozie: The Workflow Scheduler for Hadoop》<br><a href="http://book.douban.com/subject/26348732/" target="_blank" rel="external">http://book.douban.com/subject/26348732/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="Oozie能做什么(What_Oozie_Does)">Oozie能做什么(What Oozie Does)</h3><p>Oozie是一个Java Web应用，用于Apache Hadoop的任务(jobs)调度。Oozie顺序的合并多个任务(jobs)成为一个]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Oozie" scheme="http://navigating.github.io/tags/Oozie/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据技术百度指数201508]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%99%BE%E5%BA%A6%E6%8C%87%E6%95%B0201508/"/>
    <id>http://navigating.github.io/2015/大数据技术百度指数201508/</id>
    <published>2015-09-16T06:17:00.000Z</published>
    <updated>2015-09-16T09:35:47.738Z</updated>
    <content type="html"><![CDATA[<p>关于大数据技术点的搜索指数，这里只关注一下百度指数的结果。</p>
<ol>
<li>当前最热的依次是：Hadoop、Redis、MongoDB、Spark、Storm。可以看到国内Redis、MongoDB的用户很多，有时候比HBase都热，可见热度之高。</li>
<li>从趋势来看，Spark是最强劲的，Hadoop、Redis表现都很不错。</li>
<li>从搜索热词看，当前主要表现在入门介绍、安装、教程的需求量非常大。对于使用中的问题、优化议题还不多，对于监控更少。</li>
<li>当前搜索热词来源Top5的城市：北京、上海、深圳、广州、南京。北京、珠三角、长三角，同时整体上中部IT发展的相对不错。</li>
<li>用户人群的年龄主要是30～39之间，几乎达到50%，其次是20～29，几乎40%，其他年龄段的人很少。</li>
</ol>
<p>首先是意外的收获，就是发现Redis和MongoDB在国内这么火，热词竟然超过了Hadoop了，也许是两个比较简单易用，不像Hadoop如今已经发展成为了一个大家族了。<br>第二个意外是，发现Cloudera、Hortonworks、CHD、HDP尽然不是指数热词，看来一般印象的大数据技术等于Hadoop真的是偏见啊。<br>第三个意外，发现中部城市IT整体发展的不错，除了上海、南京，还包括：成都、重庆、武汉、长沙、西安、郑州。</p>
<p>趋势研究<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_01.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_02.JPG" alt="这是一张图片"></p>
<p>需求图谱<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_03.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_04.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_05.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_06.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_07.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_08.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_09.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_10.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_11.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_12.JPG" alt="这是一张图片"></p>
<p>人群画像<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_13.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_14.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_15.JPG" alt="这是一张图片"></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>关于大数据技术点的搜索指数，这里只关注一下百度指数的结果。</p>
<ol>
<li>当前最热的依次是：Hadoop、Redis、MongoDB、Spark、Storm。可以看到国内Redis、MongoDB的用户很多，有时候比HBase都热，可见热度之高。</li>
<l]]>
    </summary>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Redis" scheme="http://navigating.github.io/tags/Redis/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="Storm" scheme="http://navigating.github.io/tags/Storm/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201508]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201508/"/>
    <id>http://navigating.github.io/2015/大数据动态之201508/</id>
    <published>2015-09-07T08:13:04.000Z</published>
    <updated>2015-09-07T08:17:00.928Z</updated>
    <content type="html"><![CDATA[<p>Cloudera：<br>Cloudera Navigator路线图<br><a href="http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-governance-cloudera-navigator-roadmap/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-governance-cloudera-navigator-roadmap/</a><br>NoSQL性能测试开放标准套件YCSB加入Cloudera实验室项目中<br><a href="http://blog.cloudera.com/blog/2015/08/ycsb-the-open-standard-for-nosql-benchmarking-joins-cloudera-labs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/ycsb-the-open-standard-for-nosql-benchmarking-joins-cloudera-labs/</a><br>Spark在TripAdvisor的机器学习应用案例<br><a href="http://blog.cloudera.com/blog/2015/08/using-apache-spark-for-massively-parallel-nlp-at-tripadvisor/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/using-apache-spark-for-massively-parallel-nlp-at-tripadvisor/</a><br>CDH支持Mesos<br><a href="http://blog.cloudera.com/blog/2015/08/how-to-run-apache-mesos-on-cdh/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/how-to-run-apache-mesos-on-cdh/</a><br>HBase开始支持HBase-Spark模块<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br>Navigator Encrypt开始支持YARN Container安全<br><a href="http://blog.cloudera.com/blog/2015/08/how-to-secure-yarn-containers-with-cloudera-navigator-encrypt/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/how-to-secure-yarn-containers-with-cloudera-navigator-encrypt/</a><br>基于Kafka和HBase的近实时集成架构案例: Santanders<br><a href="http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/</a> </p>
<p>Hortonworks:<br>Microsoft Azure Gallery开始支持HDP 2.3<br><a href="http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/</a><br>Microsoft Azure支持Spark<br><a href="http://hortonworks.com/blog/microsoft-and-hortonworks-do-spark-in-the-cloud/" target="_blank" rel="external">http://hortonworks.com/blog/microsoft-and-hortonworks-do-spark-in-the-cloud/</a><br>Storm的容错Nimbus架构<br><a href="http://hortonworks.com/blog/fault-tolerant-nimbus-in-apache-storm/" target="_blank" rel="external">http://hortonworks.com/blog/fault-tolerant-nimbus-in-apache-storm/</a> </p>
<p>MapR<br>Spark Streaming with HBase<br><a href="https://www.mapr.com/blog/spark-streaming-hbase" target="_blank" rel="external">https://www.mapr.com/blog/spark-streaming-hbase</a><br>Apache Drill Architecture: The Ultimate Guide<br><a href="https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide" target="_blank" rel="external">https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide</a><br>HBase架构深度剖析<br><a href="https://www.mapr.com/blog/in-depth-look-hbase-architecture" target="_blank" rel="external">https://www.mapr.com/blog/in-depth-look-hbase-architecture</a><br>HBase Schema设计指导<br><a href="https://www.mapr.com/blog/guidelines-hbase-schema-design" target="_blank" rel="external">https://www.mapr.com/blog/guidelines-hbase-schema-design</a><br>如何利用Spark进行机器学习的并行与交互处理<br><a href="https://www.mapr.com/blog/parallel-and-iterative-processing-machine-learning-recommendations-spark" target="_blank" rel="external">https://www.mapr.com/blog/parallel-and-iterative-processing-machine-learning-recommendations-spark</a></p>
<p>Databricks<br>Spark 1.5发布，包含Tungsten，其利用代码生成技术和Cache感知算法，大幅度提升运行时的性能：<br><a href="https://databricks.com/blog/2015/08/18/spark-1-5-preview-now-available-in-databricks.html" target="_blank" rel="external">https://databricks.com/blog/2015/08/18/spark-1-5-preview-now-available-in-databricks.html</a><br><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" target="_blank" rel="external">https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html</a></p>
<p>mongoDB<br>mongoDB 2.x版本发布了2个，3.x发布了3个：<br><a href="http://blog.mongodb.org/post/128063809158/mongodb-306-rc2-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/128063809158/mongodb-306-rc2-is-released</a><br><a href="http://blog.mongodb.org/post/127802855483/mongodb-317-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/127802855483/mongodb-317-is-released</a><br><a href="http://blog.mongodb.org/post/126436298628/mongodb-2611-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/126436298628/mongodb-2611-is-released</a><br><a href="http://blog.mongodb.org/post/126436227873/mongodb-306-rc0-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/126436227873/mongodb-306-rc0-is-released</a><br><a href="http://blog.mongodb.org/post/125850939688/mongodb-2611-rc0-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/125850939688/mongodb-2611-rc0-is-released</a> </p>
<p>Redis</p>
<p>参考：<br>NoSQL大数据分类<br><a href="http://www.nosql-database.org/" target="_blank" rel="external">http://www.nosql-database.org/</a><br>Autodesk基于Mesos的通用事件系统架构<br><a href="http://www.csdn.net/article/2015-08-27/2825550" target="_blank" rel="external">http://www.csdn.net/article/2015-08-27/2825550</a><br>QingCloud推出Spark即服务<br><a href="http://mt.sohu.com/20150826/n419752360.shtml" target="_blank" rel="external">http://mt.sohu.com/20150826/n419752360.shtml</a><br>Spark大数据分析框架的核心部件<br><a href="http://my.oschina.net/u/2306127/blog/489024?p=1" target="_blank" rel="external">http://my.oschina.net/u/2306127/blog/489024?p=1</a><br>Hadoop和大数据：60款顶级开源工具<br><a href="http://os.51cto.com/art/201508/487936.htm" target="_blank" rel="external">http://os.51cto.com/art/201508/487936.htm</a><br>【微信分享】QingCloud周小四：Spark学习简谈<br><a href="http://www.csdn.net/article/2015-08-07/2825404" target="_blank" rel="external">http://www.csdn.net/article/2015-08-07/2825404</a><br>【微信分享】李滔：搜狐基于Spark的新闻和广告推荐实战<br><a href="http://www.csdn.net/article/2015-07-31/2825353" target="_blank" rel="external">http://www.csdn.net/article/2015-07-31/2825353</a><br>【微信分享】王团结：七牛是如何搞定每天500亿条日志的<br><a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br>对七牛云存储日志处理的思考<br><a href="http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/" target="_blank" rel="external">http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/</a><br>STORM在线业务实践-集群空闲CPU飙高问题排查<br><a href="http://daiwa.ninja/index.php/2015/07/18/storm-cpu-overload/" target="_blank" rel="external">http://daiwa.ninja/index.php/2015/07/18/storm-cpu-overload/</a><br>Spark与Flink：对比与分析<br><a href="http://www.csdn.net/article/2015-07-16/2825232" target="_blank" rel="external">http://www.csdn.net/article/2015-07-16/2825232</a><br>一共81个，开源大数据处理工具汇总（上）<br><a href="http://www.36dsj.com/archives/24852" target="_blank" rel="external">http://www.36dsj.com/archives/24852</a><br>一共81个，开源大数据处理工具汇总（下）<br><a href="http://home.hylanda.com/show_26_11558.html" target="_blank" rel="external">http://home.hylanda.com/show_26_11558.html</a></p>
<p>总结：</p>
<pre><code><span class="bullet">1. </span>Cloudera和Hortonworks都开始注重数据管理和数据治理，Cloudera是通过增强Cloudera Navigator来实现，Hortonworks通过引入Informatic组件Fabric来实现。
<span class="bullet">2. </span>Spark 1.5发布；
<span class="bullet">3. </span>HBase、Cassandra是Column Families/Wide Column Store；
<span class="bullet">4. </span>MongoDB是Document Store；
<span class="bullet">5. </span>Redis是Key Value/Tuple Store；
<span class="bullet">6. </span>Neo4J是Graph Databases；
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>Cloudera：<br>Cloudera Navigator路线图<br><a href="http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-gov]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="mongoDB" scheme="http://navigating.github.io/tags/mongoDB/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[SparkOnHBase(Cloudera)]]></title>
    <link href="http://navigating.github.io/2015/SparkOnHBase-Cloudera/"/>
    <id>http://navigating.github.io/2015/SparkOnHBase-Cloudera/</id>
    <published>2015-08-18T05:35:51.000Z</published>
    <updated>2015-08-18T05:46:25.533Z</updated>
    <content type="html"><![CDATA[<p>2014年2月4日，Cloudera宣布CDH支持Spark，在CDH 4.4中引入Spark 0.9。<br><a href="http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/" target="_blank" rel="external">http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/</a><br>在引入的时候强调了三点：</p>
<pre><code><span class="bullet">1. </span>Machine Learning
<span class="bullet">2. </span>Spark Streaming
<span class="bullet">3. </span>Faster Batch
</code></pre><p>2014年7月，在github上创建了Apache HBase与Spark的集成项目SparkOnHBase<br><a href="http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/" target="_blank" rel="external">http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/</a><br><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="external">https://github.com/cloudera-labs/SparkOnHBase</a><br>当前SparkOnHBase主要集中在这几个方面的功能改进：</p>
<pre><code>1. 在MR的map或者reduce阶段对HBase的全量访问(Full Access)；
2. 支持bulk <span class="operator"><span class="keyword">load</span>；
<span class="number">3.</span> 支持<span class="keyword">get</span>, put, <span class="keyword">delete</span>等bulk操作(bulk operation)；
<span class="number">4.</span> 支持成为<span class="keyword">SQL</span> <span class="keyword">engines</span>。</span>
</code></pre><p>2015年8月SparkOnHBase项目有了里程碑似的进展，被提交到HBase的主干(trunk)上，模块名为HBase-Spark Module，HBASE-13992 。<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br><a href="https://issues.apache.org/jira/browse/HBASE-13992" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-13992</a><br>HBase-Spark module相比于SparkOnHBase在架构上没有什么变化：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Cloudera_Spark_2015_01.png" alt="这是一张图片"><br>在具体实现上当前有三点改进：</p>
<pre><code><span class="bullet">1. </span>使用了全新的HBase 1.0+的API；
<span class="bullet">2. </span>从RDD和DStream functions操作HBase的直接支持；
<span class="bullet">3. </span>简化 foreach 和 map functions；
</code></pre><p>计划工作有两项：</p>
<pre><code><span class="bullet">1. </span>Spark-HBase Module支持bulkload；
<span class="bullet">2. </span>Spark-HBase Module支持Spark DataFrame DataSource；
</code></pre><p><a href="https://issues.apache.org/jira/browse/HBASE-14150" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-14150</a><br><a href="https://issues.apache.org/jira/browse/HBASE-14181" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-14181</a> </p>
<p>实际上集成Spark作为计算引擎的项目还有Hive和Pig：<br><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/spark.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/spark.html</a><br><a href="http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/</a><br><a href="http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/" target="_blank" rel="external">http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/</a> </p>
<p>参考：<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="external">https://github.com/cloudera-labs/SparkOnHBase</a><br><a href="http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>2014年2月4日，Cloudera宣布CDH支持Spark，在CDH 4.4中引入Spark 0.9。<br><a href="http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/" targ]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Cloudera" scheme="http://navigating.github.io/tags/Cloudera/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《Hadoop生态技术在阿里全网商品搜索实战》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8AHadoop%E7%94%9F%E6%80%81%E6%8A%80%E6%9C%AF%E5%9C%A8%E9%98%BF%E9%87%8C%E5%85%A8%E7%BD%91%E5%95%86%E5%93%81%E6%90%9C%E7%B4%A2%E5%AE%9E%E6%88%98%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《Hadoop生态技术在阿里全网商品搜索实战》/</id>
    <published>2015-08-17T04:41:00.000Z</published>
    <updated>2015-08-18T08:55:01.529Z</updated>
    <content type="html"><![CDATA[<p>资料参见文档：<a href="http://wenku.it168.com/d_001428550.shtml" target="_blank" rel="external">http://wenku.it168.com/d_001428550.shtml</a><br>版本:</p>
<pre><code><span class="bullet">1. </span>Hadoop: 基于 Hadoop 2.2 的阿里定制版
<span class="bullet">2. </span>HBase: 基于 HBase 0.94 的阿里定制版
</code></pre><p>部署方式：</p>
<pre><code><span class="bullet">1. </span>服务总数近1000台，分2个集群；
<span class="bullet">2. </span>Hadoop/HBase共同部署；
</code></pre><p>分析：服务器数量可能是2014年初的数据；HBase部署方式可能是RS和DN部署在同一个节点上。<br>服务器配置：</p>
<pre><code><span class="bullet">1. </span>CPU：24/32 Cores
<span class="bullet">2. </span>Memory：48G/96G
<span class="bullet">3. </span>Disk：12 <span class="bullet">* 1T SATA Disk 或者 12 *</span> 2T SATA Disk
</code></pre><p>分析：服务器配置计算能力比较强，内存和磁盘配置都不是很高。<br>大数据组件：</p>
<pre><code><span class="bullet">1. </span>HDFS + YARN
<span class="bullet">2. </span>HBase
<span class="bullet">3. </span>MR
<span class="bullet">4. </span>iStream
<span class="bullet">5. </span>Spark
<span class="bullet">6. </span>HQueue
<span class="bullet">7. </span>Phoenix
<span class="bullet">8. </span>OpenTSDB
<span class="bullet">9. </span>Zookeeper
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_01.JPG" alt="这是一张图片"><br>分析：</p>
<pre><code>* 对于基于HBase的HQueue是一个创新，当前没有看到更多的资料，无法和Kafka对比。(在性能和TPC上可能Kafka更强大，但通过对HBase的复用做出Queue，很赞。)
* iStream是一个Steaming <span class="function_start"><span class="keyword">on</span></span> YARN的产品，从架构上看很类似storm的设计理念。
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_02.JPG" alt="这是一张图片"><br>HBase<br>HBase应用</p>
<pre><code><span class="number">1</span>. <span class="function"><span class="title">Phoenix</span><span class="params">(SQL on HBase)</span></span>
<span class="number">2</span>. <span class="function"><span class="title">OpenTSDB</span><span class="params">(Metrics on HBase)</span></span>
<span class="number">3</span>. <span class="function"><span class="title">HQueue</span><span class="params">(Queue on HBase)</span></span>
</code></pre><p>分析：</p>
<pre><code><span class="bullet">* </span>在HBase集群上运行了Phoenix、OpenTSDB、HQueue三种应用，因此HBase具有作为一种数据存储的基础设施的能力。
</code></pre><p>HBase网页库存储方案</p>
<pre><code>1. 版本从0<span class="class">.25</span>、0<span class="class">.26</span>、0<span class="class">.90</span>、0<span class="class">.92</span>、0<span class="class">.94</span>、0<span class="class">.98</span>逐步升级的。
2. <span class="tag">HBase</span>集群规模从30多台持续升级到300多台。
3. <span class="tag">HBase</span> <span class="tag">Region</span>个数从 1<span class="tag">K</span> 增长到 20<span class="tag">K</span>。
4. 网页数量从 十亿 增长到 百亿。
</code></pre><p>存储业务数据的CF如下：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_05.JPG" alt="这是一张图片"><br>在HBase/Hadoop的I/O上的优化如下：</p>
<pre><code><span class="bullet">1. </span>Compression：Snappy/Gzip
<span class="bullet">2. </span>Block Encoding：Diff
<span class="bullet">3. </span>Block Size：64KB - 1MB
<span class="bullet">4. </span>Block Cache：InMemory
<span class="bullet">5. </span>Bloom Filter：ROW
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_06.JPG" alt="这是一张图片"><br>HBase Coprocessor应用<br>在网页库中使用了三种Coprocessor：</p>
<pre><code><span class="bullet">1. </span>Trace Coprocessor
<span class="bullet">2. </span>Clone Coprocessor
<span class="bullet">3. </span>Incremental Coprocessor
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_07.JPG" alt="这是一张图片"><br>分析：</p>
<pre><code><span class="keyword">*</span> 如果HBase集群就是两个集群中的一个，那么裸存储容量最大为：12 <span class="keyword">*</span> 2T <span class="keyword">*</span> 300 = 7200T = 7.2P，如果考虑到压缩、复制因子、数据冗余、容量冗余，可以存储有效数据约为：8P 数据。 
<span class="keyword">*</span> 平均每台服务器运行的Region个数：20K/300 = 67 个，这个数字比较符合HBase官方推荐的值。
<span class="keyword">*</span> Compression方法用了snappy和gzip两种。CF访问频繁，使用snappy，速度快；Raw CF访问较少，使用gzip，压缩比高。
<span class="keyword">*</span> Block Encoding使用Diff，0.98后改用PrefixTree；
<span class="keyword">*</span> Block Size的大小为 64KB - 1MB 
</code></pre><p>实时处理架构<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_08.JPG" alt="这是一张图片"><br>分析</p>
<pre><code><span class="subst">*</span> Metrics实时采集的流程大约是：HBase <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iStream <span class="subst">-&gt; </span>OpenTSDB <span class="keyword">on</span> HBase
<span class="subst">*</span> 流处理的全流程：HBase <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iStream <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iSearch/iStream
<span class="subst">*</span> 参见前文分析，猜测iStream是一个类似Storm的YARN框架。
</code></pre><p>关于阿里搜索自研的iStream的架构与文档参加如下：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_09.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_10.JPG" alt="这是一张图片"></p>
<p><a href="http://www.infoq.com/cn/news/2014/09/hadoop-alibaba-yarn" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/09/hadoop-alibaba-yarn</a><br><a href="http://club.alibabatech.org/resource_detail.htm?topicId=140" target="_blank" rel="external">http://club.alibabatech.org/resource_detail.htm?topicId=140</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>资料参见文档：<a href="http://wenku.it168.com/d_001428550.shtml" target="_blank" rel="external">http://wenku.it168.com/d_001428550.shtml</a><br>]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="HQueue" scheme="http://navigating.github.io/tags/HQueue/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="iStream" scheme="http://navigating.github.io/tags/iStream/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop发行版(2015第二季)]]></title>
    <link href="http://navigating.github.io/2015/Hadoop%E5%8F%91%E8%A1%8C%E7%89%88(2015%E7%AC%AC%E4%BA%8C%E5%AD%A3)/"/>
    <id>http://navigating.github.io/2015/Hadoop发行版(2015第二季)/</id>
    <published>2015-08-11T14:40:02.000Z</published>
    <updated>2015-08-11T14:56:06.872Z</updated>
    <content type="html"><![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" target="_blank" rel="external">http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support</a><br>其中在最有名为人所知的三家：<br>1.Cloudera<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_1.JPG" alt="这是一张图片"><br>2.Hortonwork<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_2.JPG" alt="这是一张图片"><br>3.MapR<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_3.JPG" alt="这是一张图片"><br>这三个厂商之中，MapR最为封闭；Hortonworks最为开放，产品线全开源，在线文档比较丰富。国内使用Cloudera CDH和Hortonworks的应该是最多的。<br>国内市场当前有两家也非常有竞争力，一家是Huawei，一家是星环科技。<br>4.Huawei FusionInsight<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_7.JPG" alt="这是一张图片"><br>5.星环科技TDH，TDH对Spark的支持据说非常不错的，有良好的性能表现。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_6.JPG" alt="这是一张图片"><br>准实时计算框架/即席查询<br>1.CDH的框架有：Impala + Spark；<br>2.HDP的框架有：Tez + Spark；<br>3.MapR的框架有：Drill + Tez + Spark。<br>关于Spark：<br>2014年大数据最热门的技术路线就是算是Spark了，而且得力于Spark不遗余力的推广和快速成长。Cloudera是最早支持Spark，也是最激进的。下图即是Spark在Cloudera产品线中的定位：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_4.JPG" alt="这是一张图片"><br>实际上基于Hadoop的快速计算框架的发展才刚刚开始，社区中已经有如下几种：<br>1.Spark/Shark<br>2.Hortonworks Tez/Stinger<br>3.Cloudera Impala<br>4.Apache Drill<br>5.Apache Flink<br>6.Apache Nifi<br>7.Facebook Presto</p>
<p>SQL on Hadoop<br>SQL on Hadoop的发展主要是传统的SQL过于强大，人才库非常庞大，从Hadoop出现的第一天就在SQL发力。当前技术路线上更是百花齐放，这里从开源和商业产品来说。<br>Open Source</p>
<pre><code><span class="bullet">1. </span>Apache Hive(Hive on MR)
<span class="bullet">2. </span>Hortonworks Tez/Stinger(Hive on Tez)
<span class="bullet">3. </span>Cloudera Impala
<span class="bullet">4. </span>Shark
<span class="bullet">5. </span>Spark SQL
<span class="bullet">6. </span>Apache Drill - MapR
<span class="bullet">7. </span>Facebook Presto
<span class="bullet">8. </span>Apache Phoenix(on HBase) - Saleforce
<span class="bullet">9. </span>Apache Kylin
<span class="bullet">10. </span>Apache Tajo - (Database Lab, Korea University)
<span class="bullet">11. </span>Cascading Lingual - (Cascading, Optiq)
<span class="bullet">12. </span>Dato (GraphLab) - Dato
</code></pre><p>Commercial</p>
<pre><code><span class="bullet">1. </span>EMC HAWQ
<span class="bullet">2. </span>IBM BigSQL
<span class="bullet">3. </span>TERADATA SQL-H
<span class="bullet">4. </span>Hadapt/HadoopDB
<span class="bullet">5. </span>Transwarp Inceptor
</code></pre><p>在开源领域里面，当前比受追捧的主要是：Hive、Impala、Spark、Phoenix。</p>
<p>参考：<br>SQL on Hadoop开源项目总结<br><a href="http://segmentfault.com/a/1190000002799235" target="_blank" rel="external">http://segmentfault.com/a/1190000002799235</a><br>如何选择满足需求的SQL on Hadoop系统<br><a href="http://www.searchbi.com.cn/showcontent_89816.htm" target="_blank" rel="external">http://www.searchbi.com.cn/showcontent_89816.htm</a><br>2015Hadoop技术峰会演讲速记3： 基于Transwarp Stream和Discover的实时大数据人流密度估计<br><a href="http://www.transwarp.cn/news/detail?id=70" target="_blank" rel="external">http://www.transwarp.cn/news/detail?id=70</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="SQL on Hadoop" scheme="http://navigating.github.io/tags/SQL-on-Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《七牛是如何搞定每天500亿条日志的》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8A%E4%B8%83%E7%89%9B%E6%98%AF%E5%A6%82%E4%BD%95%E6%90%9E%E5%AE%9A%E6%AF%8F%E5%A4%A9500%E4%BA%BF%E6%9D%A1%E6%97%A5%E5%BF%97%E7%9A%84%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《七牛是如何搞定每天500亿条日志的》/</id>
    <published>2015-08-10T06:48:00.000Z</published>
    <updated>2015-08-10T14:12:54.949Z</updated>
    <content type="html"><![CDATA[<p>七牛是如何搞定每天500亿条日志的 <a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/qiniu_01.jpg" alt=""><br>日志处理的大致分为三步：</p>
<pre><code><span class="bullet">1. </span>日志采集，主要是通过Agent和Flume；
<span class="bullet">2. </span>日志流转，主要是通过Kafka；
<span class="bullet">3. </span>日志计算，主要是通过Spark Streaming作为计算引擎；
</code></pre><p>大致的处理流程：</p>
<pre><code><span class="number">1.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>HDFS <span class="subst">-&gt; </span>mongoDB
<span class="number">2.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>Spark <span class="subst">-&gt; </span>mongoDB
<span class="number">3.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>Spark <span class="subst">-&gt; </span>opentsdb 
</code></pre><p>流程3只是见于图上，文字上没有任何提到。<br>在日志采集中，通过Agent将业务应用和日志采集进行了分离，采取了Agent主动来拉的模式。专门强调了Agent 的设计需求：<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">每台机器上会有一个Agent去同步这些日志，这是个典型的队列模型，业务进程在不断的push，Agent在不停的pop。Agent需要有记忆功能，用来保存同步的位置(<span class="command">offset</span>)，这样才尽可能保证数据准确性，但不可能做到完全准确。由于发送数据和保存<span class="command">offset</span>是两个动作，不具有事务性，不可避免的会出现数据不一致性情况，通常是发送成功后保存<span class="command">offset</span>，那么在Agent异常退出或机器断电时可能会造成多余的数据。</span><br><span class="line">在这里，Agent需要足够轻，这主要体现在运维和逻辑两个方面。Agent在每台机器上都会部署，运维成本、接入成本是需要考虑的。Agent不应该有解析日志、过滤、统计等动作，这些逻辑应该给数据消费者。倘若Agent有较多的逻辑，那它是不可完成的，不可避免的经常会有升级变更动作。</span><br></pre></td></tr></table></figure></p>
<p>为什么Agent没有直接将日志发送给Kafka，而是通过Flume来做：<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">具体架构上，Agent并没把数据直接发送到Kafka，在Kafka前面有层由Flume构成的<span class="keyword">forward</span>。这样做有两个原因：</span><br><span class="line"><span class="number">1</span>. Kafka的API对非JVM系的语言支持很不友好，<span class="keyword">forward</span>对外提供更加通用的http接口。</span><br><span class="line"><span class="number">2</span>. <span class="keyword">forward</span>层可以做路由、Kafka topic和Kafka partition key等逻辑，进一步减少Agent端的逻辑。</span><br></pre></td></tr></table></figure></p>
<p>Kafka使用建议<br>1.Topic划分。尽量通过划分Topic分离不同类型的数据；<br>2.Kafka partition数目直接关系整体的吞吐量。3个Partition能够跑满一块磁盘的IO。<br>3.Partition key设计。partition key选择不当，可能会造成数据倾斜。在对数据有顺序性要求才需使用partition key。Kafka的producer sdk在没指定partition key时，在一定时间内只会往一个partition写数据，这种情况下当producer数少于partition数也会造成数据倾斜，可以提高producer数目来解决这个问题。<br>实时计算Spark Streaming<br>1.当前Spark只用作统计，没有进行迭代计算(DAG)。场景比较简单。<br>2.Spark Streaming从Kafka中读数据，统计完结果如mongoDB。可以理解是Spark Streaming + mongoDB的应用。<br>3.Spark Streaming对存储计算结果的数据库tps要求较高。比如有10万个域名需要统计流量，batch interval为10s，每个域名有4个相关统计项，算下来平均是4万 tps，考虑到峰值可能更高，固态硬盘上的mongo也只能抗1万tps，后续我们会考虑用redis来抗这么高的tps。难道Redis能够支持很高的TPS？<br>4.有状态的Task的挑战：有外部状态的task逻辑上不可重入的，当开启speculation参数时候，可能会造成计算的结果不准确。说个简单的例子。这个任务，如果被重做了，会造成落入mongo的结果比实际多。有状态的对象生命周期不好管理，这种对象不可能做到每个task都去new一个。我们的策略是一个JVM内一个对象，同时在代码层面做好并发控制。<br>七牛数据平台规模<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">线上的规模：Flume ＋ Kafka ＋ Spark8台高配机器，日均500亿条数据，峰值80万tps。</span><br></pre></td></tr></table></figure></p>
<p>因此，<br>1.如果是Flume/Kafka/Spark共享同一个物理集群，硬件压力如何？<br>2.如果每条日志 0.1K，那么每天总数据量 50G <em> 0.1K = 5T，每个节点每秒 5T/24</em>3600/8 = 7.23M。 </p>
<p>参考：<br>【微信分享】王团结：七牛是如何搞定每天500亿条日志的<br><a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br>对七牛云存储日志处理的思考<br><a href="http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/" target="_blank" rel="external">http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>七牛是如何搞定每天500亿条日志的 <a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/201]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Flume" scheme="http://navigating.github.io/tags/Flume/"/>
    
      <category term="Kafka" scheme="http://navigating.github.io/tags/Kafka/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《腾讯在Spark上的应用与实践优化》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8A%E8%85%BE%E8%AE%AF%E5%9C%A8Spark%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%E4%BC%98%E5%8C%96%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《腾讯在Spark上的应用与实践优化》/</id>
    <published>2015-08-07T08:28:42.000Z</published>
    <updated>2015-08-07T08:37:53.504Z</updated>
    <content type="html"><![CDATA[<p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.net/detail/happytofly/8637461</a></p>
<p>TDW: Tencent Distributed Data Warehouse，腾讯分布式数据仓库；<br>GAIA：腾讯自研的基于YARN定制化和优化的资源管理系统；<br>Lhoste：腾讯自研的作业的工作流调度系统，类似于Oozie；<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_1.JPG" alt=""></p>
<p>TDW集群规模：</p>
<pre><code><span class="bullet">1. </span>Gaia集群节点数：8000+；
<span class="bullet">2. </span>HDFS的存储空间：150PB+；
<span class="bullet">3. </span>每天新增数据：1PB+；
<span class="bullet">4. </span>每天任务数：1M+；
<span class="bullet">5. </span>每天计算量：10PB+；
</code></pre><p>Spark集群：</p>
<pre><code><span class="bullet">1. </span>Spark部署在Gaia之上，即是Spark on YARN模式，每个节点是 24 cores 和 60G 内存；
<span class="bullet">2. </span>底层存储包括：HDFS、HBase、Hive、MySQL；
<span class="bullet">3. </span>作业类型，包括：ETL、SparkSQL、Machine Learning、Graph Compute、Streaming；
<span class="bullet">4. </span>每天任务数，10K+；
<span class="bullet">5. </span>腾讯从2013年开始引入Spark 0.6，已经使用2年了；
</code></pre><p>Spark的典型应用：</p>
<pre><code><span class="bullet">1. </span>预测用户的广告点击概率；
<span class="bullet">2. </span>计算两个好友间的共同好友数；
<span class="bullet">3. </span>用于ETL的SparkSQL和DAG任务；
</code></pre><p>Case 1: 预测用户的广告点击概率<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_4.JPG" alt=""></p>
<pre><code><span class="number">1</span>. 数据是通过<span class="function"><span class="title">DCT</span><span class="params">(Data Collect Tool)</span></span>推送到HDFS上，然后Spark直接将HDFS数据导入到 RDD&amp;Cache；
<span class="number">2</span>. <span class="number">60</span>次迭代计算的时间为<span class="number">10</span>～<span class="number">15</span>分钟，即每次迭代<span class="number">10</span>～<span class="number">15</span>秒；
</code></pre><p>Case 2: 计算两个好友间的共同好友数</p>
<pre><code>1. 根据shuffle数量来确定partition数量；
2. 尽量使用sort-based shuffle，减少reduce的内存使用；
3. 当连接超时后选择重试来减少executor丢失的概率；
4. 避免executor被YARN给<span class="operator"><span class="keyword">kill</span>掉，设置 spark.yarn.executor.memoryoverhead
<span class="number">5.</span> 执行语句 <span class="keyword">INSERT</span> <span class="keyword">TABLE</span> test_result <span class="keyword">SELECT</span> t3.d, <span class="keyword">COUNT</span>(*) FROＭ( <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> a, b <span class="keyword">FROM</span> join_1 ) t1 <span class="keyword">JOIN</span> （<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> b, c <span class="keyword">FROM</span> join_2 ) t2 <span class="keyword">ON</span> (t1.a = t2.c) <span class="keyword">JOIN</span> (<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c, d <span class="keyword">FROM</span> c, d <span class="keyword">FROM</span> join_3 ) t3 <span class="keyword">ON</span> (t2.b = t3.d) <span class="keyword">GROUP</span> <span class="keyword">BY</span> t3.d 使用Hive需要<span class="number">30</span>分钟，使用SparkSQL需要<span class="number">5</span>分钟；
<span class="number">6.</span> 当有小表时使用broadcase <span class="keyword">join</span>代替Common <span class="keyword">join</span>；
<span class="number">7.</span> 尽量使用ReduceByKey代替GroupByKey；
<span class="number">8.</span> 设置spark.serializer = org.apache.spark.serializer.KryoSerializer；
<span class="number">9.</span> 使用YARN时，设置spark.shuffle.service.enabled = <span class="literal">true</span>；
<span class="number">10.</span> 在早期版本中Spark通过启动参数固定executor的数量，当前支持动态资源扩缩容特性

    * spark.dynamicAllocation.enabled = <span class="literal">true</span>
    * spark.dynamicAllocation.executorIdleTimeout = <span class="number">120</span>
    * spark.dynamicAllocation.schedulerBacklogTimeout = <span class="number">10</span>
    * spark.dynamicAllocation.minExecutors/maxExecutors

<span class="number">11.</span> 当申请固定的executors时且task数大于executor数时，存在着资源的空闲状态。</span>
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_5.JPG" alt=""><br>&lt;完&gt;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[13~14年收集的大数据的一些技术架构图]]></title>
    <link href="http://navigating.github.io/2015/13-14%E5%B9%B4%E6%94%B6%E9%9B%86%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E5%9B%BE/"/>
    <id>http://navigating.github.io/2015/13-14年收集的大数据的一些技术架构图/</id>
    <published>2015-08-05T05:16:53.000Z</published>
    <updated>2015-08-05T05:38:56.350Z</updated>
    <content type="html"><![CDATA[<p>1 Big Data Solution</p>
<p>1.1 HP</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_01.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_02.png" alt=""></p>
<p>1.2 Oracle</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_03.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_04.JPG" alt=""></p>
<p>1.3 IBM</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_05.jpg" alt=""></p>
<p>1.4 Microsoft</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_06.png" alt=""></p>
<p>1.5 Huawei</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_07.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_08.jpg" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_09.jpg" alt=""></p>
<p>2 Big Data on Cloud</p>
<p>2.1 Amazon AWS</p>
<p>2.1.1 Netflix BigData on AWS<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_10.png" alt=""></p>
<p>2.2 Microsoft Azure</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_11.png" alt=""></p>
<p>2.3 Facebook</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_12.png" alt=""></p>
<p>2.4 Linkedin</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_13.png" alt=""></p>
<p>2.5 Twitter</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_14.png" alt=""></p>
<p>2.6 Alibaba/Taobao</p>
<p>2.6.1 淘宝数据魔方<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_15.png" alt=""></p>
<p>2.6.2 阿里大数据应用平台<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_16.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_17.png" alt=""></p>
<p>2.6.3 阿里搜索实时流计算<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_18.png" alt=""></p>
<p>2.7 Tencent</p>
<p>2.7.1 腾讯大规模Hadoop集群TDW<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_19.png" alt=""></p>
<p>2.7.2 腾讯实时计算平台 广点通<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_20.png" alt=""></p>
<p>2.8 JD(京东)</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_21.png" alt=""></p>
<p>2.9 CMCC(中国移动)</p>
<p>2.9.1 大云PaaS 2.5<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_22.png" alt=""></p>
<p>3 Hadoop Distribution</p>
<p>3.1 Apache Hadoop</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_23.png" alt=""></p>
<p>3.2 Cloudera</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_24.png" alt=""></p>
<p>3.3 Hortonworks</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_25.png" alt=""></p>
<p>3.4 MapR</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_26.png" alt=""></p>
<p>3.5 Intel</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_27.png" alt=""></p>
<p>3.6 EMC Pivotal HD</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_28.jpg" alt=""></p>
<p>3.7 IBM</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_29.jpg" alt=""></p>
<p>3.8 Huawei</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_30.jpg" alt=""></p>
<p>4 Landscape</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_31.jpg" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_32.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_33.png" alt=""></p>
<p>5 参考</p>
<ul>
<li><a href="http://www-01.ibm.com/software/data/bigdata/platform/resources.html" target="_blank" rel="external">IBM Report</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1E7OTT7&amp;ct=130225&amp;st=sb" target="_blank" rel="external">Gartner - Hadoop Is Not a Data Integration Solution</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1DBWMQY&amp;ct=121220&amp;st=sb" target="_blank" rel="external">Gartner - Magic Quadrant for Data Masking Technology 2012</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1IMDMZ5&amp;ct=130819&amp;st=sb" target="_blank" rel="external"> Magic Quadrant for Cloud Infrastructure as a Service 2013</a></li>
<li><a href="http://wenku.it168.com/d_000048434.shtml" target="_blank" rel="external">Facebook Hadoop</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>1 Big Data Solution</p>
<p>1.1 HP</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_0]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MapR" scheme="http://navigating.github.io/tags/MapR/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[读《微软研发制胜策略》]]></title>
    <link href="http://navigating.github.io/2015/%E8%AF%BB%E3%80%8A%E5%BE%AE%E8%BD%AF%E7%A0%94%E5%8F%91%E5%88%B6%E8%83%9C%E7%AD%96%E7%95%A5%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/读《微软研发制胜策略》/</id>
    <published>2015-08-04T14:30:45.000Z</published>
    <updated>2015-08-04T14:31:31.555Z</updated>
    <content type="html"><![CDATA[<p>软件开发的核心就是：达成项目目标，提高生产率，提高软件的质量。除此之外，都不要重要。<br>管理上、复用上，一切的核心就是人的问题，提高人的能力是第一生产力。</p>
<p>1.项目中一个现象就是紧紧的去控制进度，调整进度，进度的跟踪只是一种日常的事务工作。<br>2.观念的改变是第一位的，什么是观念改变的原则：规则不是法律，是可以触碰的。什么是我们要改变的规则，就是要有主动、计划、灵活。<br>3.紧密的进度计划，是一般的管理人员的通常做法，他的好处就是看到不断的工作，会有不断的压力；如果运用不当，就可能让人觉得厌烦和沮丧。<br>4.为了日程进度，牺牲质量往往是不值得的，除非你要一笑而过的做法。再不管这个项目的后续开发和维护了。对于产品或者项目的期限，要谨慎，要反思可能为了进度而牺牲质量。这叫着草率的期限。<br>5.一个好的日程表会兼顾公司和员工的利益的。<br>6.没有期限的目标不过是梦想而已。<br>7.把一个大项目，切分成n个小项目来做，每一个项目的周期大约是2个月。叫着阶段式的日程控制法。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>软件开发的核心就是：达成项目目标，提高生产率，提高软件的质量。除此之外，都不要重要。<br>管理上、复用上，一切的核心就是人的问题，提高人的能力是第一生产力。</p>
<p>1.项目中一个现象就是紧紧的去控制进度，调整进度，进度的跟踪只是一种日常的事务工作。<br>2.观念]]>
    </summary>
    
      <category term="读书" scheme="http://navigating.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201507]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201507/"/>
    <id>http://navigating.github.io/2015/大数据动态之201507/</id>
    <published>2015-07-31T08:22:01.000Z</published>
    <updated>2015-08-01T02:27:01.570Z</updated>
    <content type="html"><![CDATA[<p>Hortonworks<br>HDP 2.3发布：<br>HDP 2.3新增加组件Apache Atlas、Apache Calcite<br><a href="http://hortonworks.com/blog/available-now-hdp-2-3/" target="_blank" rel="external">http://hortonworks.com/blog/available-now-hdp-2-3/</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-2/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-2/</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/</a><br>Spark 1.2开始支持ORC(Columnar Formats)<br><a href="http://hortonworks.com/blog/bringing-orc-support-into-apache-spark/" target="_blank" rel="external">http://hortonworks.com/blog/bringing-orc-support-into-apache-spark/</a><br>Spark in HDInsight新特性一览<br><a href="http://hortonworks.com/blog/spark-in-hdinsight/" target="_blank" rel="external">http://hortonworks.com/blog/spark-in-hdinsight/</a> </p>
<p>Cloudera<br>HBase 1.0 开始支持Thrift客户端鉴权<br><a href="http://blog.cloudera.com/blog/2015/07/thrift-client-authentication-support-in-apache-hbase-1-0/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/thrift-client-authentication-support-in-apache-hbase-1-0/</a><br>Pig on MR优化<br><a href="http://blog.cloudera.com/blog/2015/07/how-to-tune-mapreduce-parallelism-in-apache-pig-jobs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/how-to-tune-mapreduce-parallelism-in-apache-pig-jobs/</a><br>Apache Zeppelin on CDH<br><a href="http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/</a><br>大数据欺诈检测架构<br><a href="http://blog.cloudera.com/blog/2015/07/designing-fraud-detection-architecture-that-works-like-your-brain-does/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/designing-fraud-detection-architecture-that-works-like-your-brain-does/</a> </p>
<p>MapR<br>YARN资源管理实践<br><a href="https://www.mapr.com/blog/best-practices-yarn-resource-management" target="_blank" rel="external">https://www.mapr.com/blog/best-practices-yarn-resource-management</a><br>Hive 1.0对Transaction的支持<br><a href="https://www.mapr.com/blog/hive-transaction-feature-hive-10" target="_blank" rel="external">https://www.mapr.com/blog/hive-transaction-feature-hive-10</a> </p>
<p>Databricks<br>Spark Streaming执行模型<br><a href="https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html</a><br>Spark 1.4 MLP新特性<br><a href="https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html</a><br>从Spark 1.2开始支持ORC<br><a href="https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html</a><br>从Spark 1.4开始支持窗口函数<br><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br>从Spark 1.4开始新的Web UI<br><a href="https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html</a> </p>
<p>Phoenix对join的支持，TPC in Apache Phoenix<br><a href="https://blogs.apache.org/phoenix/entry/tpc_in_apache_phoenix" target="_blank" rel="external">https://blogs.apache.org/phoenix/entry/tpc_in_apache_phoenix</a> </p>
<p>Cassandra<br><a href="http://cassandra.apache.org/" target="_blank" rel="external">http://cassandra.apache.org/</a> </p>
<p>mongoDB<br><a href="https://www.mongodb.org/" target="_blank" rel="external">https://www.mongodb.org/</a> </p>
<p>Confluent<br>基于Kafka的实时流处理<br><a href="http://www.confluent.io/" target="_blank" rel="external">http://www.confluent.io/</a><br>大数据生态系统之Kafka价值<br><a href="http://www.confluent.io/blog/the-value-of-apache-kafka-in-big-data-ecosystem/" target="_blank" rel="external">http://www.confluent.io/blog/the-value-of-apache-kafka-in-big-data-ecosystem/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hortonworks<br>HDP 2.3发布：<br>HDP 2.3新增加组件Apache Atlas、Apache Calcite<br><a href="http://hortonworks.com/blog/available-now-hdp-2-3/" targ]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Cassandra" scheme="http://navigating.github.io/tags/Cassandra/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="mongoDB" scheme="http://navigating.github.io/tags/mongoDB/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用Hexo搭建Github静态博客]]></title>
    <link href="http://navigating.github.io/2015/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BAGithub%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2/"/>
    <id>http://navigating.github.io/2015/使用Hexo搭建Github静态博客/</id>
    <published>2015-07-28T09:20:22.000Z</published>
    <updated>2015-09-18T15:20:42.160Z</updated>
    <content type="html"><![CDATA[<p>环境：</p>
<pre><code><span class="bullet">1. </span>Windows XP
<span class="bullet">2. </span>Git
</code></pre><p>步骤：</p>
<pre><code><span class="bullet">1. </span>安装Node.js
<span class="bullet">2. </span>安装Hexo
<span class="bullet">3. </span>创建博客(初始化Hexo)
<span class="bullet">4. </span>创建文章本地调试
<span class="bullet">5. </span>配置Github
<span class="bullet">6. </span>远程发布
<span class="bullet">7. </span>支持sitemap和feed
<span class="bullet">8. </span>支持百度统计
<span class="bullet">9. </span>支持图片
<span class="bullet">10. </span>支持Swiftype站内搜索
<span class="bullet">11. </span>参考资源
</code></pre><h2 id="安装Node-js">安装Node.js</h2><p>下载并安装，<a href="https://nodejs.org/" target="_blank" rel="external">https://nodejs.org/</a></p>
<h2 id="安装Hexo">安装Hexo</h2><p>通过命令 npm install -g hexo 安装</p>
<pre><code><span class="attribute">D</span>:\git\hexo&gt;npm install -g hexo

npm WARN optional dep failed, continuing fsevents<span class="variable">@0</span>.<span class="number">3.6</span>
npm WARN optional dep failed, continuing fsevents<span class="variable">@0</span>.<span class="number">3.6</span>
-


&gt; dtrace-provider<span class="variable">@0</span>.<span class="number">5.0</span> install <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo\node_modules\bunyan\node_modules\dtrace-provider
&gt; node scripts/install.js

<span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\hexo -&gt; <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo\bin\hexo
hexo<span class="variable">@3</span>.<span class="number">1.1</span> <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo
├── pretty-hrtime<span class="variable">@1</span>.<span class="number">0.0</span>
├── hexo-front-matter<span class="variable">@0</span>.<span class="number">2.2</span>
├── abbrev<span class="variable">@1</span>.<span class="number">0.7</span>
├── titlecase<span class="variable">@1</span>.<span class="number">0.2</span>
├── archy<span class="variable">@1</span>.<span class="number">0.0</span>
├── text-table<span class="variable">@0</span>.<span class="number">2.0</span>
├── tildify<span class="variable">@1</span>.<span class="number">1.0</span> (os-homedir<span class="variable">@1</span>.<span class="number">0.1</span>)
├── strip-indent<span class="variable">@1</span>.<span class="number">0.1</span> (get-stdin<span class="variable">@4</span>.<span class="number">0.1</span>)
├── hexo-i18n<span class="variable">@0</span>.<span class="number">2.1</span> (sprintf-js<span class="variable">@1</span>.<span class="number">0.3</span>)
├── chalk<span class="variable">@1</span>.<span class="number">1.0</span> (escape-string-regexp<span class="variable">@1</span>.<span class="number">0.3</span>, supports-color<span class="variable">@2</span>.<span class="number">0.0</span>, ansi-styles<span class="variable">@2</span>.<span class="number">1.0</span>, strip-ansi<span class="variable">@3</span>.<span class="number">0.0</span>, has-ansi<span class="variable">@2</span>.<span class="number">0.0</span>)
├── bluebird<span class="variable">@2</span>.<span class="number">9.34</span>
├── minimatch<span class="variable">@2</span>.<span class="number">0.10</span> (brace-expansion<span class="variable">@1</span>.<span class="number">1.0</span>)
├── through2<span class="variable">@1</span>.<span class="number">1.1</span> (xtend<span class="variable">@4</span>.<span class="number">0.0</span>, readable-stream<span class="variable">@1</span>.<span class="number">1.13</span>)
├── swig-extras<span class="variable">@0</span>.<span class="number">0.1</span> (markdown<span class="variable">@0</span>.<span class="number">5.0</span>)
├── hexo-fs<span class="variable">@0</span>.<span class="number">1.3</span> (escape-string-regexp<span class="variable">@1</span>.<span class="number">0.3</span>, graceful-fs<span class="variable">@3</span>.<span class="number">0.8</span>, chokidar<span class="variable">@0</span>.<span class="number">12.6</span>)
├── js-yaml<span class="variable">@3</span>.<span class="number">3.1</span> (esprima<span class="variable">@2</span>.<span class="number">2.0</span>, argparse<span class="variable">@1</span>.<span class="number">0.2</span>)
├── nunjucks<span class="variable">@1</span>.<span class="number">3.4</span> (optimist<span class="variable">@0</span>.<span class="number">6.1</span>, chokidar<span class="variable">@0</span>.<span class="number">12.6</span>)
├── warehouse<span class="variable">@1</span>.<span class="number">0.2</span> (graceful-fs<span class="variable">@3</span>.<span class="number">0.8</span>, cuid<span class="variable">@1</span>.<span class="number">2.5</span>, JSONStream<span class="variable">@0</span>.<span class="number">10.0</span>)
├── cheerio<span class="variable">@0</span>.<span class="number">19.0</span> (entities<span class="variable">@1</span>.<span class="number">1.1</span>, dom-serializer<span class="variable">@0</span>.<span class="number">1.0</span>, css-select<span class="variable">@1</span>.<span class="number">0.0</span>, htmlparser2<span class="variable">@3</span>.<span class="number">8.3</span>)
├── bunyan<span class="variable">@1</span>.<span class="number">4.0</span> (safe-json-stringify<span class="variable">@1</span>.<span class="number">0.3</span>, dtrace-provider<span class="variable">@0</span>.<span class="number">5.0</span>, mv<span class="variable">@2</span>.<span class="number">1.1</span>)

├── hexo-cli<span class="variable">@0</span>.<span class="number">1.7</span> (minimist<span class="variable">@1</span>.<span class="number">1.2</span>)
├── moment-timezone<span class="variable">@0</span>.<span class="number">3.1</span>
├── moment<span class="variable">@2</span>.<span class="number">10.3</span>
├── hexo-util<span class="variable">@0</span>.<span class="number">1.7</span> (ent<span class="variable">@2</span>.<span class="number">2.0</span>, highlight.js<span class="variable">@8</span>.<span class="number">6.0</span>)
├── swig<span class="variable">@1</span>.<span class="number">4.2</span> (optimist<span class="variable">@0</span>.<span class="number">6.1</span>, uglify-js<span class="variable">@2</span>.<span class="number">4.24</span>)
└── lodash<span class="variable">@3</span>.<span class="number">10.0</span>

<span class="attribute">D</span>:\git\hexo&gt;
</code></pre><h2 id="创建博客(初始化hexo)">创建博客(初始化hexo)</h2><p>创建博客站点的本地目录，然后在文件夹下执行命令：</p>
<pre><code><span class="variable">$ </span>hexo init
</code></pre><p>[info] Copying data<br>[info] You are almost done! Don’t forget to run <code>npm install</code> before you start b<br>logging with Hexo!</p>
<p>Hexo会自动在目标文件夹下建立网站所需要的文件。然后按照提示，安装node_modules，执行如下命令：</p>
<pre><code>$ hexo <span class="keyword">install</span>
</code></pre><h2 id="创建文章本地调试">创建文章本地调试</h2><p>预览本地调试模式，执行如下命令：</p>
<pre><code>$ hexo <span class="keyword">server</span>
</code></pre><p>[info] Hexo is running at <a href="http://localhost:4000/" target="_blank" rel="external">http://localhost:4000/</a>. Press Ctrl+C to stop.</p>
<p>关键命令简介：</p>
<pre><code><span class="title">hexo</span> n     <span class="comment">#创建新的文章</span>
hexo g     <span class="comment">#重新生成站点</span>
hexo s     <span class="comment">#启动本地服务</span>
hexo d     <span class="comment">#发布到github</span>
</code></pre><p>创建文章</p>
<pre><code>$ hexo <span class="keyword">new</span> <span class="string">"使用Hexo搭建Github静态博客"</span> 
</code></pre><p>在Hexo工作文件夹下source_posts发现新创建的md文件 使用Hexo搭建Github静态博客.md 。</p>
<h2 id="配置Github">配置Github</h2><p>部署到Github需要修改配置文件_config.yml文件，在Hexo工作目录之下：</p>
<pre><code># Deployment
## <span class="string">Docs:</span> <span class="string">http:</span><span class="comment">//hexo.io/docs/deployment.html</span>
<span class="label">
deploy:</span>
<span class="label">    type:</span> git
<span class="label">    repository:</span> git<span class="annotation">@github</span>.<span class="string">com:</span>&lt;Your Github Username&gt;/&lt;Your github.io url&gt;
<span class="label">    branch:</span> master
</code></pre><p>注意，当前type为git，而不是github</p>
<p>测试Github是否好用    </p>
<pre><code><span class="title">ssh</span> -T git<span class="variable">@github</span>.com
</code></pre><h2 id="远程发布">远程发布</h2><p>远程部署到Github，通过执行如下命令：    </p>
<pre><code><span class="variable">$ </span>hexi deploy
</code></pre><p>Troubleshooting<br>出现错误：Error: spawn git ENOENT<br>解决方案：<br><a href="http://blog.csdn.net/rainloving/article/details/46595559" target="_blank" rel="external">http://blog.csdn.net/rainloving/article/details/46595559</a> </p>
<p>使用github出现：fatal: unable to access: Failed connect to github.com:8080: No error<br>解决方案：<br><a href="http://www.zhihu.com/question/26954892" target="_blank" rel="external">http://www.zhihu.com/question/26954892</a> </p>
<p>使用github出现：ssh:connect to host github.com port 22: Bad file number<br>解决方案：<br><a href="http://www.xnbing.org/?p=759" target="_blank" rel="external">http://www.xnbing.org/?p=759</a><br><a href="http://blog.csdn.net/temotemo/article/details/7641883" target="_blank" rel="external">http://blog.csdn.net/temotemo/article/details/7641883</a> </p>
<h2 id="支持sitemap和feed">支持sitemap和feed</h2><p>首先安装sitemap和feed插件</p>
<pre><code>$ npm <span class="keyword">install</span> hexo-generator-sitemap
$ npm <span class="keyword">install</span> hexo-generator-feed
</code></pre><p>修改配置，在文件 _config.yml 增加以下内容</p>
<pre><code><span class="preprocessor"># Extensions</span>
<span class="label">Plugins:</span>
- hexo-generator-feed
- hexo-generator-sitemap

<span class="preprocessor">#Feed Atom</span>
<span class="label">feed:</span>
    type: atom
    path: atom.xml
    limit: <span class="number">20</span>

<span class="preprocessor">#sitemap</span>
<span class="label">sitemap:</span>
    path: sitemap.xml
</code></pre><p>在 themes\landscape_config.yml 中添加：</p>
<pre><code><span class="attribute">menu</span>:
    <span class="attribute">Home</span>: /
    <span class="attribute">Archives</span>: /archives
    <span class="attribute">Sitemap</span>: /sitemap.xml
<span class="attribute">rss</span>: /atom.xml
</code></pre><h2 id="支持百度统计">支持百度统计</h2><p>在 <a href="http://tongji.baidu.com" target="_blank" rel="external">http://tongji.baidu.com</a> 注册帐号，添加网站，生成统计功能的 JS 代码。</p>
<p>在 themes\landscape_config.yml 中新添加一行：</p>
<pre><code><span class="keyword">baidu_t</span>ongji: <span class="keyword">true</span>
</code></pre><p>在 themes\landscape\layout_partial\head.ejs 中head的结束标签  之前新添加一行代码</p>
<pre><code>&lt;<span class="preprocessor">%</span>- partial<span class="comment">('baidu_tongji')</span> <span class="preprocessor">%</span>&gt;
</code></pre><p>在 themes\landscape\layout_partial 中新创建一个文件 baidu_tongji.ejs 并添加如下内容：</p>
<pre><code><span class="xml"></span>&lt;%<span class="ruby"> <span class="keyword">if</span> (theme.baidu_tongji){ </span>%&gt;<span class="xml">
<span class="tag">&lt;<span class="title">script</span> <span class="attribute">type</span>=<span class="value">"text/javascript"</span>&gt;</span><span class="apache">
    <span class="tag">&lt;百度统计的 JS 代码&gt;</span>
</span><span class="tag">&lt;/<span class="title">script</span>&gt;</span>
</span>&lt;%<span class="ruby"> } </span>%&gt;<span class="xml"></span>
</code></pre><p>添加统计，参考：<br><a href="http://ibruce.info/2013/11/22/hexo-your-blog/" target="_blank" rel="external">http://ibruce.info/2013/11/22/hexo-your-blog/</a><br><a href="http://www.cnblogs.com/zhcncn/p/4097881.html" target="_blank" rel="external">http://www.cnblogs.com/zhcncn/p/4097881.html</a> </p>
<h2 id="支持图片">支持图片</h2><p>在source目录下创建images目录，然后将图片放在其中。<br>在文章中引用本地图片的语法例如：</p>
<pre><code>![<span class="link_label">这是一张图片</span>](<span class="link_url">/images/2005_TuoZhanXunLian.jpg</span>)
</code></pre><h2 id="支持swiftype站内搜索">支持swiftype站内搜索</h2><p>在 <a href="http://siftype.com" target="_blank" rel="external">http://siftype.com</a> 注册一个帐号，按着网站引导流程就可以了。<br>安装 install code 代码到hexo，添加到 themes\landscape\layout_partial\after-footer.ejs，类似添加百度统计的代码。<br>然后回到swiftype网站对 install code 进行确认。通过会在下方弹出一条消息：</p>
<pre><code><span class="title">Installation</span> successfully activated
</code></pre><h2 id="添加robots-txt">添加robots.txt</h2><p><a href="http://blog.lmintlcx.com/post/blog-with-hexo.html" target="_blank" rel="external">http://blog.lmintlcx.com/post/blog-with-hexo.html</a> </p>
<h2 id="支持多说评论">支持多说评论</h2><p>首先，在多说官网 <a href="http://www.duoshuo.com" target="_blank" rel="external">http://www.duoshuo.com</a> 激活账户，并添加网站配置；<br>其次，参考多说官网的Hexo配置指南进行配置，<a href="http://dev.duoshuo.com/threads/541d3b2b40b5abcd2e4df0e9" target="_blank" rel="external">http://dev.duoshuo.com/threads/541d3b2b40b5abcd2e4df0e9</a><br>针对landscap theme就是两步：<br>第一步在Hexo根目录下的配置文件 _config.yml 中添加多说配置项</p>
<pre><code><span class="attribute">duoshuo_shortname</span>: <span class="string">你站点的short_name</span>
</code></pre><p>第二步修改themes\landscape\layout_partial\article.ejs模板，用多说推荐的代码替换之前的Disqus代码。<br>将<br><figure class="highlight erb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"></span>&lt;%<span class="ruby"> <span class="keyword">if</span> (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; </span>%&gt;<span class="xml"></span><br><span class="line"><span class="tag">&lt;<span class="title">section</span> <span class="attribute">id</span>=<span class="value">"comments"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">div</span> <span class="attribute">id</span>=<span class="value">"disqus_thread"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">noscript</span>&gt;</span>Please enable JavaScript to view the <span class="tag">&lt;<span class="title">a</span> <span class="attribute">href</span>=<span class="value">"//disqus.com/?ref_noscript"</span>&gt;</span>comments powered by Disqus.<span class="tag">&lt;/<span class="title">a</span>&gt;</span><span class="tag">&lt;/<span class="title">noscript</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">section</span>&gt;</span></span><br><span class="line"></span>&lt;%<span class="ruby"> &#125; </span>%&gt;<span class="xml"></span></span><br></pre></td></tr></table></figure></p>
<p>改为<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">%</span> <span class="attribute">if</span> (!<span class="attribute">index</span> &amp;&amp; <span class="attribute">post.comments</span> &amp;&amp; <span class="attribute">config.duoshuo_shortname</span>)&#123; %&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">section</span> <span class="attribute">id</span>=<span class="value">"comments"</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 多说评论框 start --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">div</span> <span class="attribute">class</span>=<span class="value">"ds-thread"</span> <span class="attribute">data-thread-key</span>=<span class="value">"&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;"</span> <span class="attribute">data-title</span>=<span class="value">"&lt;%= post.title %&gt;"</span> <span class="attribute">data-url</span>=<span class="value">"&lt;%= page.permalink %&gt;"</span>&gt;</span><span class="tag">&lt;/<span class="title">div</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 多说评论框 end --&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 多说公共JS代码 start (一个网页只需插入一次) --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">script</span> <span class="attribute">type</span>=<span class="value">"text/javascript"</span>&gt;</span><span class="javascript"></span><br><span class="line">  <span class="keyword">var</span> duoshuoQuery = &#123;short_name:<span class="string">'&lt;%= config.duoshuo_shortname %&gt;'</span>&#125;;</span><br><span class="line">    (<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> ds = <span class="built_in">document</span>.createElement(<span class="string">'script'</span>);</span><br><span class="line">      ds.type = <span class="string">'text/javascript'</span>;ds.async = <span class="literal">true</span>;</span><br><span class="line">      ds.src = (<span class="built_in">document</span>.location.protocol == <span class="string">'https:'</span> ? <span class="string">'https:'</span> : <span class="string">'http:'</span>) + <span class="string">'//static.duoshuo.com/embed.js'</span>;</span><br><span class="line">      ds.charset = <span class="string">'UTF-8'</span>;</span><br><span class="line">      (<span class="built_in">document</span>.getElementsByTagName(<span class="string">'head'</span>)[<span class="number">0</span>] </span><br><span class="line">       || <span class="built_in">document</span>.getElementsByTagName(<span class="string">'body'</span>)[<span class="number">0</span>]).appendChild(ds);</span><br><span class="line">    &#125;)();</span><br><span class="line">    </span><span class="tag">&lt;/<span class="title">script</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 多说公共JS代码 end --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">section</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">%</span> &#125; %&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="参考资源">参考资源</h2><p><a href="http://blog.lmintlcx.com/post/blog-with-hexo.html" target="_blank" rel="external">http://blog.lmintlcx.com/post/blog-with-hexo.html</a><br><a href="https://github.com/bruce-sha" target="_blank" rel="external">https://github.com/bruce-sha</a><br><a href="http://zipperary.com/2013/05/28/hexo-guide-2/" target="_blank" rel="external">http://zipperary.com/2013/05/28/hexo-guide-2/</a><br><a href="http://zipperary.com/2013/05/29/hexo-guide-3/" target="_blank" rel="external">http://zipperary.com/2013/05/29/hexo-guide-3/</a><br><a href="http://zipperary.com/2013/05/30/hexo-guide-4/" target="_blank" rel="external">http://zipperary.com/2013/05/30/hexo-guide-4/</a><br><a href="http://cnfeat.com/2014/05/10/2014-05-11-how-to-build-a-blog/" target="_blank" rel="external">http://cnfeat.com/2014/05/10/2014-05-11-how-to-build-a-blog/</a><br><a href="http://www.cnblogs.com/zhcncn/p/4097881.html" target="_blank" rel="external">http://www.cnblogs.com/zhcncn/p/4097881.html</a><br><a href="http://blog.moyizhou.cn/web/search-engine-for-static-pages/" target="_blank" rel="external">http://blog.moyizhou.cn/web/search-engine-for-static-pages/</a><br><a href="http://www.jerryfu.net/post/search-engine-for-hexo-with-swiftype.html" target="_blank" rel="external">http://www.jerryfu.net/post/search-engine-for-hexo-with-swiftype.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>环境：</p>
<pre><code><span class="bullet">1. </span>Windows XP
<span class="bullet">2. </span>Git
</code></pre><p>步骤：</p>
<pre><code><span ]]>
    </summary>
    
      <category term="blog" scheme="http://navigating.github.io/tags/blog/"/>
    
      <category term="github" scheme="http://navigating.github.io/tags/github/"/>
    
      <category term="hexo" scheme="http://navigating.github.io/tags/hexo/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://navigating.github.io/2015/hello-world/"/>
    <id>http://navigating.github.io/2015/hello-world/</id>
    <published>2015-07-27T09:20:22.000Z</published>
    <updated>2015-07-28T09:21:58.301Z</updated>
    <content type="html"><![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io]]>
    </summary>
    
      <category term="hexo" scheme="http://navigating.github.io/tags/hexo/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop 2.7.1 发布]]></title>
    <link href="http://navigating.github.io/2015/Hadoop-2-7-1-%E5%8F%91%E5%B8%83/"/>
    <id>http://navigating.github.io/2015/Hadoop-2-7-1-发布/</id>
    <published>2015-07-09T13:49:30.000Z</published>
    <updated>2015-07-30T13:50:50.764Z</updated>
    <content type="html"><![CDATA[<p>2015年7月6日，Apache Hadoop的稳定版本 2.7.1 正式发布。<br><a href="http://hadoop.apache.org/releases.html#Release+Notes" target="_blank" rel="external">http://hadoop.apache.org/releases.html#Release+Notes</a> </p>
<p>Hadoop 2.7的一个小版本发布了，本版本属于稳定版本。<br>修复了2.7.0中存在的131个bug。<br>这是2.7.x第一个稳定版本，增强的功能列表请通过2.7.0版本部分查看。<br>按着计划，下一个2.7.x的小版本是2.7.2.</p>
<p>原文：<br>06 July, 2015: Release 2.7.1 (stable) availableA point release for the 2.7 line. This release is now considered stable.<br>Please see the Hadoop 2.7.1 Release Notes for the list of 131 bug fixes and patches since the previous release 2.7.0. Please look at the 2.7.0 section below for the list of enhancements enabled by this first stable release of 2.7.x.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>2015年7月6日，Apache Hadoop的稳定版本 2.7.1 正式发布。<br><a href="http://hadoop.apache.org/releases.html#Release+Notes" target="_blank" rel="external"]]>
    </summary>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[读《Deploying Apache Kafka: A Practical FAQ》]]></title>
    <link href="http://navigating.github.io/2015/%E8%AF%BB%E3%80%8ADeploying-Apache-Kafka-A-Practical-FAQ%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/读《Deploying-Apache-Kafka-A-Practical-FAQ》/</id>
    <published>2015-07-02T14:57:45.000Z</published>
    <updated>2015-07-30T15:01:55.553Z</updated>
    <content type="html"><![CDATA[<p>Cloudera发布了Kafka的好文，《Deploying Apache Kafka: A Practical FAQ》，参见：<a href="http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-practical-faq" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-practical-faq</a></p>
<p>是否应当为Kafka Broker使用 固态硬盘 (SSD)<br>实际上使用SSD盘并不能显著地改善 Kafka 的性能，主要有两个原因：</p>
<pre><code>* Kafka写磁盘是异步的，不是同步的。就是说，除了启动、停止之外，Kafka的任何操作都不会去等待磁盘同步（sync）完成；而磁盘同步(disk syncs)总是在后台完成的。这就是为什么Kafka消息至少复制到三个副本是至关重要的，因为一旦单个副本崩溃，这个副本就会丢失数据无法同步写到磁盘。
* 每一个Kafka <span class="keyword">Partition</span>被存储为一个串行的WAL（<span class="keyword">Write</span> Ahead <span class="keyword">Log</span>）日志文件。因此，除了极少数的数据查询，Kafka中的磁盘读写都是串行的。现代的操作系统已经对串行读写做了大量的优化工作。
</code></pre><p>如何对Kafka Broker上持久化的数据进行加密<br>目前，Kafka不提供任何机制对Broker上持久化的数据进行加密。用户可以自己对写入到Kafka的数据进行加密，即是，生产者(Producers)在写Kafka之前加密数据，消费者(Consumers)能解密收到的消息。这就要求生产者(Producers)把加密协议(protocols)和密钥(keys)分享给消费者(Consumers)。<br>另外一种选择，就是使用软件提供的文件系统级别的加密，例如Cloudera Navigator Encrypt。Cloudera Navigator Encrypt是Cloudera企业版(Cloudera Enterprise)的一部分，在应用程序和文件系统之间提供了一个透明的加密层。<br>Apache Zookeeper正成为Kafka集群的一个痛点(pain point)，真的吗？<br>Kafka高级消费者(high-level consumer)的早期版本(0.8.1或更早)使用Zookeeper来维护读的偏移量(offsets，主要是Topic的每个Partition的读偏移量)。如果有大量生产者(consumers)同时从Kafka中读数据，对Kafka的读写负载可能就会超出它的容量，Zookeeper就变成一个瓶颈(bottleneck)。当然，这仅仅出现在一些很极端的案例中(extreme cases)，即有成百上千个消费者(consumers)在使用同一个Zookeeper集群来管理偏移量(offset)。<br>不过，这个问题已经在Kafka当前的版本(0.8.2)中解决。从版本0.8.2开始，高级消费者(high-level consumer)能够使用Kafka自己来管理偏移量(offsets)。本质上讲，它使用一个单独的Kafka Topic来管理最近的读偏移量(read offsets)，因此偏移量管理(offset management)不再要求Zookeeper必须存在。然后，用户将不得不面临选择是用Kafka还是Zookeeper来管理偏移量(offsets)，由消费者(consumer)配置参数 offsets.storage 决定。<br>Cloudera强烈推荐使用Kafka来存储偏移量。当然，为了保证向后兼容性，你可以继续选择使用Zookeeper存储偏移量。(例如，你可能有一个监控平台需要从Zookeeper中读取偏移量信息。) 假如你不得不使用Zookeeper进行偏移量(offset)管理，我们推荐你为Kafka集群使用一个专用的Zookeeper集群。假如一个专用的Zookeeper集群仍然有性能瓶颈，你依然可以通过在Zookeeper节点上使用固态硬盘(SSD)来解决问题。<br>Kafka是否支持跨数据中心的可用性<br>Kafka跨数据中心可用性的推荐解决方案是使用MirrorMaker(<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330" target="_blank" rel="external">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330</a> ) 。在你的每一个数据中心都搭建一个Kafka集群，在Kafka集群之间使用MirrorMaker来完成近实时的数据复制。<br>使用MirrorMaker的架构模式是为每一个”逻辑”的topic在每一个数据中心创建一个topic：例如，在逻辑上你有一个”clicks”的topic，那么你实际上有”DC1.clicks”和“DC2.clicks”两个topic(DC1和DC2指得是你的数据中心)。DC1向DC1.clicks中写数据，DC2向DC2.clicks中写数据。MirrorMaker将复制所有的DC1 topics到DC2，并且复制所有的DC2 topics到DC1。现在每个DC上的应用程序都能够访问写入到两个DC的事件。这个应用程序能够合并信息和处理相应的冲突。<br>另一种更复杂的模式是在每一个DC都搭建本地和聚合Kafka集群。这个模式已经被Linkedin使用，Linkedin Kafka运维团队已经在这篇Blog(<a href="https://engineering.linkedin.com/kafka/running-kafka-scale" target="_blank" rel="external">https://engineering.linkedin.com/kafka/running-kafka-scale</a> )中有详细的描述(参见“Tiers and Aggregation”)。<br>Kafka支持哪些类型的数据转换(data transformation)<br>数据流过的Kafka的时候，Kafka并不能进行数据转换。为了处理数据转换，我们推荐如下方法：</p>
<pre><code>* 对于简单事件处理，使用<span class="constant">Flume Kafka </span>integration(<span class="symbol">http:</span>/<span class="regexp">/blog.cloudera.com/blog</span><span class="regexp">/2014/</span><span class="number">11</span>/flafka-apache-flume-meets-apache-kafka-<span class="keyword">for</span>-event-processing )，并且写一个简单的<span class="constant">Apache Flume Interceptor。</span>
* 对于复杂(事件)处理，使用<span class="constant">Apache Spark Streaming从Kafka中</span>读数据和处理数据。
</code></pre><p>在这两种情况下，被转换或者处理的数据可被写会到新的Kafka Topic中，或者直接传送到数据的最终消费者(Consumer)那里。<br>对于实时事件处理模式更全面的描述，看看这篇文章(<a href="http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/</a> )。<br>如何通过Kafka发送大消息或者超大负荷量？<br>Cloudera的性能测试表明Kafka达到最大吞吐量的消息大小为10K左右。更大的消息将导致吞吐量下降。然后，在一些情况下，用户需要发送比10K大的多的消息。<br>如果消息负荷大小是每100s处理MB级别，我们推荐探索以下选择：</p>
<pre><code><span class="bullet">* </span>如果可以使用共享存储(HDFS、S3、NAS)，那么将超负载放在共享存储上，仅用Kafka发送负载数据位置的消息。
<span class="bullet">* </span>对于大消息，在写入Kafka之前将消息拆分成更小的部分，使用消息Key确保所有的拆分部分都写入到同一个partition中，以便于它们能被同一个消息着(Consumer)消费的到，在消费的时候将拆分部分重新组装成一个大消息。
</code></pre><p>在通过Kafka发送大消息时，请记住以下几点：<br>压缩配置</p>
<pre><code><span class="keyword">*</span> Kafka生产者(Producers)能够压缩消息。通过配置参数compression.codec确保压缩已经开启。有效的选项为<span class="string">"gzip"</span>和<span class="string">"snappy"</span>。
</code></pre><p>Broker配置</p>
<pre><code>* message.<span class="built_in">max</span>.<span class="keyword">bytes</span> (default: <span class="number">1000000</span>): Broker能够接受的最大消息。增加这个值以便于匹配你的最大消息。
* <span class="built_in">log</span>.<span class="keyword">segment</span>.<span class="keyword">bytes</span> (default: <span class="number">1</span>GB): Kafka数据文件的大小。确保它至少大于一条消息。默认情况下已经够用，一般最大的消息不会超过<span class="number">1</span>G大小。
* replica.fetch.<span class="built_in">max</span>.<span class="keyword">bytes</span> (default: <span class="number">1</span>MB): Broker间复制的最大的数据大小。这个值必须大于message.<span class="built_in">max</span>.<span class="keyword">bytes</span>，否则一个Broker接受到消息但是会复制失败，从而导致潜在的数据丢失。
</code></pre><p>Consumer配置</p>
<pre><code>* <span class="tag">fetch</span><span class="class">.message</span><span class="class">.max</span><span class="class">.bytes</span> (<span class="rule"><span class="attribute">default</span>:<span class="value"> <span class="number">1</span>MB): Consumer所读消息的最大大小。这个值应该大于或者等于Broker配置的message.max.bytes的值。</span></span>
</code></pre><p>其他方面的考虑：</p>
<pre><code>* <span class="tag">Broker</span>需要针对复制为每一个<span class="tag">partition</span>分配一个<span class="tag">replica</span><span class="class">.fetch</span><span class="class">.max</span><span class="class">.bytes</span>大小的缓存区。需要计算确认( <span class="tag">partition</span>的数量 * 最大消息的大小 )不会超过可用的内存，否则就会引发<span class="tag">OOMs</span>（内存溢出异常）。
* <span class="tag">Consumers</span>有同样的问题，因子参数为 <span class="tag">fetch</span><span class="class">.message</span><span class="class">.max</span><span class="class">.bytes</span> ：确认每一个<span class="tag">partition</span>的消费者针对最大的消息有足够可用的内存。
* 大消息可能引发更长时间的垃圾回收停顿(<span class="tag">garbage</span> <span class="tag">collection</span> <span class="tag">pauses</span>)(<span class="tag">brokers</span>需要申请更大块的内存)。注意观察<span class="tag">GC</span>日志和服务器日志。假如发现长时间的<span class="tag">GC</span>停顿导致<span class="tag">Kafka</span>丢失了<span class="tag">Zookeeper</span> <span class="tag">session</span>，你可能需要为<span class="tag">zookeeper</span><span class="class">.session</span><span class="class">.timeout</span><span class="class">.ms</span>配置更长的<span class="tag">timeout</span>值。
</code></pre><p>Kafka是否支持MQTT或JMS协议<br>目前，Kafka针对上述协议不提供直接支持。但是，用户可以自己编写Adaptors从MQTT或者JMS中读取数据，然后写入到Kafka中。</p>
<p>更多关于在CDH中使用Kafka的信息，下载Deployment Guide(<a href="http://www.cloudera.com/content/cloudera/en/resources/library/datasheet/kafka-reference-architecture.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/resources/library/datasheet/kafka-reference-architecture.html</a> ) 或者 观看webinar “Bringing Real-Time Data to Hadoop”(<a href="http://www.cloudera.com/content/cloudera/en/resources/library/recordedwebinar/kafka-webinar-recording.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/resources/library/recordedwebinar/kafka-webinar-recording.html</a> )。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Cloudera发布了Kafka的好文，《Deploying Apache Kafka: A Practical FAQ》，参见：<a href="http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-]]>
    </summary>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="Kafka" scheme="http://navigating.github.io/tags/Kafka/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201506]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201506/"/>
    <id>http://navigating.github.io/2015/大数据动态之201506/</id>
    <published>2015-06-09T13:52:23.000Z</published>
    <updated>2015-08-10T06:25:16.483Z</updated>
    <content type="html"><![CDATA[<p>Pinot：LinkedIn的实时数据分析系统<br><a href="http://www.infoq.com/cn/news/2014/10/linkdln" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/10/linkdln</a><br><a href="https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot" target="_blank" rel="external">https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot</a></p>
<p>Twitter Heron：Twitter发布新的大数据实时分析系统Heron<br><a href="http://geek.csdn.net/news/detail/33750" target="_blank" rel="external">http://geek.csdn.net/news/detail/33750</a><br><a href="http://www.longda.us/?p=529" target="_blank" rel="external">http://www.longda.us/?p=529</a> </p>
<p>Cloudera<br>HBase对MOBs( Moderate Objects, 主要是大小100K到10M的对象存储 )的支持<br><a href="http://blog.cloudera.com/blog/2015/06/inside-apache-hbases-new-support-for-mobs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/inside-apache-hbases-new-support-for-mobs/</a><br>准实时计算架构模式<br><a href="http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/</a><br>(翻译：<a href="http://zhuanlan.zhihu.com/donglaoshi/20082628" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20082628</a> )<br>CDH 5.4 新功能：敏感数据处理(Sensitive Data Redaction)<br><a href="http://blog.cloudera.com/blog/2015/06/new-in-cdh-5-4-sensitive-data-redaction/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/new-in-cdh-5-4-sensitive-data-redaction/</a> </p>
<p>Hortonworks<br>YARN的CapacityScheduler对Resource-preemption的支持<br><a href="http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/" target="_blank" rel="external">http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/</a><br>Hadoop集群对Multihoming的支持<br><a href="http://hortonworks.com/blog/multihoming-on-hadoop-yarn-clusters/" target="_blank" rel="external">http://hortonworks.com/blog/multihoming-on-hadoop-yarn-clusters/</a><br>HDP 2.3企业级HDFS数据加密<br><a href="http://hortonworks.com/blog/new-in-hdp-2-3-enterprise-grade-hdfs-data-at-rest-encryption/" target="_blank" rel="external">http://hortonworks.com/blog/new-in-hdp-2-3-enterprise-grade-hdfs-data-at-rest-encryption/</a><br>Apache Slider 0.80.0版本发布<br><a href="http://hortonworks.com/blog/announcing-apache-slider-0-80-0/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-apache-slider-0-80-0/</a><br>Apache Spark 1.3.1 on HDP 2.2<br><a href="http://hortonworks.com/blog/apache-spark-on-hdp-learn-try-and-do/" target="_blank" rel="external">http://hortonworks.com/blog/apache-spark-on-hdp-learn-try-and-do/</a><br><a href="http://hortonworks.com/hadoop-tutorial/using-apache-spark-technical-preview-with-hdp-2-2/" target="_blank" rel="external">http://hortonworks.com/hadoop-tutorial/using-apache-spark-technical-preview-with-hdp-2-2/</a><br>Ambari 2.0.1 和 HDP 2.2.6 发布<br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html</a><br><a href="http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_releasenotes_ambari_2.0.1.0/content/ch_relnotes-ambari-2.0.1.0.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_releasenotes_ambari_2.0.1.0/content/ch_relnotes-ambari-2.0.1.0.html</a></p>
<p>其他：<br>Graphite的百万Metrics实践之路<br><a href="http://calvin1978.blogcn.com/articles/graphite.html" target="_blank" rel="external">http://calvin1978.blogcn.com/articles/graphite.html</a><br>HBaseCon 2015 大会幻灯片 &amp; 视频<br><a href="http://hbasecon.com/archive.html" target="_blank" rel="external">http://hbasecon.com/archive.html</a><br>HBase在腾讯大数据的应用实践<br><a href="http://www.d1net.com/bigdata/news/353500.html" target="_blank" rel="external">http://www.d1net.com/bigdata/news/353500.html</a><br>从Spark到Hadoop的架构实践<br><a href="http://www.csdn.net/article/2015-06-08/2824889" target="_blank" rel="external">http://www.csdn.net/article/2015-06-08/2824889</a><br>56网大数据<br><a href="http://share.csdn.net/slides/10903" target="_blank" rel="external">http://share.csdn.net/slides/10903</a><br>七牛技术总监陈超：记Spark Summit China 2015<br><a href="http://www.csdn.net/article/2015-04-30/2824594-spark-summit-china-2015" target="_blank" rel="external">http://www.csdn.net/article/2015-04-30/2824594-spark-summit-china-2015</a><br>唯品会美研中心郭安琪：2015 Hadoop Summit见闻<br><a href="http://zhuanlan.zhihu.com/donglaoshi/20072576" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20072576</a><br>华为叶琪：论Spark Streaming的数据可靠性和一致性<br><a href="http://www.csdn.net/article/2015-06-12/2824938" target="_blank" rel="external">http://www.csdn.net/article/2015-06-12/2824938</a><br>Hadoop Summit 2015<br><a href="http://2015.hadoopsummit.org/san-jose/agenda/" target="_blank" rel="external">http://2015.hadoopsummit.org/san-jose/agenda/</a><br>Spark Summit 2015<br><a href="https://spark-summit.org/2015/" target="_blank" rel="external">https://spark-summit.org/2015/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Pinot：LinkedIn的实时数据分析系统<br><a href="http://www.infoq.com/cn/news/2014/10/linkdln" target="_blank" rel="external">http://www.infoq.com/cn/]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201505]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201505/"/>
    <id>http://navigating.github.io/2015/大数据动态之201505/</id>
    <published>2015-05-19T02:17:28.000Z</published>
    <updated>2015-08-10T06:24:50.273Z</updated>
    <content type="html"><![CDATA[<p>近期动态：<br>Hadoop 2.7发布。<br>Hortonworks HDP 2.2.4.2发布。<br>Ambari 2.0发布。<br>Cloudera Enterperise 5.4发布。<br>Hive 1.2.0 发布，支持Hive on Spark。</p>
<p>HDP 2.2/HDP 2.2.4/Ambari 2.0/Ambari 2.0.1</p>
<pre><code><span class="bullet">1. </span>HDP支持异构存储Heterogeneous storage，主要是对SSD的支持；
<span class="bullet">2. </span>Hive开始支持 ACID 事务，向企业级应用场景前进了一大步；
<span class="bullet">3. </span>HDP支持Spark 1.2.1；
<span class="bullet">4. </span>HDP支持通过DominantResourceCalculator对CPU的资源隔离与资源调度；
<span class="bullet">5. </span>Ambari 支持Blurprint，通过 REST API 管理和运维有更好的支持；
<span class="bullet">6. </span>Ambari 支持Stacks，通过Stacks方式来定义一系列的集成组件；
<span class="bullet">7. </span>Ambari 2.0支持HDP 2.2平台的Rolling Upgrades；
<span class="bullet">8. </span>Ambari 2.0支持安装、配置Apache Ranger；
<span class="bullet">9. </span>Ambari 2.0开始集成Ambari Alerts；
<span class="bullet">10. </span>Ambari 2.0开始集成Ambari Metrics，替代之前的Ganglia；
<span class="bullet">11. </span>Ambari 2.0开始支持User Views功能，User Views提供给运维人员更好的界面，包括Tez View、Capacity Scheduler View、Hive View、Pig View、Files View；
</code></pre><p>HDP 2.2之后部署的结构与之前有调整，新部署的结构与说明如下：</p>
<p>目录结构<br>从HDP 2.2之后，HDP安装后的目录结构发生了变化，之前安装后的Hadoop在/usr/lib目录下，现在变更到/usr/hdp目录下，结构如下：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"> &#123;code&#125;</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/hadoop/</span>conf</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/lib</span><br><span class="line">│   │   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop<span class="regexp">/lib/</span>native</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/libexec</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/man</span><br><span class="line">│   └── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/sbin</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs/lib</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs/sbin</span><br><span class="line">│   └── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs/webapps</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/hbase/</span>conf</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase/doc</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase/include</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase/lib</span><br><span class="line">└── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper/bin</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/zookeeper/</span>conf</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper/doc</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper/lib</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper/man</span><br><span class="line"> &#123;code&#125;</span><br><span class="line"> &#123;code&#125;</span><br><span class="line"><span class="regexp">/usr/</span>hdp/<span class="number">2.2</span>.3.0-<span class="number">2611</span></span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/hadoop/</span>conf</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/lib</span><br><span class="line">│   │   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop<span class="regexp">/lib/</span>native</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/libexec</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/man</span><br><span class="line">│   └── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/sbin</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs/lib</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs/sbin</span><br><span class="line">│   └── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs/webapps</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/hbase/</span>conf</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase/doc</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase/include</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase/lib</span><br><span class="line">└── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper/bin</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/zookeeper/</span>conf</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper/doc</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper/lib</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper/man</span><br><span class="line"> &#123;code&#125;</span><br></pre></td></tr></table></figure></p>
<p>管理活动版本<br>HDP 2.0之后推出了hdp-select服务，通过这个服务可以管理活动版本，默认就会安装hdp-select，可以通过hdp-select命令验证是否安装。<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; hdp-<span class="keyword">select</span><br><span class="line"></span>&gt; hdp-<span class="keyword">select </span>versions</span><br></pre></td></tr></table></figure></p>
<p>同样支持管理命令，例如：</p>
<pre><code>&gt; <span class="tag">hdp-select</span> <span class="tag">set</span> <span class="tag">hadoop-hdfs-datanode</span> 2<span class="class">.2</span><span class="class">.3</span><span class="class">.0-2600</span>
</code></pre><p>安装后的库、工具和脚本<br>库<br>HDP 2.0之前安装后库放在/usr/lib下，现在放在/usr/hdp/current下：</p>
<pre><code><span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-hdfs-namenode/
<span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-yarn-resourcemanager
<span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-mapreduce-client/hadoop-mapreduce-examples.jar
</code></pre><p>Daemon Scripts</p>
<pre><code><span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-hdfs-namenode<span class="regexp">/../</span>hadoop<span class="regexp">/sbin/</span>hadoop-deamon.sh
<span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-yarn-resourcemanager<span class="regexp">/sbin/y</span>arn-daemon.sh
<span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-yarn-nodemanager<span class="regexp">/sbin/y</span>arn-daemon.sh
</code></pre><p>Configuration files</p>
<pre><code><span class="regexp">/etc/</span>hadoop<span class="regexp">/conf</span>
</code></pre><p>Bin Scripts</p>
<pre><code><span class="regexp">/usr/</span>bin<span class="regexp">/hadoop -&gt; /u</span>sr<span class="regexp">/hdp/</span>current<span class="regexp">/hadoop-client/</span>bin<span class="regexp">/hadoop</span>
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>近期动态：<br>Hadoop 2.7发布。<br>Hortonworks HDP 2.2.4.2发布。<br>Ambari 2.0发布。<br>Cloudera Enterperise 5.4发布。<br>Hive 1.2.0 发布，支持Hive on Spark。</p]]>
    </summary>
    
      <category term="Ambari" scheme="http://navigating.github.io/tags/Ambari/"/>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201502]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201502/"/>
    <id>http://navigating.github.io/2015/大数据动态之201502/</id>
    <published>2015-03-24T14:10:07.000Z</published>
    <updated>2015-08-18T06:42:18.267Z</updated>
    <content type="html"><![CDATA[<p>本月Hadoop技术动态：<br>1.经过6年的孵化，Hive 1.0 发布了。<br>2.经过7年的孵化，HBase 1.0 发布了。<br>3.Cloudera 开始提供 Hive-on-Spark Beta版的下载。</p>
<p>HBase 1.0 需要特别关注的特性：<br>1.API的重新组织和变更；<br>2.读的高可用；<br>3.在线配置变更；</p>
<p>HDP 2.2 发布有一段时间：<br><a href="http://hortonworks.com/blog/announcing-hive-1-0-stable-moment-time/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-hive-1-0-stable-moment-time/</a><br><a href="http://hortonworks.com/blog/start-new-era-apache-hbase-1-0/" target="_blank" rel="external">http://hortonworks.com/blog/start-new-era-apache-hbase-1-0/</a><br><a href="http://blog.cloudera.com/blog/2015/02/apache-hbase-1-0-is-released/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/apache-hbase-1-0-is-released/</a><br><a href="http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/</a><br><a href="https://issues.apache.org/jira/secure/attachment/12652517/Hive-on-Spark.pdf" target="_blank" rel="external">https://issues.apache.org/jira/secure/attachment/12652517/Hive-on-Spark.pdf</a></p>
<p>Cluster Manager Framework:<br>1.YARN<br>2.Apache Helix</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>本月Hadoop技术动态：<br>1.经过6年的孵化，Hive 1.0 发布了。<br>2.经过7年的孵化，HBase 1.0 发布了。<br>3.Cloudera 开始提供 Hive-on-Spark Beta版的下载。</p>
<p>HBase 1.0 需要特别关注的特]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop发行版(2015第一季)]]></title>
    <link href="http://navigating.github.io/2015/Hadoop%E5%8F%91%E8%A1%8C%E7%89%88(2015%E7%AC%AC%E4%B8%80%E5%AD%A3)/"/>
    <id>http://navigating.github.io/2015/Hadoop发行版(2015第一季)/</id>
    <published>2015-01-09T14:40:02.000Z</published>
    <updated>2015-08-11T14:54:26.940Z</updated>
    <content type="html"><![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" target="_blank" rel="external">http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support</a><br>其中在最有名为人所知的三家：<br>1.Cloudera<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_1.JPG" alt="这是一张图片"></p>
<p>2.Hortonwork<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_2.JPG" alt="这是一张图片"></p>
<p>3.MapR<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_3.JPG" alt="这是一张图片"></p>
<p>这三个厂商之中，MapR最为封闭；Hortonworks最为开放，产品线全开源，在线文档比较丰富。国内使用Cloudera CDH和Hortonworks的应该是最多的。<br>准实时计算框架/即席查询<br>1.CDH的框架有：Impala + Spark；<br>2.HDP的框架有：Tez + Spark；<br>3.MapR的框架有：Drill + Tez + Spark。<br>关于Spark：<br>2014年大数据最热门的技术路线就是算是Spark了，而且得力于Spark不遗余力的推广和快速成长。Cloudera是最早支持Spark，也是最激进的。下图即是Spark在Cloudera产品线中的定位：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_4.JPG" alt="这是一张图片"></p>
<p>实际上快速计算框架的发展才刚刚开始，社区中已经有如下几种：<br>1.Spark/Shark<br>2.Hortonworks Tez/Stinger<br>3.Cloudera Impala<br>4.Apache Drill<br>5.Apache Flink<br>6.Apache Nifi<br>7.Facebook Presto</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="SQL on Hadoop" scheme="http://navigating.github.io/tags/SQL-on-Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
</feed>