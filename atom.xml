<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[On The Open Way]]></title>
  <subtitle><![CDATA[自信人生二百年，会当水击三千里！]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://navigating.github.io//"/>
  <updated>2015-08-07T08:37:53.504Z</updated>
  <id>http://navigating.github.io//</id>
  
  <author>
    <name><![CDATA[Steven Xu]]></name>
    <email><![CDATA[xxx@qq.com]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[学习《腾讯在Spark上的应用与实践优化》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8A%E8%85%BE%E8%AE%AF%E5%9C%A8Spark%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%E4%BC%98%E5%8C%96%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《腾讯在Spark上的应用与实践优化》/</id>
    <published>2015-08-07T08:28:42.000Z</published>
    <updated>2015-08-07T08:37:53.504Z</updated>
    <content type="html"><![CDATA[<p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.net/detail/happytofly/8637461</a></p>
<p>TDW: Tencent Distributed Data Warehouse，腾讯分布式数据仓库；<br>GAIA：腾讯自研的基于YARN定制化和优化的资源管理系统；<br>Lhoste：腾讯自研的作业的工作流调度系统，类似于Oozie；<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_1.JPG" alt=""></p>
<p>TDW集群规模：</p>
<pre><code><span class="bullet">1. </span>Gaia集群节点数：8000+；
<span class="bullet">2. </span>HDFS的存储空间：150PB+；
<span class="bullet">3. </span>每天新增数据：1PB+；
<span class="bullet">4. </span>每天任务数：1M+；
<span class="bullet">5. </span>每天计算量：10PB+；
</code></pre><p>Spark集群：</p>
<pre><code><span class="bullet">1. </span>Spark部署在Gaia之上，即是Spark on YARN模式，每个节点是 24 cores 和 60G 内存；
<span class="bullet">2. </span>底层存储包括：HDFS、HBase、Hive、MySQL；
<span class="bullet">3. </span>作业类型，包括：ETL、SparkSQL、Machine Learning、Graph Compute、Streaming；
<span class="bullet">4. </span>每天任务数，10K+；
<span class="bullet">5. </span>腾讯从2013年开始引入Spark 0.6，已经使用2年了；
</code></pre><p>Spark的典型应用：</p>
<pre><code><span class="bullet">1. </span>预测用户的广告点击概率；
<span class="bullet">2. </span>计算两个好友间的共同好友数；
<span class="bullet">3. </span>用于ETL的SparkSQL和DAG任务；
</code></pre><p>Case 1: 预测用户的广告点击概率<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_4.JPG" alt=""></p>
<pre><code><span class="number">1</span>. 数据是通过<span class="function"><span class="title">DCT</span><span class="params">(Data Collect Tool)</span></span>推送到HDFS上，然后Spark直接将HDFS数据导入到 RDD&amp;Cache；
<span class="number">2</span>. <span class="number">60</span>次迭代计算的时间为<span class="number">10</span>～<span class="number">15</span>分钟，即每次迭代<span class="number">10</span>～<span class="number">15</span>秒；
</code></pre><p>Case 2: 计算两个好友间的共同好友数</p>
<pre><code>1. 根据shuffle数量来确定partition数量；
2. 尽量使用sort-based shuffle，减少reduce的内存使用；
3. 当连接超时后选择重试来减少executor丢失的概率；
4. 避免executor被YARN给<span class="operator"><span class="keyword">kill</span>掉，设置 spark.yarn.executor.memoryoverhead
<span class="number">5.</span> 执行语句 <span class="keyword">INSERT</span> <span class="keyword">TABLE</span> test_result <span class="keyword">SELECT</span> t3.d, <span class="keyword">COUNT</span>(*) FROＭ( <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> a, b <span class="keyword">FROM</span> join_1 ) t1 <span class="keyword">JOIN</span> （<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> b, c <span class="keyword">FROM</span> join_2 ) t2 <span class="keyword">ON</span> (t1.a = t2.c) <span class="keyword">JOIN</span> (<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c, d <span class="keyword">FROM</span> c, d <span class="keyword">FROM</span> join_3 ) t3 <span class="keyword">ON</span> (t2.b = t3.d) <span class="keyword">GROUP</span> <span class="keyword">BY</span> t3.d 使用Hive需要<span class="number">30</span>分钟，使用SparkSQL需要<span class="number">5</span>分钟；
<span class="number">6.</span> 当有小表时使用broadcase <span class="keyword">join</span>代替Common <span class="keyword">join</span>；
<span class="number">7.</span> 尽量使用ReduceByKey代替GroupByKey；
<span class="number">8.</span> 设置spark.serializer = org.apache.spark.serializer.KryoSerializer；
<span class="number">9.</span> 使用YARN时，设置spark.shuffle.service.enabled = <span class="literal">true</span>；
<span class="number">10.</span> 在早期版本中Spark通过启动参数固定executor的数量，当前支持动态资源扩缩容特性

    * spark.dynamicAllocation.enabled = <span class="literal">true</span>
    * spark.dynamicAllocation.executorIdleTimeout = <span class="number">120</span>
    * spark.dynamicAllocation.schedulerBacklogTimeout = <span class="number">10</span>
    * spark.dynamicAllocation.minExecutors/maxExecutors

<span class="number">11.</span> 当申请固定的executors时且task数大于executor数时，存在着资源的空闲状态。</span>
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_5.JPG" alt=""><br>&lt;完&gt;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[13~14年收集的大数据的一些技术架构图]]></title>
    <link href="http://navigating.github.io/2015/13-14%E5%B9%B4%E6%94%B6%E9%9B%86%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E5%9B%BE/"/>
    <id>http://navigating.github.io/2015/13-14年收集的大数据的一些技术架构图/</id>
    <published>2015-08-05T05:16:53.000Z</published>
    <updated>2015-08-05T05:38:56.350Z</updated>
    <content type="html"><![CDATA[<p>1 Big Data Solution</p>
<p>1.1 HP</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_01.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_02.png" alt=""></p>
<p>1.2 Oracle</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_03.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_04.JPG" alt=""></p>
<p>1.3 IBM</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_05.jpg" alt=""></p>
<p>1.4 Microsoft</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_06.png" alt=""></p>
<p>1.5 Huawei</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_07.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_08.jpg" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_09.jpg" alt=""></p>
<p>2 Big Data on Cloud</p>
<p>2.1 Amazon AWS</p>
<p>2.1.1 Netflix BigData on AWS<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_10.png" alt=""></p>
<p>2.2 Microsoft Azure</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_11.png" alt=""></p>
<p>2.3 Facebook</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_12.png" alt=""></p>
<p>2.4 Linkedin</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_13.png" alt=""></p>
<p>2.5 Twitter</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_14.png" alt=""></p>
<p>2.6 Alibaba/Taobao</p>
<p>2.6.1 淘宝数据魔方<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_15.png" alt=""></p>
<p>2.6.2 阿里大数据应用平台<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_16.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_17.png" alt=""></p>
<p>2.6.3 阿里搜索实时流计算<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_18.png" alt=""></p>
<p>2.7 Tencent</p>
<p>2.7.1 腾讯大规模Hadoop集群TDW<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_19.png" alt=""></p>
<p>2.7.2 腾讯实时计算平台 广点通<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_20.png" alt=""></p>
<p>2.8 JD(京东)</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_21.png" alt=""></p>
<p>2.9 CMCC(中国移动)</p>
<p>2.9.1 大云PaaS 2.5<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_22.png" alt=""></p>
<p>3 Hadoop Distribution</p>
<p>3.1 Apache Hadoop</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_23.png" alt=""></p>
<p>3.2 Cloudera</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_24.png" alt=""></p>
<p>3.3 Hortonworks</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_25.png" alt=""></p>
<p>3.4 MapR</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_26.png" alt=""></p>
<p>3.5 Intel</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_27.png" alt=""></p>
<p>3.6 EMC Pivotal HD</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_28.jpg" alt=""></p>
<p>3.7 IBM</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_29.jpg" alt=""></p>
<p>3.8 Huawei</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_30.jpg" alt=""></p>
<p>4 Landscape</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_31.jpg" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_32.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_33.png" alt=""></p>
<p>5 参考</p>
<ul>
<li><a href="http://www-01.ibm.com/software/data/bigdata/platform/resources.html" target="_blank" rel="external">IBM Report</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1E7OTT7&amp;ct=130225&amp;st=sb" target="_blank" rel="external">Gartner - Hadoop Is Not a Data Integration Solution</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1DBWMQY&amp;ct=121220&amp;st=sb" target="_blank" rel="external">Gartner - Magic Quadrant for Data Masking Technology 2012</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1IMDMZ5&amp;ct=130819&amp;st=sb" target="_blank" rel="external"> Magic Quadrant for Cloud Infrastructure as a Service 2013</a></li>
<li><a href="http://wenku.it168.com/d_000048434.shtml" target="_blank" rel="external">Facebook Hadoop</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>1 Big Data Solution</p>
<p>1.1 HP</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_0]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MapR" scheme="http://navigating.github.io/tags/MapR/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[读《微软研发制胜策略》]]></title>
    <link href="http://navigating.github.io/2015/%E8%AF%BB%E3%80%8A%E5%BE%AE%E8%BD%AF%E7%A0%94%E5%8F%91%E5%88%B6%E8%83%9C%E7%AD%96%E7%95%A5%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/读《微软研发制胜策略》/</id>
    <published>2015-08-04T14:30:45.000Z</published>
    <updated>2015-08-04T14:31:31.555Z</updated>
    <content type="html"><![CDATA[<p>软件开发的核心就是：达成项目目标，提高生产率，提高软件的质量。除此之外，都不要重要。<br>管理上、复用上，一切的核心就是人的问题，提高人的能力是第一生产力。</p>
<p>1.项目中一个现象就是紧紧的去控制进度，调整进度，进度的跟踪只是一种日常的事务工作。<br>2.观念的改变是第一位的，什么是观念改变的原则：规则不是法律，是可以触碰的。什么是我们要改变的规则，就是要有主动、计划、灵活。<br>3.紧密的进度计划，是一般的管理人员的通常做法，他的好处就是看到不断的工作，会有不断的压力；如果运用不当，就可能让人觉得厌烦和沮丧。<br>4.为了日程进度，牺牲质量往往是不值得的，除非你要一笑而过的做法。再不管这个项目的后续开发和维护了。对于产品或者项目的期限，要谨慎，要反思可能为了进度而牺牲质量。这叫着草率的期限。<br>5.一个好的日程表会兼顾公司和员工的利益的。<br>6.没有期限的目标不过是梦想而已。<br>7.把一个大项目，切分成n个小项目来做，每一个项目的周期大约是2个月。叫着阶段式的日程控制法。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>软件开发的核心就是：达成项目目标，提高生产率，提高软件的质量。除此之外，都不要重要。<br>管理上、复用上，一切的核心就是人的问题，提高人的能力是第一生产力。</p>
<p>1.项目中一个现象就是紧紧的去控制进度，调整进度，进度的跟踪只是一种日常的事务工作。<br>2.观念]]>
    </summary>
    
      <category term="读书" scheme="http://navigating.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop发行版]]></title>
    <link href="http://navigating.github.io/2015/Hadoop%E5%8F%91%E8%A1%8C%E7%89%88/"/>
    <id>http://navigating.github.io/2015/Hadoop发行版/</id>
    <published>2015-08-01T14:40:02.000Z</published>
    <updated>2015-08-04T07:08:40.853Z</updated>
    <content type="html"><![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" target="_blank" rel="external">http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support</a><br>其中在最有名为人所知的三家：<br>1.Cloudera<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_1.JPG" alt="这是一张图片"></p>
<p>2.Hortonwork<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_2.JPG" alt="这是一张图片"></p>
<p>3.MapR<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_3.JPG" alt="这是一张图片"></p>
<p>这三个厂商之中，MapR最为封闭；Hortonworks最为开放，产品线全开源，在线文档比较丰富。国内使用Cloudera CDH和Hortonworks的应该是最多的。<br>准实时计算框架/即席查询<br>1.CDH的框架有：Impala + Spark；<br>2.HDP的框架有：Tez + Spark；<br>3.MapR的框架有：Drill + Tez + Spark。<br>关于Spark：<br>2014年大数据最热门的技术路线就是算是Spark了，而且得力于Spark不遗余力的推广和快速成长。Cloudera是最早支持Spark，也是最激进的。下图即是Spark在Cloudera产品线中的定位：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_4.JPG" alt="这是一张图片"></p>
<p>实际上快速计算框架的发展才刚刚开始，社区中已经有如下几种：<br>1.Spark/Shark<br>2.Hortonworks Tez/Stinger<br>3.Cloudera Impala<br>4.Apache Drill<br>5.Apache Flink<br>6.Apache Nifi<br>7.Facebook Presto</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="SQL on Hadoop" scheme="http://navigating.github.io/tags/SQL-on-Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201507]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201507/"/>
    <id>http://navigating.github.io/2015/大数据动态之201507/</id>
    <published>2015-07-31T08:22:01.000Z</published>
    <updated>2015-08-01T02:27:01.570Z</updated>
    <content type="html"><![CDATA[<p>Hortonworks<br>HDP 2.3发布：<br>HDP 2.3新增加组件Apache Atlas、Apache Calcite<br><a href="http://hortonworks.com/blog/available-now-hdp-2-3/" target="_blank" rel="external">http://hortonworks.com/blog/available-now-hdp-2-3/</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-2/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-2/</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/</a><br>Spark 1.2开始支持ORC(Columnar Formats)<br><a href="http://hortonworks.com/blog/bringing-orc-support-into-apache-spark/" target="_blank" rel="external">http://hortonworks.com/blog/bringing-orc-support-into-apache-spark/</a><br>Spark in HDInsight新特性一览<br><a href="http://hortonworks.com/blog/spark-in-hdinsight/" target="_blank" rel="external">http://hortonworks.com/blog/spark-in-hdinsight/</a> </p>
<p>Cloudera<br>HBase 1.0 开始支持Thrift客户端鉴权<br><a href="http://blog.cloudera.com/blog/2015/07/thrift-client-authentication-support-in-apache-hbase-1-0/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/thrift-client-authentication-support-in-apache-hbase-1-0/</a><br>Pig on MR优化<br><a href="http://blog.cloudera.com/blog/2015/07/how-to-tune-mapreduce-parallelism-in-apache-pig-jobs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/how-to-tune-mapreduce-parallelism-in-apache-pig-jobs/</a><br>Apache Zeppelin on CDH<br><a href="http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/</a><br>大数据欺诈检测架构<br><a href="http://blog.cloudera.com/blog/2015/07/designing-fraud-detection-architecture-that-works-like-your-brain-does/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/designing-fraud-detection-architecture-that-works-like-your-brain-does/</a> </p>
<p>MapR<br>YARN资源管理实践<br><a href="https://www.mapr.com/blog/best-practices-yarn-resource-management" target="_blank" rel="external">https://www.mapr.com/blog/best-practices-yarn-resource-management</a><br>Hive 1.0对Transaction的支持<br><a href="https://www.mapr.com/blog/hive-transaction-feature-hive-10" target="_blank" rel="external">https://www.mapr.com/blog/hive-transaction-feature-hive-10</a> </p>
<p>Databricks<br>Spark Streaming执行模型<br><a href="https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html</a><br>Spark 1.4 MLP新特性<br><a href="https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html</a><br>从Spark 1.2开始支持ORC<br><a href="https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html</a><br>从Spark 1.4开始支持窗口函数<br><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br>从Spark 1.4开始新的Web UI<br><a href="https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html</a> </p>
<p>Phoenix对join的支持，TPC in Apache Phoenix<br><a href="https://blogs.apache.org/phoenix/entry/tpc_in_apache_phoenix" target="_blank" rel="external">https://blogs.apache.org/phoenix/entry/tpc_in_apache_phoenix</a> </p>
<p>Cassandra<br><a href="http://cassandra.apache.org/" target="_blank" rel="external">http://cassandra.apache.org/</a> </p>
<p>mongoDB<br><a href="https://www.mongodb.org/" target="_blank" rel="external">https://www.mongodb.org/</a> </p>
<p>Confluent<br>基于Kafka的实时流处理<br><a href="http://www.confluent.io/" target="_blank" rel="external">http://www.confluent.io/</a><br>大数据生态系统之Kafka价值<br><a href="http://www.confluent.io/blog/the-value-of-apache-kafka-in-big-data-ecosystem/" target="_blank" rel="external">http://www.confluent.io/blog/the-value-of-apache-kafka-in-big-data-ecosystem/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hortonworks<br>HDP 2.3发布：<br>HDP 2.3新增加组件Apache Atlas、Apache Calcite<br><a href="http://hortonworks.com/blog/available-now-hdp-2-3/" targ]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Cassandra" scheme="http://navigating.github.io/tags/Cassandra/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="mongoDB" scheme="http://navigating.github.io/tags/mongoDB/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用Hexo搭建Github静态博客]]></title>
    <link href="http://navigating.github.io/2015/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BAGithub%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2/"/>
    <id>http://navigating.github.io/2015/使用Hexo搭建Github静态博客/</id>
    <published>2015-07-28T09:20:22.000Z</published>
    <updated>2015-08-01T05:37:12.996Z</updated>
    <content type="html"><![CDATA[<p>环境：</p>
<pre><code><span class="bullet">1. </span>Windows XP
<span class="bullet">2. </span>Git
</code></pre><p>步骤：</p>
<pre><code><span class="bullet">1. </span>安装Node.js
<span class="bullet">2. </span>安装Hexo
<span class="bullet">3. </span>创建博客(初始化Hexo)
<span class="bullet">4. </span>创建文章本地调试
<span class="bullet">5. </span>配置Github
<span class="bullet">6. </span>远程发布
<span class="bullet">7. </span>支持sitemap和feed
<span class="bullet">8. </span>支持百度统计
<span class="bullet">9. </span>支持图片
<span class="bullet">10. </span>支持Swiftype站内搜索
<span class="bullet">11. </span>参考资源
</code></pre><h2 id="安装Node-js">安装Node.js</h2><p>下载并安装，<a href="https://nodejs.org/" target="_blank" rel="external">https://nodejs.org/</a></p>
<h2 id="安装Hexo">安装Hexo</h2><p>通过命令 npm install -g hexo 安装</p>
<pre><code><span class="attribute">D</span>:\git\hexo&gt;npm install -g hexo

npm WARN optional dep failed, continuing fsevents<span class="variable">@0</span>.<span class="number">3.6</span>
npm WARN optional dep failed, continuing fsevents<span class="variable">@0</span>.<span class="number">3.6</span>
-


&gt; dtrace-provider<span class="variable">@0</span>.<span class="number">5.0</span> install <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo\node_modules\bunyan\node_modules\dtrace-provider
&gt; node scripts/install.js

<span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\hexo -&gt; <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo\bin\hexo
hexo<span class="variable">@3</span>.<span class="number">1.1</span> <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo
├── pretty-hrtime<span class="variable">@1</span>.<span class="number">0.0</span>
├── hexo-front-matter<span class="variable">@0</span>.<span class="number">2.2</span>
├── abbrev<span class="variable">@1</span>.<span class="number">0.7</span>
├── titlecase<span class="variable">@1</span>.<span class="number">0.2</span>
├── archy<span class="variable">@1</span>.<span class="number">0.0</span>
├── text-table<span class="variable">@0</span>.<span class="number">2.0</span>
├── tildify<span class="variable">@1</span>.<span class="number">1.0</span> (os-homedir<span class="variable">@1</span>.<span class="number">0.1</span>)
├── strip-indent<span class="variable">@1</span>.<span class="number">0.1</span> (get-stdin<span class="variable">@4</span>.<span class="number">0.1</span>)
├── hexo-i18n<span class="variable">@0</span>.<span class="number">2.1</span> (sprintf-js<span class="variable">@1</span>.<span class="number">0.3</span>)
├── chalk<span class="variable">@1</span>.<span class="number">1.0</span> (escape-string-regexp<span class="variable">@1</span>.<span class="number">0.3</span>, supports-color<span class="variable">@2</span>.<span class="number">0.0</span>, ansi-styles<span class="variable">@2</span>.<span class="number">1.0</span>, strip-ansi<span class="variable">@3</span>.<span class="number">0.0</span>, has-ansi<span class="variable">@2</span>.<span class="number">0.0</span>)
├── bluebird<span class="variable">@2</span>.<span class="number">9.34</span>
├── minimatch<span class="variable">@2</span>.<span class="number">0.10</span> (brace-expansion<span class="variable">@1</span>.<span class="number">1.0</span>)
├── through2<span class="variable">@1</span>.<span class="number">1.1</span> (xtend<span class="variable">@4</span>.<span class="number">0.0</span>, readable-stream<span class="variable">@1</span>.<span class="number">1.13</span>)
├── swig-extras<span class="variable">@0</span>.<span class="number">0.1</span> (markdown<span class="variable">@0</span>.<span class="number">5.0</span>)
├── hexo-fs<span class="variable">@0</span>.<span class="number">1.3</span> (escape-string-regexp<span class="variable">@1</span>.<span class="number">0.3</span>, graceful-fs<span class="variable">@3</span>.<span class="number">0.8</span>, chokidar<span class="variable">@0</span>.<span class="number">12.6</span>)
├── js-yaml<span class="variable">@3</span>.<span class="number">3.1</span> (esprima<span class="variable">@2</span>.<span class="number">2.0</span>, argparse<span class="variable">@1</span>.<span class="number">0.2</span>)
├── nunjucks<span class="variable">@1</span>.<span class="number">3.4</span> (optimist<span class="variable">@0</span>.<span class="number">6.1</span>, chokidar<span class="variable">@0</span>.<span class="number">12.6</span>)
├── warehouse<span class="variable">@1</span>.<span class="number">0.2</span> (graceful-fs<span class="variable">@3</span>.<span class="number">0.8</span>, cuid<span class="variable">@1</span>.<span class="number">2.5</span>, JSONStream<span class="variable">@0</span>.<span class="number">10.0</span>)
├── cheerio<span class="variable">@0</span>.<span class="number">19.0</span> (entities<span class="variable">@1</span>.<span class="number">1.1</span>, dom-serializer<span class="variable">@0</span>.<span class="number">1.0</span>, css-select<span class="variable">@1</span>.<span class="number">0.0</span>, htmlparser2<span class="variable">@3</span>.<span class="number">8.3</span>)
├── bunyan<span class="variable">@1</span>.<span class="number">4.0</span> (safe-json-stringify<span class="variable">@1</span>.<span class="number">0.3</span>, dtrace-provider<span class="variable">@0</span>.<span class="number">5.0</span>, mv<span class="variable">@2</span>.<span class="number">1.1</span>)

├── hexo-cli<span class="variable">@0</span>.<span class="number">1.7</span> (minimist<span class="variable">@1</span>.<span class="number">1.2</span>)
├── moment-timezone<span class="variable">@0</span>.<span class="number">3.1</span>
├── moment<span class="variable">@2</span>.<span class="number">10.3</span>
├── hexo-util<span class="variable">@0</span>.<span class="number">1.7</span> (ent<span class="variable">@2</span>.<span class="number">2.0</span>, highlight.js<span class="variable">@8</span>.<span class="number">6.0</span>)
├── swig<span class="variable">@1</span>.<span class="number">4.2</span> (optimist<span class="variable">@0</span>.<span class="number">6.1</span>, uglify-js<span class="variable">@2</span>.<span class="number">4.24</span>)
└── lodash<span class="variable">@3</span>.<span class="number">10.0</span>

<span class="attribute">D</span>:\git\hexo&gt;
</code></pre><h2 id="创建博客(初始化hexo)">创建博客(初始化hexo)</h2><p>创建博客站点的本地目录，然后在文件夹下执行命令：</p>
<pre><code><span class="variable">$ </span>hexo init
</code></pre><p>[info] Copying data<br>[info] You are almost done! Don’t forget to run <code>npm install</code> before you start b<br>logging with Hexo!</p>
<p>Hexo会自动在目标文件夹下建立网站所需要的文件。然后按照提示，安装node_modules，执行如下命令：</p>
<pre><code>$ hexo <span class="keyword">install</span>
</code></pre><h2 id="创建文章本地调试">创建文章本地调试</h2><p>预览本地调试模式，执行如下命令：</p>
<pre><code>$ hexo <span class="keyword">server</span>
</code></pre><p>[info] Hexo is running at <a href="http://localhost:4000/" target="_blank" rel="external">http://localhost:4000/</a>. Press Ctrl+C to stop.</p>
<p>关键命令简介：</p>
<pre><code><span class="title">hexo</span> n     <span class="comment">#创建新的文章</span>
hexo g     <span class="comment">#重新生成站点</span>
hexo s     <span class="comment">#启动本地服务</span>
hexo d     <span class="comment">#发布到github</span>
</code></pre><p>创建文章</p>
<pre><code>$ hexo <span class="keyword">new</span> <span class="string">"使用Hexo搭建Github静态博客"</span> 
</code></pre><p>在Hexo工作文件夹下source_posts发现新创建的md文件 使用Hexo搭建Github静态博客.md 。</p>
<h2 id="配置Github">配置Github</h2><p>部署到Github需要修改配置文件_config.yml文件，在Hexo工作目录之下：</p>
<pre><code># Deployment
## <span class="string">Docs:</span> <span class="string">http:</span><span class="comment">//hexo.io/docs/deployment.html</span>
<span class="label">
deploy:</span>
<span class="label">    type:</span> git
<span class="label">    repository:</span> git<span class="annotation">@github</span>.<span class="string">com:</span>&lt;Your Github Username&gt;/&lt;Your github.io url&gt;
<span class="label">    branch:</span> master
</code></pre><p>注意，当前type为git，而不是github</p>
<p>测试Github是否好用    </p>
<pre><code><span class="title">ssh</span> -T git<span class="variable">@github</span>.com
</code></pre><h2 id="远程发布">远程发布</h2><p>远程部署到Github，通过执行如下命令：    </p>
<pre><code><span class="variable">$ </span>hexi deploy
</code></pre><p>Troubleshooting<br>出现错误：Error: spawn git ENOENT<br>解决方案：<br><a href="http://blog.csdn.net/rainloving/article/details/46595559" target="_blank" rel="external">http://blog.csdn.net/rainloving/article/details/46595559</a> </p>
<p>使用github出现：fatal: unable to access: Failed connect to github.com:8080: No error<br>解决方案：<br><a href="http://www.zhihu.com/question/26954892" target="_blank" rel="external">http://www.zhihu.com/question/26954892</a> </p>
<p>使用github出现：ssh:connect to host github.com port 22: Bad file number<br>解决方案：<br><a href="http://www.xnbing.org/?p=759" target="_blank" rel="external">http://www.xnbing.org/?p=759</a><br><a href="http://blog.csdn.net/temotemo/article/details/7641883" target="_blank" rel="external">http://blog.csdn.net/temotemo/article/details/7641883</a> </p>
<h2 id="支持sitemap和feed">支持sitemap和feed</h2><p>首先安装sitemap和feed插件</p>
<pre><code>$ npm <span class="keyword">install</span> hexo-generator-sitemap
$ npm <span class="keyword">install</span> hexo-generator-feed
</code></pre><p>修改配置，在文件 _config.yml 增加以下内容</p>
<pre><code><span class="preprocessor"># Extensions</span>
<span class="label">Plugins:</span>
- hexo-generator-feed
- hexo-generator-sitemap

<span class="preprocessor">#Feed Atom</span>
<span class="label">feed:</span>
    type: atom
    path: atom.xml
    limit: <span class="number">20</span>

<span class="preprocessor">#sitemap</span>
<span class="label">sitemap:</span>
    path: sitemap.xml
</code></pre><p>在 themes\landscape_config.yml 中添加：</p>
<pre><code><span class="attribute">menu</span>:
    <span class="attribute">Home</span>: /
    <span class="attribute">Archives</span>: /archives
    <span class="attribute">Sitemap</span>: /sitemap.xml
<span class="attribute">rss</span>: /atom.xml
</code></pre><h2 id="支持百度统计">支持百度统计</h2><p>在 <a href="http://tongji.baidu.com" target="_blank" rel="external">http://tongji.baidu.com</a> 注册帐号，添加网站，生成统计功能的 JS 代码。</p>
<p>在 themes\landscape_config.yml 中新添加一行：</p>
<pre><code><span class="keyword">baidu_t</span>ongji: <span class="keyword">true</span>
</code></pre><p>在 themes\landscape\layout_partial\head.ejs 中head的结束标签  之前新添加一行代码</p>
<pre><code>&lt;<span class="preprocessor">%</span>- partial<span class="comment">('baidu_tongji')</span> <span class="preprocessor">%</span>&gt;
</code></pre><p>在 themes\landscape\layout_partial 中新创建一个文件 baidu_tongji.ejs 并添加如下内容：</p>
<pre><code><span class="xml"></span>&lt;%<span class="ruby"> <span class="keyword">if</span> (theme.baidu_tongji){ </span>%&gt;<span class="xml">
<span class="tag">&lt;<span class="title">script</span> <span class="attribute">type</span>=<span class="value">"text/javascript"</span>&gt;</span><span class="apache">
    <span class="tag">&lt;百度统计的 JS 代码&gt;</span>
</span><span class="tag">&lt;/<span class="title">script</span>&gt;</span>
</span>&lt;%<span class="ruby"> } </span>%&gt;<span class="xml"></span>
</code></pre><p>添加统计，参考：<br><a href="http://ibruce.info/2013/11/22/hexo-your-blog/" target="_blank" rel="external">http://ibruce.info/2013/11/22/hexo-your-blog/</a><br><a href="http://www.cnblogs.com/zhcncn/p/4097881.html" target="_blank" rel="external">http://www.cnblogs.com/zhcncn/p/4097881.html</a> </p>
<h2 id="支持图片">支持图片</h2><p>在source目录下创建images目录，然后将图片放在其中。<br>在文章中引用本地图片的语法例如：</p>
<pre><code>![<span class="link_label">这是一张图片</span>](<span class="link_url">/images/2005_TuoZhanXunLian.jpg</span>)
</code></pre><h2 id="支持swiftype站内搜索">支持swiftype站内搜索</h2><p>在 <a href="http://siftype.com" target="_blank" rel="external">http://siftype.com</a> 注册一个帐号，按着网站引导流程就可以了。<br>安装 install code 代码到hexo，添加到 themes\landscape\layout_partial\after-footer.ejs，类似添加百度统计的代码。<br>然后回到swiftype网站对 install code 进行确认。通过会在下方弹出一条消息：</p>
<pre><code><span class="title">Installation</span> successfully activated
</code></pre><h2 id="添加robots-txt">添加robots.txt</h2><p><a href="http://blog.lmintlcx.com/post/blog-with-hexo.html" target="_blank" rel="external">http://blog.lmintlcx.com/post/blog-with-hexo.html</a> </p>
<h2 id="参考资源">参考资源</h2><p><a href="http://blog.lmintlcx.com/post/blog-with-hexo.html" target="_blank" rel="external">http://blog.lmintlcx.com/post/blog-with-hexo.html</a><br><a href="https://github.com/bruce-sha" target="_blank" rel="external">https://github.com/bruce-sha</a><br><a href="http://zipperary.com/2013/05/28/hexo-guide-2/" target="_blank" rel="external">http://zipperary.com/2013/05/28/hexo-guide-2/</a><br><a href="http://zipperary.com/2013/05/29/hexo-guide-3/" target="_blank" rel="external">http://zipperary.com/2013/05/29/hexo-guide-3/</a><br><a href="http://zipperary.com/2013/05/30/hexo-guide-4/" target="_blank" rel="external">http://zipperary.com/2013/05/30/hexo-guide-4/</a><br><a href="http://cnfeat.com/2014/05/10/2014-05-11-how-to-build-a-blog/" target="_blank" rel="external">http://cnfeat.com/2014/05/10/2014-05-11-how-to-build-a-blog/</a><br><a href="http://www.cnblogs.com/zhcncn/p/4097881.html" target="_blank" rel="external">http://www.cnblogs.com/zhcncn/p/4097881.html</a><br><a href="http://blog.moyizhou.cn/web/search-engine-for-static-pages/" target="_blank" rel="external">http://blog.moyizhou.cn/web/search-engine-for-static-pages/</a><br><a href="http://www.jerryfu.net/post/search-engine-for-hexo-with-swiftype.html" target="_blank" rel="external">http://www.jerryfu.net/post/search-engine-for-hexo-with-swiftype.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>环境：</p>
<pre><code><span class="bullet">1. </span>Windows XP
<span class="bullet">2. </span>Git
</code></pre><p>步骤：</p>
<pre><code><span ]]>
    </summary>
    
      <category term="blog" scheme="http://navigating.github.io/tags/blog/"/>
    
      <category term="github" scheme="http://navigating.github.io/tags/github/"/>
    
      <category term="hexo" scheme="http://navigating.github.io/tags/hexo/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://navigating.github.io/2015/hello-world/"/>
    <id>http://navigating.github.io/2015/hello-world/</id>
    <published>2015-07-27T09:20:22.000Z</published>
    <updated>2015-07-28T09:21:58.301Z</updated>
    <content type="html"><![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io]]>
    </summary>
    
      <category term="hexo" scheme="http://navigating.github.io/tags/hexo/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop 2.7.1 发布]]></title>
    <link href="http://navigating.github.io/2015/Hadoop-2-7-1-%E5%8F%91%E5%B8%83/"/>
    <id>http://navigating.github.io/2015/Hadoop-2-7-1-发布/</id>
    <published>2015-07-09T13:49:30.000Z</published>
    <updated>2015-07-30T13:50:50.764Z</updated>
    <content type="html"><![CDATA[<p>2015年7月6日，Apache Hadoop的稳定版本 2.7.1 正式发布。<br><a href="http://hadoop.apache.org/releases.html#Release+Notes" target="_blank" rel="external">http://hadoop.apache.org/releases.html#Release+Notes</a> </p>
<p>Hadoop 2.7的一个小版本发布了，本版本属于稳定版本。<br>修复了2.7.0中存在的131个bug。<br>这是2.7.x第一个稳定版本，增强的功能列表请通过2.7.0版本部分查看。<br>按着计划，下一个2.7.x的小版本是2.7.2.</p>
<p>原文：<br>06 July, 2015: Release 2.7.1 (stable) availableA point release for the 2.7 line. This release is now considered stable.<br>Please see the Hadoop 2.7.1 Release Notes for the list of 131 bug fixes and patches since the previous release 2.7.0. Please look at the 2.7.0 section below for the list of enhancements enabled by this first stable release of 2.7.x.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>2015年7月6日，Apache Hadoop的稳定版本 2.7.1 正式发布。<br><a href="http://hadoop.apache.org/releases.html#Release+Notes" target="_blank" rel="external"]]>
    </summary>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[读《Deploying Apache Kafka: A Practical FAQ》]]></title>
    <link href="http://navigating.github.io/2015/%E8%AF%BB%E3%80%8ADeploying-Apache-Kafka-A-Practical-FAQ%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/读《Deploying-Apache-Kafka-A-Practical-FAQ》/</id>
    <published>2015-07-02T14:57:45.000Z</published>
    <updated>2015-07-30T15:01:55.553Z</updated>
    <content type="html"><![CDATA[<p>Cloudera发布了Kafka的好文，《Deploying Apache Kafka: A Practical FAQ》，参见：<a href="http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-practical-faq" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-practical-faq</a></p>
<p>是否应当为Kafka Broker使用 固态硬盘 (SSD)<br>实际上使用SSD盘并不能显著地改善 Kafka 的性能，主要有两个原因：</p>
<pre><code>* Kafka写磁盘是异步的，不是同步的。就是说，除了启动、停止之外，Kafka的任何操作都不会去等待磁盘同步（sync）完成；而磁盘同步(disk syncs)总是在后台完成的。这就是为什么Kafka消息至少复制到三个副本是至关重要的，因为一旦单个副本崩溃，这个副本就会丢失数据无法同步写到磁盘。
* 每一个Kafka <span class="keyword">Partition</span>被存储为一个串行的WAL（<span class="keyword">Write</span> Ahead <span class="keyword">Log</span>）日志文件。因此，除了极少数的数据查询，Kafka中的磁盘读写都是串行的。现代的操作系统已经对串行读写做了大量的优化工作。
</code></pre><p>如何对Kafka Broker上持久化的数据进行加密<br>目前，Kafka不提供任何机制对Broker上持久化的数据进行加密。用户可以自己对写入到Kafka的数据进行加密，即是，生产者(Producers)在写Kafka之前加密数据，消费者(Consumers)能解密收到的消息。这就要求生产者(Producers)把加密协议(protocols)和密钥(keys)分享给消费者(Consumers)。<br>另外一种选择，就是使用软件提供的文件系统级别的加密，例如Cloudera Navigator Encrypt。Cloudera Navigator Encrypt是Cloudera企业版(Cloudera Enterprise)的一部分，在应用程序和文件系统之间提供了一个透明的加密层。<br>Apache Zookeeper正成为Kafka集群的一个痛点(pain point)，真的吗？<br>Kafka高级消费者(high-level consumer)的早期版本(0.8.1或更早)使用Zookeeper来维护读的偏移量(offsets，主要是Topic的每个Partition的读偏移量)。如果有大量生产者(consumers)同时从Kafka中读数据，对Kafka的读写负载可能就会超出它的容量，Zookeeper就变成一个瓶颈(bottleneck)。当然，这仅仅出现在一些很极端的案例中(extreme cases)，即有成百上千个消费者(consumers)在使用同一个Zookeeper集群来管理偏移量(offset)。<br>不过，这个问题已经在Kafka当前的版本(0.8.2)中解决。从版本0.8.2开始，高级消费者(high-level consumer)能够使用Kafka自己来管理偏移量(offsets)。本质上讲，它使用一个单独的Kafka Topic来管理最近的读偏移量(read offsets)，因此偏移量管理(offset management)不再要求Zookeeper必须存在。然后，用户将不得不面临选择是用Kafka还是Zookeeper来管理偏移量(offsets)，由消费者(consumer)配置参数 offsets.storage 决定。<br>Cloudera强烈推荐使用Kafka来存储偏移量。当然，为了保证向后兼容性，你可以继续选择使用Zookeeper存储偏移量。(例如，你可能有一个监控平台需要从Zookeeper中读取偏移量信息。) 假如你不得不使用Zookeeper进行偏移量(offset)管理，我们推荐你为Kafka集群使用一个专用的Zookeeper集群。假如一个专用的Zookeeper集群仍然有性能瓶颈，你依然可以通过在Zookeeper节点上使用固态硬盘(SSD)来解决问题。<br>Kafka是否支持跨数据中心的可用性<br>Kafka跨数据中心可用性的推荐解决方案是使用MirrorMaker(<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330" target="_blank" rel="external">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330</a> ) 。在你的每一个数据中心都搭建一个Kafka集群，在Kafka集群之间使用MirrorMaker来完成近实时的数据复制。<br>使用MirrorMaker的架构模式是为每一个”逻辑”的topic在每一个数据中心创建一个topic：例如，在逻辑上你有一个”clicks”的topic，那么你实际上有”DC1.clicks”和“DC2.clicks”两个topic(DC1和DC2指得是你的数据中心)。DC1向DC1.clicks中写数据，DC2向DC2.clicks中写数据。MirrorMaker将复制所有的DC1 topics到DC2，并且复制所有的DC2 topics到DC1。现在每个DC上的应用程序都能够访问写入到两个DC的事件。这个应用程序能够合并信息和处理相应的冲突。<br>另一种更复杂的模式是在每一个DC都搭建本地和聚合Kafka集群。这个模式已经被Linkedin使用，Linkedin Kafka运维团队已经在这篇Blog(<a href="https://engineering.linkedin.com/kafka/running-kafka-scale" target="_blank" rel="external">https://engineering.linkedin.com/kafka/running-kafka-scale</a> )中有详细的描述(参见“Tiers and Aggregation”)。<br>Kafka支持哪些类型的数据转换(data transformation)<br>数据流过的Kafka的时候，Kafka并不能进行数据转换。为了处理数据转换，我们推荐如下方法：</p>
<pre><code>* 对于简单事件处理，使用<span class="constant">Flume Kafka </span>integration(<span class="symbol">http:</span>/<span class="regexp">/blog.cloudera.com/blog</span><span class="regexp">/2014/</span><span class="number">11</span>/flafka-apache-flume-meets-apache-kafka-<span class="keyword">for</span>-event-processing )，并且写一个简单的<span class="constant">Apache Flume Interceptor。</span>
* 对于复杂(事件)处理，使用<span class="constant">Apache Spark Streaming从Kafka中</span>读数据和处理数据。
</code></pre><p>在这两种情况下，被转换或者处理的数据可被写会到新的Kafka Topic中，或者直接传送到数据的最终消费者(Consumer)那里。<br>对于实时事件处理模式更全面的描述，看看这篇文章(<a href="http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/</a> )。<br>如何通过Kafka发送大消息或者超大负荷量？<br>Cloudera的性能测试表明Kafka达到最大吞吐量的消息大小为10K左右。更大的消息将导致吞吐量下降。然后，在一些情况下，用户需要发送比10K大的多的消息。<br>如果消息负荷大小是每100s处理MB级别，我们推荐探索以下选择：</p>
<pre><code><span class="bullet">* </span>如果可以使用共享存储(HDFS、S3、NAS)，那么将超负载放在共享存储上，仅用Kafka发送负载数据位置的消息。
<span class="bullet">* </span>对于大消息，在写入Kafka之前将消息拆分成更小的部分，使用消息Key确保所有的拆分部分都写入到同一个partition中，以便于它们能被同一个消息着(Consumer)消费的到，在消费的时候将拆分部分重新组装成一个大消息。
</code></pre><p>在通过Kafka发送大消息时，请记住以下几点：<br>压缩配置</p>
<pre><code><span class="keyword">*</span> Kafka生产者(Producers)能够压缩消息。通过配置参数compression.codec确保压缩已经开启。有效的选项为<span class="string">"gzip"</span>和<span class="string">"snappy"</span>。
</code></pre><p>Broker配置</p>
<pre><code>* message.<span class="built_in">max</span>.<span class="keyword">bytes</span> (default: <span class="number">1000000</span>): Broker能够接受的最大消息。增加这个值以便于匹配你的最大消息。
* <span class="built_in">log</span>.<span class="keyword">segment</span>.<span class="keyword">bytes</span> (default: <span class="number">1</span>GB): Kafka数据文件的大小。确保它至少大于一条消息。默认情况下已经够用，一般最大的消息不会超过<span class="number">1</span>G大小。
* replica.fetch.<span class="built_in">max</span>.<span class="keyword">bytes</span> (default: <span class="number">1</span>MB): Broker间复制的最大的数据大小。这个值必须大于message.<span class="built_in">max</span>.<span class="keyword">bytes</span>，否则一个Broker接受到消息但是会复制失败，从而导致潜在的数据丢失。
</code></pre><p>Consumer配置</p>
<pre><code>* <span class="tag">fetch</span><span class="class">.message</span><span class="class">.max</span><span class="class">.bytes</span> (<span class="rule"><span class="attribute">default</span>:<span class="value"> <span class="number">1</span>MB): Consumer所读消息的最大大小。这个值应该大于或者等于Broker配置的message.max.bytes的值。</span></span>
</code></pre><p>其他方面的考虑：</p>
<pre><code>* <span class="tag">Broker</span>需要针对复制为每一个<span class="tag">partition</span>分配一个<span class="tag">replica</span><span class="class">.fetch</span><span class="class">.max</span><span class="class">.bytes</span>大小的缓存区。需要计算确认( <span class="tag">partition</span>的数量 * 最大消息的大小 )不会超过可用的内存，否则就会引发<span class="tag">OOMs</span>（内存溢出异常）。
* <span class="tag">Consumers</span>有同样的问题，因子参数为 <span class="tag">fetch</span><span class="class">.message</span><span class="class">.max</span><span class="class">.bytes</span> ：确认每一个<span class="tag">partition</span>的消费者针对最大的消息有足够可用的内存。
* 大消息可能引发更长时间的垃圾回收停顿(<span class="tag">garbage</span> <span class="tag">collection</span> <span class="tag">pauses</span>)(<span class="tag">brokers</span>需要申请更大块的内存)。注意观察<span class="tag">GC</span>日志和服务器日志。假如发现长时间的<span class="tag">GC</span>停顿导致<span class="tag">Kafka</span>丢失了<span class="tag">Zookeeper</span> <span class="tag">session</span>，你可能需要为<span class="tag">zookeeper</span><span class="class">.session</span><span class="class">.timeout</span><span class="class">.ms</span>配置更长的<span class="tag">timeout</span>值。
</code></pre><p>Kafka是否支持MQTT或JMS协议<br>目前，Kafka针对上述协议不提供直接支持。但是，用户可以自己编写Adaptors从MQTT或者JMS中读取数据，然后写入到Kafka中。</p>
<p>更多关于在CDH中使用Kafka的信息，下载Deployment Guide(<a href="http://www.cloudera.com/content/cloudera/en/resources/library/datasheet/kafka-reference-architecture.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/resources/library/datasheet/kafka-reference-architecture.html</a> ) 或者 观看webinar “Bringing Real-Time Data to Hadoop”(<a href="http://www.cloudera.com/content/cloudera/en/resources/library/recordedwebinar/kafka-webinar-recording.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/resources/library/recordedwebinar/kafka-webinar-recording.html</a> )。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Cloudera发布了Kafka的好文，《Deploying Apache Kafka: A Practical FAQ》，参见：<a href="http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-]]>
    </summary>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="Kafka" scheme="http://navigating.github.io/tags/Kafka/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201506]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201506/"/>
    <id>http://navigating.github.io/2015/大数据动态之201506/</id>
    <published>2015-06-09T13:52:23.000Z</published>
    <updated>2015-08-01T02:16:57.704Z</updated>
    <content type="html"><![CDATA[<p>Pinot：LinkedIn的实时数据分析系统<br><a href="http://www.infoq.com/cn/news/2014/10/linkdln" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/10/linkdln</a><br><a href="https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot" target="_blank" rel="external">https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot</a></p>
<p>Twitter Heron：Twitter发布新的大数据实时分析系统Heron<br><a href="http://geek.csdn.net/news/detail/33750" target="_blank" rel="external">http://geek.csdn.net/news/detail/33750</a><br><a href="http://www.longda.us/?p=529" target="_blank" rel="external">http://www.longda.us/?p=529</a> </p>
<p>Cloudera<br>HBase对MOBs( Moderate Objects, 主要是大小100K到10M的对象存储 )的支持<br><a href="http://blog.cloudera.com/blog/2015/06/inside-apache-hbases-new-support-for-mobs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/inside-apache-hbases-new-support-for-mobs/</a><br>准实时计算架构模式<br><a href="http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/</a><br>(翻译：<a href="http://zhuanlan.zhihu.com/donglaoshi/20082628" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20082628</a> )<br>CDH 5.4 新功能：敏感数据处理(Sensitive Data Redaction)<br><a href="http://blog.cloudera.com/blog/2015/06/new-in-cdh-5-4-sensitive-data-redaction/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/new-in-cdh-5-4-sensitive-data-redaction/</a> </p>
<p>Hortonworks<br>YARN的CapacityScheduler对Resource-preemption的支持<br><a href="http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/" target="_blank" rel="external">http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/</a><br>Hadoop集群对Multihoming的支持<br><a href="http://hortonworks.com/blog/multihoming-on-hadoop-yarn-clusters/" target="_blank" rel="external">http://hortonworks.com/blog/multihoming-on-hadoop-yarn-clusters/</a><br>HDP 2.3企业级HDFS数据加密<br><a href="http://hortonworks.com/blog/new-in-hdp-2-3-enterprise-grade-hdfs-data-at-rest-encryption/" target="_blank" rel="external">http://hortonworks.com/blog/new-in-hdp-2-3-enterprise-grade-hdfs-data-at-rest-encryption/</a><br>Apache Slider 0.80.0版本发布<br><a href="http://hortonworks.com/blog/announcing-apache-slider-0-80-0/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-apache-slider-0-80-0/</a><br>Apache Spark 1.3.1 on HDP 2.2<br><a href="http://hortonworks.com/blog/apache-spark-on-hdp-learn-try-and-do/" target="_blank" rel="external">http://hortonworks.com/blog/apache-spark-on-hdp-learn-try-and-do/</a><br><a href="http://hortonworks.com/hadoop-tutorial/using-apache-spark-technical-preview-with-hdp-2-2/" target="_blank" rel="external">http://hortonworks.com/hadoop-tutorial/using-apache-spark-technical-preview-with-hdp-2-2/</a><br>Ambari 2.0.1 和 HDP 2.2.6 发布<br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html</a><br><a href="http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_releasenotes_ambari_2.0.1.0/content/ch_relnotes-ambari-2.0.1.0.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_releasenotes_ambari_2.0.1.0/content/ch_relnotes-ambari-2.0.1.0.html</a></p>
<p>其他：<br>Graphite的百万Metrics实践之路<br><a href="http://calvin1978.blogcn.com/articles/graphite.html" target="_blank" rel="external">http://calvin1978.blogcn.com/articles/graphite.html</a><br>HBaseCon 2015 大会幻灯片 &amp; 视频<br><a href="http://hbasecon.com/archive.html" target="_blank" rel="external">http://hbasecon.com/archive.html</a><br>HBase在腾讯大数据的应用实践<br><a href="http://www.d1net.com/bigdata/news/353500.html" target="_blank" rel="external">http://www.d1net.com/bigdata/news/353500.html</a><br>从Spark到Hadoop的架构实践<br><a href="http://www.csdn.net/article/2015-06-08/2824889" target="_blank" rel="external">http://www.csdn.net/article/2015-06-08/2824889</a><br>56网大数据<br><a href="http://share.csdn.net/slides/10903" target="_blank" rel="external">http://share.csdn.net/slides/10903</a><br>七牛技术总监陈超：记Spark Summit China 2015<br><a href="http://www.csdn.net/article/2015-04-30/2824594-spark-summit-china-2015" target="_blank" rel="external">http://www.csdn.net/article/2015-04-30/2824594-spark-summit-china-2015</a><br>唯品会美研中心郭安琪：2015 Hadoop Summit见闻<br><a href="http://zhuanlan.zhihu.com/donglaoshi/20072576" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20072576</a><br>华为叶琪：论Spark Streaming的数据可靠性和一致性<br><a href="http://www.csdn.net/article/2015-06-12/2824938" target="_blank" rel="external">http://www.csdn.net/article/2015-06-12/2824938</a><br>Hadoop Summit 2015<br><a href="http://2015.hadoopsummit.org/san-jose/agenda/" target="_blank" rel="external">http://2015.hadoopsummit.org/san-jose/agenda/</a><br>Spark Summit 2015<br><a href="https://spark-summit.org/2015/" target="_blank" rel="external">https://spark-summit.org/2015/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Pinot：LinkedIn的实时数据分析系统<br><a href="http://www.infoq.com/cn/news/2014/10/linkdln" target="_blank" rel="external">http://www.infoq.com/cn/]]>
    </summary>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201505]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201505/"/>
    <id>http://navigating.github.io/2015/大数据动态之201505/</id>
    <published>2015-05-19T02:17:28.000Z</published>
    <updated>2015-08-01T09:57:53.841Z</updated>
    <content type="html"><![CDATA[<p>近期动态：<br>Hadoop 2.7发布。<br>Hortonworks HDP 2.2.4.2发布。<br>Ambari 2.0发布。<br>Cloudera Enterperise 5.4发布。<br>Hive 1.2.0 发布，支持Hive on Spark。</p>
<p>HDP 2.2/HDP 2.2.4/Ambari 2.0/Ambari 2.0.1</p>
<pre><code><span class="bullet">1. </span>HDP支持异构存储Heterogeneous storage，主要是对SSD的支持；
<span class="bullet">2. </span>Hive开始支持 ACID 事务，向企业级应用场景前进了一大步；
<span class="bullet">3. </span>HDP支持Spark 1.2.1；
<span class="bullet">4. </span>HDP支持通过DominantResourceCalculator对CPU的资源隔离与资源调度；
<span class="bullet">5. </span>Ambari 支持Blurprint，通过 REST API 管理和运维有更好的支持；
<span class="bullet">6. </span>Ambari 支持Stacks，通过Stacks方式来定义一系列的集成组件；
<span class="bullet">7. </span>Ambari 2.0支持HDP 2.2平台的Rolling Upgrades；
<span class="bullet">8. </span>Ambari 2.0支持安装、配置Apache Ranger；
<span class="bullet">9. </span>Ambari 2.0开始集成Ambari Alerts；
<span class="bullet">10. </span>Ambari 2.0开始集成Ambari Metrics，替代之前的Ganglia；
<span class="bullet">11. </span>Ambari 2.0开始支持User Views功能，User Views提供给运维人员更好的界面，包括Tez View、Capacity Scheduler View、Hive View、Pig View、Files View；
</code></pre><p>HDP 2.2之后部署的结构与之前有调整，新部署的结构与说明如下：</p>
<p>目录结构<br>从HDP 2.2之后，HDP安装后的目录结构发生了变化，之前安装后的Hadoop在/usr/lib目录下，现在变更到/usr/hdp目录下，结构如下：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"> &#123;code&#125;</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/hadoop/</span>conf</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/lib</span><br><span class="line">│   │   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop<span class="regexp">/lib/</span>native</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/libexec</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/man</span><br><span class="line">│   └── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop/sbin</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs/lib</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs/sbin</span><br><span class="line">│   └── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hadoop-hdfs/webapps</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/hbase/</span>conf</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase/doc</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase/include</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>hbase/lib</span><br><span class="line">└── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper/bin</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/zookeeper/</span>conf</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper/doc</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper/lib</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.0.0-2041/</span>zookeeper/man</span><br><span class="line"> &#123;code&#125;</span><br><span class="line"> &#123;code&#125;</span><br><span class="line"><span class="regexp">/usr/</span>hdp/<span class="number">2.2</span>.3.0-<span class="number">2611</span></span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/hadoop/</span>conf</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/lib</span><br><span class="line">│   │   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop<span class="regexp">/lib/</span>native</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/libexec</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/man</span><br><span class="line">│   └── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop/sbin</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs/lib</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs/sbin</span><br><span class="line">│   └── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hadoop-hdfs/webapps</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase/bin</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/hbase/</span>conf</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase/doc</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase/include</span><br><span class="line">│   ├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>hbase/lib</span><br><span class="line">└── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper/bin</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper<span class="regexp">/conf -&gt; /</span>etc<span class="regexp">/zookeeper/</span>conf</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper/doc</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper/lib</span><br><span class="line">├── <span class="regexp">/usr/</span>hdp<span class="regexp">/2.2.3.0-2611/</span>zookeeper/man</span><br><span class="line"> &#123;code&#125;</span><br></pre></td></tr></table></figure></p>
<p>管理活动版本<br>HDP 2.0之后推出了hdp-select服务，通过这个服务可以管理活动版本，默认就会安装hdp-select，可以通过hdp-select命令验证是否安装。<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; hdp-<span class="keyword">select</span><br><span class="line"></span>&gt; hdp-<span class="keyword">select </span>versions</span><br></pre></td></tr></table></figure></p>
<p>同样支持管理命令，例如：</p>
<pre><code>&gt; <span class="tag">hdp-select</span> <span class="tag">set</span> <span class="tag">hadoop-hdfs-datanode</span> 2<span class="class">.2</span><span class="class">.3</span><span class="class">.0-2600</span>
</code></pre><p>安装后的库、工具和脚本<br>库<br>HDP 2.0之前安装后库放在/usr/lib下，现在放在/usr/hdp/current下：</p>
<pre><code><span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-hdfs-namenode/
<span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-yarn-resourcemanager
<span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-mapreduce-client/hadoop-mapreduce-examples.jar
</code></pre><p>Daemon Scripts</p>
<pre><code><span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-hdfs-namenode<span class="regexp">/../</span>hadoop<span class="regexp">/sbin/</span>hadoop-deamon.sh
<span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-yarn-resourcemanager<span class="regexp">/sbin/y</span>arn-daemon.sh
<span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>hadoop-yarn-nodemanager<span class="regexp">/sbin/y</span>arn-daemon.sh
</code></pre><p>Configuration files</p>
<pre><code><span class="regexp">/etc/</span>hadoop<span class="regexp">/conf</span>
</code></pre><p>Bin Scripts</p>
<pre><code><span class="regexp">/usr/</span>bin<span class="regexp">/hadoop -&gt; /u</span>sr<span class="regexp">/hdp/</span>current<span class="regexp">/hadoop-client/</span>bin<span class="regexp">/hadoop</span>
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>近期动态：<br>Hadoop 2.7发布。<br>Hortonworks HDP 2.2.4.2发布。<br>Ambari 2.0发布。<br>Cloudera Enterperise 5.4发布。<br>Hive 1.2.0 发布，支持Hive on Spark。</p]]>
    </summary>
    
      <category term="Ambari" scheme="http://navigating.github.io/tags/Ambari/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="http://navigating.github.io/tags/Hive/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201502]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201502/"/>
    <id>http://navigating.github.io/2015/大数据动态之201502/</id>
    <published>2015-03-24T14:10:07.000Z</published>
    <updated>2015-08-01T02:27:00.393Z</updated>
    <content type="html"><![CDATA[<p>本月Hadoop技术动态：<br>1.经过6年的孵化，Hive 1.0 发布了。<br>2.经过7年的孵化，HBase 1.0 发布了。<br>3.Cloudera 开始提供 Hive-on-Spark Beta版的下载。</p>
<p>HBase 1.0 需要特别关注的特性：<br>1.API的重新组织和变更；<br>2.读的高可用；<br>3.在线配置变更；</p>
<p>HDP 2.2 发布有一段时间：<br><a href="http://hortonworks.com/blog/announcing-hive-1-0-stable-moment-time/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-hive-1-0-stable-moment-time/</a><br><a href="http://hortonworks.com/blog/start-new-era-apache-hbase-1-0/" target="_blank" rel="external">http://hortonworks.com/blog/start-new-era-apache-hbase-1-0/</a><br><a href="http://blog.cloudera.com/blog/2015/02/apache-hbase-1-0-is-released/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/apache-hbase-1-0-is-released/</a><br><a href="http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/</a><br><a href="https://issues.apache.org/jira/secure/attachment/12652517/Hive-on-Spark.pdf" target="_blank" rel="external">https://issues.apache.org/jira/secure/attachment/12652517/Hive-on-Spark.pdf</a></p>
<p>Cluster Manager Framework:<br>1.YARN<br>2.Apache Helix</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>本月Hadoop技术动态：<br>1.经过6年的孵化，Hive 1.0 发布了。<br>2.经过7年的孵化，HBase 1.0 发布了。<br>3.Cloudera 开始提供 Hive-on-Spark Beta版的下载。</p>
<p>HBase 1.0 需要特别关注的特]]>
    </summary>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[2013年年终总结]]></title>
    <link href="http://navigating.github.io/2013/2013%E5%B9%B4%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    <id>http://navigating.github.io/2013/2013年年终总结/</id>
    <published>2013-12-31T14:23:51.000Z</published>
    <updated>2015-08-04T14:25:21.847Z</updated>
    <content type="html"><![CDATA[<p>又到了一年总结的时候，今年倒是有点晚了。感觉今年的时间过得飞快，放佛没感觉到时光的流逝，一直没想总结这事，这两天才开始琢磨。</p>
<p>年初给自己的两个目标，一个是多读书，一个是多了解架构知识。</p>
<p>多读书，这点上今年倒是读了不少杂书，包括：</p>
<p>重读了金庸的几本武侠小说，几本书基本上以前也读过，再次阅读，一是回想当初读的那个情景、那个思绪映像，另外再读的时候完全换了视角了，以一种置身世外的解读的心态来看，常常揣摩主人公在场景下的心态与抉择；</p>
<p>重读了四大名著除红楼之外的三部，特别是《西游记》，是第一次去读，以前只翻过几页，都没看过，一直觉得应该当着故事会来读，真读起来，才发现书中有很多有意思的地方，比方说书的第一主角真应该算是孙悟空吧，还有唐僧、孙悟空、猪八戒、沙和尚的关系和观念，都很有意思，还有孙悟空的诚心打动了猪八戒，确是一本可以细读的书。</p>
<p>读了《明朝那些事儿》《浪潮之巅》《乔布斯传》《基业长青》《创新者的窘境》等，读的都很粗线条，大部分都没有细读和做笔记。</p>
<p>另外有Kindle读了一些书，Kindle读书确实很方便，对我来说唯一的缺陷就是翻页查找太不方便了，对于我这种记忆里不好的人来说比较累。</p>
<p>了解架构，关于架构的书读得少，了解一下架构的范围和大致概念，比较收获的是开始去理解EA和TOGAF，可能他们对工作不会起到很大的直接作用，但发现对工作有非常大的辅助作用。架构方面实际脚踏实地的做事上做的很欠缺。</p>
<p>工作上今年做的事，大部分都能反映到WIKI上去，这里就不说了，想对自己说的就是：太被动，主动做的事儿太少。</p>
<p>言而总之，总而言之，这一年在读书和工作上对自己的评价是，看的多，做的少，创新的东西更少。身体健康方面是：体重增加了，锻炼减少了。</p>
<p>今年给自己打分，也就60分吧。</p>
<p>说了过去，就得展望一下未来：</p>
<p>读书这块，当然要继续读，下一步可能会更聚焦，有目的性的去读一些书。少随意性的阅读，多一些专注的阅读。</p>
<p>同时会把博客捡起来，坚持写博客，记录自己的想法。</p>
<p>工作这块，现在还没有想好，脑子里就是一些可能大家都知道的关键词，希望自己能做到：多创新，多行动，就是坚持做一些没做过的事；要主动，要规划，要坚持；要能落地的事，多总结积累。</p>
<p>用这篇小文，记录那渐行渐远的2013，迎接蓬勃而来的2014。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>又到了一年总结的时候，今年倒是有点晚了。感觉今年的时间过得飞快，放佛没感觉到时光的流逝，一直没想总结这事，这两天才开始琢磨。</p>
<p>年初给自己的两个目标，一个是多读书，一个是多了解架构知识。</p>
<p>多读书，这点上今年倒是读了不少杂书，包括：</p>
<p>重]]>
    </summary>
    
      <category term="生活" scheme="http://navigating.github.io/categories/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[响应式网页设计(Responsive Web Design)]]></title>
    <link href="http://navigating.github.io/2013/%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BD%91%E9%A1%B5%E8%AE%BE%E8%AE%A1-Responsive-Web-Design/"/>
    <id>http://navigating.github.io/2013/响应式网页设计-Responsive-Web-Design/</id>
    <published>2013-08-11T15:04:46.000Z</published>
    <updated>2015-08-03T15:07:27.893Z</updated>
    <content type="html"><![CDATA[<p>响应式网页设计，也叫：自适应网页设计。2010年，由Ethan Marcotte提出这个名词(Responsive Web Design) ，指可以自动识别屏幕宽度、并做出相应调整的网页设计。其目的就是希望搭建一个网站能够满足兼容多种终端的需求，不再是为每一种终端定制开发一个版本。应该是，这个思想主要是应对当前Web应用在各种移动终端上展示的挑战的。</p>
<p>主要的应对策略：</p>
<p>允许网页宽度自动调整；<br>不允许绝对宽度；<br>使用相对大小的字体；<br>流动布局；<br>选择加载CSS；<br>CSS的@media规则；<br>图片的自适应。<br>参考：<br><a href="http://alistapart.com/article/responsive-web-design" target="_blank" rel="external">http://alistapart.com/article/responsive-web-design</a><br><a href="http://www.ruanyifeng.com/blog/2012/05/responsive_web_design.html" target="_blank" rel="external">http://www.ruanyifeng.com/blog/2012/05/responsive_web_design.html</a><br><a href="http://www.qianduan.net/responsive-web-design.html" target="_blank" rel="external">http://www.qianduan.net/responsive-web-design.html</a><br><a href="http://www.qianduan.net/media-type-and-media-query.html" target="_blank" rel="external">http://www.qianduan.net/media-type-and-media-query.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>响应式网页设计，也叫：自适应网页设计。2010年，由Ethan Marcotte提出这个名词(Responsive Web Design) ，指可以自动识别屏幕宽度、并做出相应调整的网页设计。其目的就是希望搭建一个网站能够满足兼容多种终端的需求，不再是为每一种终端定制开发一]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HBTC 2012 见闻]]></title>
    <link href="http://navigating.github.io/2012/HBTC-2012-%E8%A7%81%E9%97%BB/"/>
    <id>http://navigating.github.io/2012/HBTC-2012-见闻/</id>
    <published>2012-12-25T15:05:29.000Z</published>
    <updated>2015-08-03T15:06:54.617Z</updated>
    <content type="html"><![CDATA[<p>今年Hadoop大会，加上了BigData，全称“Hadoop&amp;Bigdata Technology Conference(Hadoop与大数据技术大会)”，随着“云计算”“Hadoop”“大数据”的热点，今年的大会事先很是期待，之后从中收获很多，坚信了我们在工作中的一些判断。从我的视角，今年HBTC主要覆盖如下的内容：<br>Hadoop与BigData主要的产品进展：<br>1.Hortonworks Hadoop/HBase<br>2.Intel Hadoop/Free Edition<br>3.Huawei Contributing Hadoop/HBase<br>4.Facebook Hadoop/HBase<br>5.Vmware Hadoop Virtualization<br>6.Oracle NoSQL<br>7.Taobao Tair<br>8.eBay Hadoop/HBase<br>意外的是Huawei对Hadoop社区的贡献度很大；Hortonworks只是一般性的宣讲，并没有重大的消息或者特性宣布。<br>Hadoop生态圈中本次被关注的技术：<br>1.Hadoop Security(Etu)<br>2.HIVE<br>3.HDFS Namenode<br>4.HBase<br>5.Pig<br>今年重点被关注的是HBase、Hive。<br>应用方面:<br>1.阿里Hadoop集群<br>a.3200台服务器，30K核，内存10TB，存储36K磁盘60PB。<br>b.支撑支付宝、CBU、聚划算、一淘、天猫、淘宝,1K+客户端/100+部门<br>c.Hadoop组件：Hive、Streaming MR、Mahout、Pig、HBase<br>d.客户端，用户/用户组权限管理/资源管理，申请/审批；云梯医生/JobTracker心跳频率/NameNode RPC性能指标；<br>e.数据采集：TimeTunnel分布式日志收集，DataX数据库同步，DBSync大表增量同步；<br>2.HIVE在腾讯分布式数据仓库<br>a.腾讯分布式数据仓库，简称：TDW；<br>b.基于Hadoop/Hive/PostgreSQL构建；<br>c.特性列表：存储和计算容灾/存储和计算线性扩展，SQL语言/SQL函数，过程语言，多维分析，MR，多种存储结构，SQL/MED，开发工具，任务调度系统，系统DB<br>d.TDW在Hive基础上进行的功能增强：    基于角色的权限管理；    兼容Oracle的分区功能；    窗口函数；多维分析；公用表表达式；DML(Update/Delete)；入库数据校验；命令行工具；DB存储引擎；SQL语法细节增强；Eclipse IDE开发环境/流程开发工具；自定义的存储格式；Hash Join；按行split；Order by limit优化；<br>3.阿里数据交换平台<br>a.平台能力：存储与计算的调度、元数据管理、数据建模、IDE；市场应用：应用市场、数据市场；数据管理：预警、质量监控、ODS、生命周期管理；数据开发：安全、审计、计量、监控；<br>b.分析可视化；数据可视化；<br>c.ODPS：开放；服务化；离线数据分析服务；<br>d.ODS：开放与共享；源头数据质量监控；元数据管理；<br>4.百度大数据平台<br>a.基础能力包括：分布式存储(KV/Table)；计算(批量计算/小批量计算/流式计算)；调度(底层资源管理/上层通用调度)；数据仓库体系(格式化/传输/数据仓库/报表&amp;多维分析引擎/Ad Hoc查询引擎/BI)<br>b.OLAP查询以MySQL作为前端。<br>5.IBM在Hadoop/大数据方面的架构与实践<br>6.Startup企业MemSQL提供实时查询方案<br>7.Yahoo Hadoop应用、运维，还有其基于Hadoop的Data workflow<br>学校<br>有不少做研究的老师过来传道，其中有不少精华的东西，特别是哈尔滨工业大学的李建中和俄亥俄州立大学的张晓东教授给我留下了深刻的印象。<br>缺失<br>1.实时查询/流计算内容很少；也许会是明年的热点吧；<br>2.Cloudera/MapR都厂商没有来，无法面对面了解其一些技术特点和产品特性；<br>个人总结<br>1.技术上，今年Hadoop生态圈的热点是HBase、HDFS NameNode、HDFS Security;<br>2.应用与平台上，阿里/淘宝发展最快，其次是腾讯/百度。这三个公司的Hadoop集群为公司内部众多部门提供hadoop平台服务，特别是阿里比较突出，其中共有的特性：<br>    a.工作的内容都是围绕为内部提供Hadoop集群/大数据服务平台；<br>    b.技术路线都是以Hadoop、HBase、Hive/Pig、关系型数据库为主；<br>    c.用户查询语言以兼容SQL语言为主；<br>    d.计算查询主要是以批量计算、实时查询为主；<br>    e.可视化方面的工作基本雷同：分析可视化、数据可视化、数据流程编排可视化；<br>    f.平台在安全、隔离、调度、元数据管理、监控、预警告警、服务化、数据集成与共享等方面提供功能。<br>3.实时查询/流计算，虽然今年没有覆盖，但是各个厂商都有提到自己已经在这两个方面着手或者取得了一些进展。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>今年Hadoop大会，加上了BigData，全称“Hadoop&amp;Bigdata Technology Conference(Hadoop与大数据技术大会)”，随着“云计算”“Hadoop”“大数据”的热点，今年的大会事先很是期待，之后从中收获很多，坚信了我们在工作中]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="http://navigating.github.io/tags/Hive/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习之--Stack Exchange网站架构]]></title>
    <link href="http://navigating.github.io/2011/%E5%AD%A6%E4%B9%A0%E4%B9%8B-Stack-Exchange%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84/"/>
    <id>http://navigating.github.io/2011/学习之-Stack-Exchange网站架构/</id>
    <published>2011-03-20T15:59:05.000Z</published>
    <updated>2015-08-09T16:04:30.767Z</updated>
    <content type="html"><![CDATA[<p>Stack Exchange系统管理员blog 上发布了一篇架构的文章。</p>
<p>网络流量 </p>
<ul>
<li>每月95M的PV</li>
<li>每秒800个HTTP请求</li>
<li>每秒180个DNS请求</li>
<li>每秒55Mb的网络带宽</li>
</ul>
<p>数据中心 </p>
<ul>
<li>1个机柜位于俄勒冈的Peak Internet(用于chat和Data Explorer)</li>
<li>2个机柜位于纽约的Peer 1 (用于其他的Stack Exchange Network)</li>
</ul>
<p>生产服务器 </p>
<ul>
<li>12 Web Servers (Windows Server 2008 R2)</li>
<li>2 Database Servers (Windows Server 2008 R2 and SQL Server 2008 R2)</li>
<li>2 Load Balancers (Ubuntu Server and HAProxy)</li>
<li>2 Caching Servers (Redis on CentOS)</li>
<li>1 Router / Firewall (Ubuntu Server)</li>
<li>3 DNS Servers (Bind on CentOS)</li>
</ul>
<p>所有的软件和技术 </p>
<ul>
<li>开发语言: C# / .NET</li>
<li>持续构建服务器: CruiseControl.NET</li>
<li>源码管理: Mercurial / Kiln</li>
<li>操作系统: Windows Server 2008 R2，Ubuntu Server，CentOS</li>
<li>数据库服务器：SQL Server 2008 R2</li>
<li>DNS服务器：Bind</li>
<li>负载均衡软件：HAProxy</li>
<li>缓存服务器：Redis</li>
<li>日志: Splunk</li>
<li>搜索索引技术: Lucene.NET</li>
<li>备份: Bacula</li>
<li>系统监控: Nagios (with n2rrd and drraw plugins)</li>
<li>监控SQL Server: SQL Monitor from Red Gate</li>
</ul>
<p>程序员和系统管理员 </p>
<ul>
<li>14 位程序员</li>
<li>2   位系统管理员</li>
</ul>
<p>(以上不含故障备份和管理服务器)<br>———————————————————–<br>分析：<br>1.使用了三种操作系统：Windows Server, CentOS, Ubuntu Server；共有22个服务器，三个机柜；<br>2.可能的架构：DNS Servers–Router/Firewall–Load Balancers–Web Servers–Caching Servers–Database Servers;<br>3.12个Web服务器每秒钟处理800个http请求，感觉投资回报比有点儿低；</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Stack Exchange系统管理员blog 上发布了一篇架构的文章。</p>
<p>网络流量 </p>
<ul>
<li>每月95M的PV</li>
<li>每秒800个HTTP请求</li>
<li>每秒180个DNS请求</li>
<li>每秒55Mb的网络带宽</]]>
    </summary>
    
      <category term="Architecture" scheme="http://navigating.github.io/tags/Architecture/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[川西旅游]]></title>
    <link href="http://navigating.github.io/2010/%E5%B7%9D%E8%A5%BF%E6%97%85%E6%B8%B8/"/>
    <id>http://navigating.github.io/2010/川西旅游/</id>
    <published>2010-11-06T15:58:48.000Z</published>
    <updated>2015-08-09T16:03:47.588Z</updated>
    <content type="html"><![CDATA[<p>川西之成都-四姑娘山-稻城亚丁-康定-海螺沟旅游-成都：<br>D1:成都-雅安-宝兴-夹金山-小金-日隆；午饭夹金山饭店，夜晚住日隆；<br>D2:看四姑娘山，双桥沟；下午日隆-小金-丹巴；午饭在双桥沟门口，住丹巴；<br>D3:丹巴-塔公-八美-新都桥；午饭在八美，住新都桥；<br>D4:新都桥-雅江-理塘-稻城；午饭在理塘，住稻城；<br>D5:稻城-亚丁-珍珠海：中午饭在景区，住亚丁村；<br>D6:亚丁-洛绒牛场-亚丁-稻城，午饭在亚丁，住稻城；<br>D7:稻城-理塘-雅江-新都桥-折多山-康定，午饭在雅江，住康定；<br>D8:康定-木格措-康定-泸定-摩西镇，午饭在泸定，住摩西镇；<br>D9:摩西镇-二郎山-雅安-成都，午饭在摩西镇，住成都；<br>D10:成都-宽窄巷-春熙路-双流机场;<br>喜欢在那儿待着的感觉，里面到底有什么呢？有什么魅力？为什么吸引人呢？也许它就在那儿吸引着你！</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>川西之成都-四姑娘山-稻城亚丁-康定-海螺沟旅游-成都：<br>D1:成都-雅安-宝兴-夹金山-小金-日隆；午饭夹金山饭店，夜晚住日隆；<br>D2:看四姑娘山，双桥沟；下午日隆-小金-丹巴；午饭在双桥沟门口，住丹巴；<br>D3:丹巴-塔公-八美-新都桥；午饭在八美，住新]]>
    </summary>
    
      <category term="生活" scheme="http://navigating.github.io/categories/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[JDK 7 in JavaOne 2010]]></title>
    <link href="http://navigating.github.io/2010/JDK-7-in-JavaOne-2010/"/>
    <id>http://navigating.github.io/2010/JDK-7-in-JavaOne-2010/</id>
    <published>2010-09-28T15:58:30.000Z</published>
    <updated>2015-08-09T16:03:23.558Z</updated>
    <content type="html"><![CDATA[<p>执行Plan B的JDK 7，愿景：“Making things programmers do everyday easier.”；其Coin项目公布的新特性：</p>
<p>Improved numeric literals<br>Strings in switch<br>Reduced varargs warnings<br>Diamond operator<br>Multi-catch with more precise rethrow<br>try-with-resources statement</p>
<p>细节请参考Joseph D.Darcy的博客：<a href="http://blogs.sun.com/darcy/entry/project_coin_javaone_2010" target="_blank" rel="external">http://blogs.sun.com/darcy/entry/project_coin_javaone_2010</a></p>
<p>文档下载参考：<a href="http://blogs.sun.com/darcy/resource/JavaOne/J1_2010-ProjectCoin.pdf" target="_blank" rel="external">http://blogs.sun.com/darcy/resource/JavaOne/J1_2010-ProjectCoin.pdf</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>执行Plan B的JDK 7，愿景：“Making things programmers do everyday easier.”；其Coin项目公布的新特性：</p>
<p>Improved numeric literals<br>Strings in switch<br]]>
    </summary>
    
      <category term="Java" scheme="http://navigating.github.io/tags/Java/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[摘要:40 Android Business Models]]></title>
    <link href="http://navigating.github.io/2010/%E6%91%98%E8%A6%81-40-Android-Business-Models/"/>
    <id>http://navigating.github.io/2010/摘要-40-Android-Business-Models/</id>
    <published>2010-09-11T15:58:10.000Z</published>
    <updated>2015-08-09T16:02:37.909Z</updated>
    <content type="html"><![CDATA[<p>Alex Curelea在Blog中从Sell app to individuals/Freemium/other sorts of products/srevices不同角度归纳了Android的四十种Business Model,分别是：<br>Model #1: Build the App, Sell the App to Individuals<br>Model #2: Make the App, Sell the App to Enterprises<br>Model #3: Make the App, Sell the App to Schools or Universities<br>Model #4: Make the App, Sell the App to OEMs<br>Model #5: Make the App, Sell the App to Content Providers<br>Model #6: Make the App, Sell the App to Service Providers</p>
<p>Model #7: Give Away the App, Sell the Customizations<br>Model #8: Give Away the App, Sell the Analytics<br>Model #9: Give Away the App, Sell the Remix<br>Model #10: Give Away the App, Sell the Bundle<br>Model #11: Give Away the App, Sell the Plugins<br>Model #12: Give Away the App, Sell the Feature Unlock Codes<br>Model #13: Give Away the App, Sell In-App Ads<br>Model #14: Give Away the App, Sell the Account<br>Model #15: Give Away the App, Sell Support<br>Model #16: Give Away the App, Sell the API<br>Model #17: Give Away the App, Sell the Content<br>Model #18: Give Away the App, Sell Premium Placement<br>Model #19: Give Away the App, Sell the Firm</p>
<p>Model #20: Sell Development Tools<br>Model #21: Sell Portability Toolkits<br>Model #23: Create a Content Marketplace<br>Model #24: Create an Android Job Board<br>Model #25: Create an Android Site Ad Broker<br>Model #26: Create Hardware Add-Ons<br>Model #27: Create Device Accessories<br>Model #28: Write Books</p>
<p>Model #31: Internationalization and Localization<br>Model #32: Bug Hunter<br>Model #33: User Experience Designer<br>Model #34: JAR Porter<br>Model #35: API Wrapper<br>Model #36: App Promoter<br>Model #37: App Market Agent<br>Model #38: Device Marketer<br>Model #39: Trainer<br>Model #40: Break Outside the Box</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Alex Curelea在Blog中从Sell app to individuals/Freemium/other sorts of products/srevices不同角度归纳了Android的四十种Business Model,分别是：<br>Model #1: Bu]]>
    </summary>
    
      <category term="Android" scheme="http://navigating.github.io/tags/Android/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java服务器开发基础知识(1)]]></title>
    <link href="http://navigating.github.io/2010/Java%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BC%80%E5%8F%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-1/"/>
    <id>http://navigating.github.io/2010/Java服务器开发基础知识-1/</id>
    <published>2010-08-25T15:59:53.000Z</published>
    <updated>2015-08-09T16:01:42.740Z</updated>
    <content type="html"><![CDATA[<p>因为实际需要所致，我们不得不考虑在现有的开源/商用的应用服务器之外开发一个，有性能要求、有并发要求的服务端应用，从技术要求的角度来分析一下，用Java实现这件事情我们可能关注的知识层面。</p>
<p>在整体上，可能需要我们从下面几个层面出发来考虑：</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2010/Java_Server_01.jpg" alt=""> </p>
<p>1.在硬件和操作系统层面：为什么需要关注这两个方面的知识，因为Java并没有自己的线程，使用的也是OS中的IO，所以我们不得不去了解系统在不同的硬件、OS上面的适用情况、运行情况。比方说，多核技术对于操作系统的影响，这种影响直接会传递到JVM层面；对于数据传输操作的DMA(Direct Memory Access, 存储器直接访问)方式的了解，便于我们更好的了解CPU和IO的关系。同样的，对于OS层面的select/poll/epoll的认识，不仅帮助我们去理解Java的IONIO(New IO)AIO(Asynchronous IO)的发展之路，同时让我们对IO/NIO/AIO在Java层面的体现有更清晰的思路。</p>
<p>Select</p>
<p>Select在1983年首次出现，是在4.2BSD中。通过select()的系统调用来监视包含多个文件描述符的数组；当select()返回后，该数组中就绪的文件描述符便会被内核修改标志位，使得进程可以获得这些文件描述符从而进行后续的读写操作。</p>
<p>Select方式现在为几乎所有的平台支持，具备良好的跨平台支持。同时，select()方式需要维护一个存储文件描述符的数据结构，随着其的增加，对其的内存操作的开销也会线性增加；每次网络响应，进程在调用select()的时候都会对所有的socket做一次线性扫描，包括那些处于非活跃状态的连接，这也是一个巨大的开销；在系统方面，每个进程能够监视的文件描述符的数量是有最大限制的，通过宏FD_SETSIZE来指定的，在Linux中这个值默认是1024，修改这个宏值并生效，有时候可能需要重新编译OS的内核:).</p>
<p>Poll</p>
<p>Poll是在1986先首次出现，是在System V Release 3中。其和select相比在本质上变化不大，只是poll没有了select方式的最大文件描述符数量的限制。</p>
<p>注：select()和poll()采用的模型都是：水平触发（Level Triggered）。</p>
<p>Epoll</p>
<p>Epoll是在Linux 2.6版本中开始有内核直接支持的实现方式。这种方式的三个特点：</p>
<p>a. 同时支持水平触发LT和边缘触发ET。</p>
<p>LT(Level Triggered)是缺省的工作方式，同时支持block和no-block socket。在这种模式中，内核将就绪的文件描述符告诉进程后，进程可以对这个就绪的fd进行IO操作。如果进程不作任何操作，那么内核会在下次调用中继续将这些文件描述符报告给进程，所以，这种模式在编程中出错误可能性要小一点。传统的select/poll都是这种模型的代表．</p>
<p>ET (edge-triggered)是高速工作方式，只支持no-block socket。在这种模式中，当文件描述符从未就绪状态变为就绪状态时，内核通过epoll报告给进程。然后内核会假设进程知道文件描述符已经就绪，并且不会再为这个(种)文件描述符发送更多的就绪通知，直到进程做了某些操作导致这个(种)文件描述符不再为就绪状态了(比如，进程在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致 了一个EWOULDBLOCK 错误）。同时请注意，如果一直不对这个(种)fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)。不过在TCP协议中，ET模式的加速效用仍需要更多的benchmark确认。</p>
<p>b. epoll模式在操作中获得就绪的文件描述符时，返回的不是实际的文件描述符，而是一个代表就绪文件描述符的值，然后在文件描述符数组中查询对应的的文件描述符。这样避免系统调用时候在内存中数据结构复制的开销。</p>
<p>c. epoll采用事件通知方式。即每一个文件描述符都对应的注册了一个类似callback，当就绪激活时，通过callback和进程进行匹配操作。避免了select在每次调用的时候都遍历整个socket的扫描开销。</p>
<p>因此epoll具备优势：支持一个进程打开大数目的socket描述符(FD)；IO效率不随FD数目增加而线性下降；用mmap加速内核与用户空间的消息传递；和内核微调一起协调工作。</p>
<p>注：似乎Apache主要采用的是select模式；而lighttpd和nginx主要采用epoll模式—带考证确认。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>因为实际需要所致，我们不得不考虑在现有的开源/商用的应用服务器之外开发一个，有性能要求、有并发要求的服务端应用，从技术要求的角度来分析一下，用Java实现这件事情我们可能关注的知识层面。</p>
<p>在整体上，可能需要我们从下面几个层面出发来考虑：</p>
<p><img]]>
    </summary>
    
      <category term="Architecture" scheme="http://navigating.github.io/tags/Architecture/"/>
    
      <category term="Java" scheme="http://navigating.github.io/tags/Java/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
</feed>