<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[On The Open Way]]></title>
  <subtitle><![CDATA[自信人生二百年，会当水击三千里！]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://navigating.github.io//"/>
  <updated>2016-05-20T09:25:35.792Z</updated>
  <id>http://navigating.github.io//</id>
  
  <author>
    <name><![CDATA[Steven Xu]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[敏捷之如何切分用户故事]]></title>
    <link href="http://navigating.github.io/2016/%E6%95%8F%E6%8D%B7%E4%B9%8B%E5%A6%82%E4%BD%95%E5%88%87%E5%88%86%E7%94%A8%E6%88%B7%E6%95%85%E4%BA%8B/"/>
    <id>http://navigating.github.io/2016/敏捷之如何切分用户故事/</id>
    <published>2016-05-20T02:41:31.000Z</published>
    <updated>2016-05-20T09:25:35.792Z</updated>
    <content type="html"><![CDATA[<p>敏捷Scrum框架一览</p>
<p><img src="http://agileforall.com/wp-content/uploads/2015/12/Scrum_Framework_lg-e1456358707313.jpg" alt="这是一张图片"></p>
<p>用户故事切分</p>
<p><img src="http://agileall.wpengine.com/wp-content/uploads/2016/01/how-to-split-a-user-story.jpg" alt="这是一张图片"></p>
<p>用户故事切分三步法：<br>第一步 准备待切分的故事<br>通过提出问题准备故事：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">原则是待切分的大故事是否满足INVEST<span class="keyword">*</span>原则（“较小的”这一点可以除外）？</span><br><span class="line">故事大小是团队速率的 1/10 到 1/6 吗？</span><br></pre></td></tr></table></figure></p>
<p>第二步 运用切分模式<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">按工作流程步骤切分</span><br><span class="line">延迟性能优化</span><br><span class="line">按简单/复杂切分</span><br><span class="line">按主要工作切分</span><br><span class="line">按不同界面切分</span><br><span class="line">按不同类型的数据切分</span><br><span class="line">按不同的业务规则切分</span><br><span class="line">按操作切分</span><br></pre></td></tr></table></figure></p>
<p>第三步 评估切分<br>通过对切分提出以下问题进行评估：<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">新故事的大小大致相等吗？</span><br><span class="line">每个故事大概是团队速率的<span class="number">1/10到1/6</span>吗？</span><br><span class="line">每个故事都满足INVEST原则吗？</span><br><span class="line">有可以降低优先级或删除掉的故事吗？</span><br><span class="line">有没有明显的故事先开始，从而可以获得早期的价值、认知或风险降低等？</span><br></pre></td></tr></table></figure></p>
<p>注：INVEST - 故事应该是：<br>独立的<br>可商谈的<br>有价值的<br>可估算的<br>较小的<br>可测试的</p>
<p>参考：</p>
<ol>
<li><a href="http://agileforall.com/resources/introduction-to-agile/" target="_blank" rel="external">Introduction to Agile</a></li>
<li><a href="http://agileforall.com/new-story-splitting-resource/" target="_blank" rel="external">New Story Splitting Resource</a></li>
<li><a href="http://agileforall.com/resources/how-to-split-a-user-story/" target="_blank" rel="external">How to Split a User Story</a></li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>敏捷Scrum框架一览</p>
<p><img src="http://agileforall.com/wp-content/uploads/2015/12/Scrum_Framework_lg-e1456358707313.jpg" alt="这是一张图片"></p>
<]]>
    </summary>
    
      <category term="Agile" scheme="http://navigating.github.io/tags/Agile/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《Thoughtworks技术雷达201604》]]></title>
    <link href="http://navigating.github.io/2016/%E5%AD%A6%E4%B9%A0%E3%80%8AThoughtworks%E6%8A%80%E6%9C%AF%E9%9B%B7%E8%BE%BE201604%E3%80%8B/"/>
    <id>http://navigating.github.io/2016/学习《Thoughtworks技术雷达201604》/</id>
    <published>2016-05-16T02:41:31.000Z</published>
    <updated>2016-05-20T07:22:42.208Z</updated>
    <content type="html"><![CDATA[<p>笔记如下：</p>
<ul>
<li><p>技术雷达从两个维度对技术进行评估，一个维度是技术归类的四个象限，包括：技术、平台、工具、语言及框架，另一个维度是反映所持有的态度的四个环，依次为：采用、试验、评估、暂缓。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">采用：我们强烈主张业界采用这些技术。如果适用我们的项目，我们会采用他们。</span><br><span class="line">试验：值得追求。重要的是理解如何建立这种能力。企业应该在风险可控的项目中尝试该项技术。</span><br><span class="line">评估：为了确认它将如何影响你所在的企业，值得做一番探究。</span><br><span class="line">暂缓：谨慎推行。</span><br></pre></td></tr></table></figure>
</li>
<li><p>在技术理念上需要关注：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Products <span class="keyword">over</span> projects</span><br><span class="line">BFF - Backend <span class="keyword">for</span> frontends</span><br><span class="line">Data Lake</span><br><span class="line">Event Storming</span><br><span class="line">QA <span class="keyword">in</span> production</span><br><span class="line">Reactive architectures</span><br></pre></td></tr></table></figure>
</li>
<li><p>平台部分关注：</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">Docker</span></span><br><span class="line">Apache Mesos</span><br><span class="line">Kubernets</span><br></pre></td></tr></table></figure>
</li>
<li><p>工具部分关注：</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">Consul</span></span><br><span class="line">Apache Kafka</span><br><span class="line">Zipkin</span><br></pre></td></tr></table></figure>
</li>
<li><p>语言和框架部分关注：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ES6</span><br><span class="line">React<span class="class">.js</span></span><br><span class="line">Spring Boot</span><br><span class="line">Swift</span><br><span class="line">Dagger</span><br><span class="line">Dapper</span><br><span class="line">Ember<span class="class">.js</span></span><br><span class="line">Reactive Native</span><br></pre></td></tr></table></figure>
</li>
<li><p>暂缓部分关注：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1. </span>Application Servers</span><br><span class="line"><span class="bullet">2. </span>Jenkins as a deployment pipeline</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>&lt;完&gt;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>笔记如下：</p>
<ul>
<li><p>技术雷达从两个维度对技术进行评估，一个维度是技术归类的四个象限，包括：技术、平台、工具、语言及框架，另一个维度是反映所持有的态度的四个环，依次为：采用、试验、评估、暂缓。</p>
<figure class="highlight"]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[简报：人工智能 - 2016Q2]]></title>
    <link href="http://navigating.github.io/2016/%E7%AE%80%E6%8A%A5%EF%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20-%202016Q2/"/>
    <id>http://navigating.github.io/2016/简报：人工智能 - 2016Q2/</id>
    <published>2016-05-16T02:41:31.000Z</published>
    <updated>2016-05-18T07:24:38.862Z</updated>
    <content type="html"><![CDATA[<p>从谷歌(Google)旗下DeepMind公司阿尔法围棋（AlphaGo）挑战围棋冠军李世石之后，人工智能进一步发酵，变成热门方向和热门关键词，各公司分别发布新闻，和开源相关产品。其中：</p>
<ul>
<li>Microsoft: DMTK</li>
<li>Google: TensorFlow、SyntaxNet</li>
<li>Apple:</li>
<li>Facebook: Torch</li>
<li>Amazon:</li>
<li>OpenAI: OpenAI Gym</li>
<li>Yahoo!：</li>
<li>Baidu: </li>
<li>IBM: SystemML</li>
</ul>
<p>深度学习开源实现：</p>
<ul>
<li>Torch</li>
<li>Theano</li>
<li>Caffe</li>
<li>Petuum</li>
<li>Deep4J</li>
<li>H2O</li>
</ul>
<p>参考：</p>
<ol>
<li><a href="http://mt.sohu.com/20160513/n449235974.shtml" target="_blank" rel="external">http://mt.sohu.com/20160513/n449235974.shtml</a></li>
<li><a href="http://www.infoq.com/cn/articles/h2o-ai-chief-architect-talk-deep-learning" target="_blank" rel="external">http://www.infoq.com/cn/articles/h2o-ai-chief-architect-talk-deep-learning</a></li>
<li><a href="http://cmsoft.10086.cn/info.html?key=348" target="_blank" rel="external">http://cmsoft.10086.cn/info.html?key=348</a></li>
</ol>
<p>&lt;完&gt;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>从谷歌(Google)旗下DeepMind公司阿尔法围棋（AlphaGo）挑战围棋冠军李世石之后，人工智能进一步发酵，变成热门方向和热门关键词，各公司分别发布新闻，和开源相关产品。其中：</p>
<ul>
<li>Microsoft: DMTK</li>
<li>Googl]]>
    </summary>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HDFS Snapshot]]></title>
    <link href="http://navigating.github.io/2016/HDFS%20Snapshot/"/>
    <id>http://navigating.github.io/2016/HDFS Snapshot/</id>
    <published>2016-04-21T07:53:46.000Z</published>
    <updated>2016-04-21T03:11:03.347Z</updated>
    <content type="html"><![CDATA[<p>参考：<br>1.<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html</a><br>2.<a href="http://debugo.com/hdfs-snapshot/" target="_blank" rel="external">http://debugo.com/hdfs-snapshot/</a><br>3.<a href="http://blog.csdn.net/linlinv3/article/details/44622203" target="_blank" rel="external">http://blog.csdn.net/linlinv3/article/details/44622203</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>参考：<br>1.<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html" target="_blank" rel="external]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《Real-Time Event Streaming What Are Your Options》]]></title>
    <link href="http://navigating.github.io/2016/%E5%AD%A6%E4%B9%A0%E3%80%8AReal-Time%20Event%20Streaming%20What%20Are%20Your%20Options%E3%80%8B/"/>
    <id>http://navigating.github.io/2016/学习《Real-Time Event Streaming What Are Your Options》/</id>
    <published>2016-04-18T04:56:08.000Z</published>
    <updated>2016-04-21T03:52:50.129Z</updated>
    <content type="html"><![CDATA[<h2 id="一个典型的流式架构(a_typical_streaming_architecture)">一个典型的流式架构(a typical streaming architecture)</h2><p><img src="https://www.mapr.com/ebooks/streaming-architecture/images/ndsa_0202.png" alt="这是一张图片"></p>
<h2 id="流式架构三组件">流式架构三组件</h2><ul>
<li>A producer: 与数据源相连的软件系统。生产者从数据源采集、转换、过滤、聚合、增强之后发布事件数据到流式系统中。</li>
<li>The streaming system: 接受生产者发布的数据，持久化这些数据，然后可靠的将数据分发给消费者。</li>
<li>Consumers: 从流中订阅数据，并操作，或者分析这些数据。</li>
</ul>
<h2 id="技术选型(Options)">技术选型(Options)</h2><p>Producers:</p>
<ul>
<li>Apache Flume</li>
<li>StreamSets Data Collector</li>
</ul>
<p>Streaming System</p>
<ul>
<li>Apache Kafka</li>
<li>MapR Streams</li>
</ul>
<p>Comsumers(Processing)</p>
<ul>
<li>Spark Streaming</li>
<li>Apache Storm</li>
<li>Apache Flink</li>
<li>Apache Apex</li>
</ul>
<h2 id="参考：">参考：</h2><p>1.<a href="https://www.mapr.com/blog/real-time-event-streaming-what-are-your-options" target="_blank" rel="external">Real-Time Event Streaming What Are Your Options?</a><br>2.<a href="https://www.mapr.com/ebooks/streaming-architecture/" target="_blank" rel="external">《Streaming Architecture》</a><br>3.<a href="https://www.mapr.com/ebooks/streaming-architecture/chapter-02-stream-based-architecture.html#ch02" target="_blank" rel="external">Stream-based Architecture</a><br>4.<a href="https://www.mapr.com/ebooks/streaming-architecture/chapter-03-streaming-platform-for-microservices.html#ch03" target="_blank" rel="external">Streaming Architecture: Ideal Platform for Microservices</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一个典型的流式架构(a_typical_streaming_architecture)">一个典型的流式架构(a typical streaming architecture)</h2><p><img src="https://www.mapr.com/ebook]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Flume" scheme="http://navigating.github.io/tags/Flume/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Kafka" scheme="http://navigating.github.io/tags/Kafka/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之2016Q2]]></title>
    <link href="http://navigating.github.io/2016/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B2016Q2/"/>
    <id>http://navigating.github.io/2016/大数据动态之2016Q2/</id>
    <published>2016-04-18T02:41:31.000Z</published>
    <updated>2016-05-16T05:39:50.973Z</updated>
    <content type="html"><![CDATA[<h2 id="Apache_Storm">Apache Storm</h2><p>Storm发布1.0.0版本，关键特性：</p>
<ul>
<li>Developer Productivity<br><strong> New Storm Connectors
</strong> Storm-Kafka Spout using new client APIs<br><strong> Distributed Log Search
</strong> Dynamic Worker Profiling</li>
<li>Enterprise Readiness<br><strong> Improved nimbus HA
</strong> Automatic Back pressure<br><strong> Distributed cache
</strong> Windowing and State Management<br>** Storm Performance improvements</li>
<li>Operational Simplicity<br><strong> Storm Topology Event inspector
</strong> Resource Aware Scheduling<br><strong> Dynamic Log Levels
</strong> Pacemaker Storm Daemon<br><a href="http://hortonworks.com/blog/announcing-apache-storm-1-0-0/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-apache-storm-1-0-0/</a><br><a href="https://storm.apache.org/2016/04/12/storm100-released.html" target="_blank" rel="external">https://storm.apache.org/2016/04/12/storm100-released.html</a></li>
</ul>
<h2 id="Cloudera">Cloudera</h2><p>How-to: Build a Prediction Engine using Spark, Kudu, and Impala<br><a href="http://blog.cloudera.com/blog/2016/05/how-to-build-a-prediction-engine-using-spark-kudu-and-impala/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/05/how-to-build-a-prediction-engine-using-spark-kudu-and-impala/</a><br>How-to: Improve Apache HBase Performance via Data Serialization with Apache Avro<br><a href="http://blog.cloudera.com/blog/2016/05/how-to-improve-apache-hbase-performance-via-data-serialization-with-apache-avro/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/05/how-to-improve-apache-hbase-performance-via-data-serialization-with-apache-avro/</a><br>Inside Santander’s Near Real-Time Data Ingest Architecture (Part 2)<br><a href="http://blog.cloudera.com/blog/2016/05/inside-santanders-near-real-time-data-ingest-architecture-part-2/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/05/inside-santanders-near-real-time-data-ingest-architecture-part-2/</a><br>Inside Santander’s Near Real-Time Data Ingest Architecture<br><a href="http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/</a><br>Apache Impala (incubating) in CDH 5.7: 4x Faster for BI Workloads on Apache Hadoop<br><a href="http://blog.cloudera.com/blog/2016/04/apache-impala-incubating-in-cdh-5-7-4x-faster-for-bi-workloads-on-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/04/apache-impala-incubating-in-cdh-5-7-4x-faster-for-bi-workloads-on-apache-hadoop/</a><br>New in Cloudera Manager 5.7: Cluster Utilization Reporting<br><a href="http://blog.cloudera.com/blog/2016/04/new-in-cloudera-manager-5-7-cluster-utilization-reporting/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/04/new-in-cloudera-manager-5-7-cluster-utilization-reporting/</a><br>Cloudera Enterprise 5.7 is Released<br><a href="http://blog.cloudera.com/blog/2016/04/cloudera-enterprise-5-7-is-released/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/04/cloudera-enterprise-5-7-is-released/</a><br>How-to: Use Impala and Kudu Together for Analytic Workloads<br><a href="http://blog.cloudera.com/blog/2016/04/how-to-use-impala-and-kudu-together-for-analytic-workloads/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/04/how-to-use-impala-and-kudu-together-for-analytic-workloads/</a><br>Quality Assurance at Cloudera: Running/Upgrading to New Releases on Our Own EDH Cluster<br><a href="http://blog.cloudera.com/blog/2016/04/quality-assurance-at-cloudera-runningupgrading-to-new-releases-on-our-own-edh-cluster/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/04/quality-assurance-at-cloudera-runningupgrading-to-new-releases-on-our-own-edh-cluster/</a><br>Quality Assurance at Cloudera: Fault Injection and Elastic Partitioning<br><a href="http://blog.cloudera.com/blog/2016/04/quality-assurance-at-cloudera-fault-injection-and-elastic-partitioning/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/04/quality-assurance-at-cloudera-fault-injection-and-elastic-partitioning/</a><br>Benchmarking Apache Parquet: The Allstate Experience<br><a href="http://blog.cloudera.com/blog/2016/04/benchmarking-apache-parquet-the-allstate-experience/" target="_blank" rel="external">http://blog.cloudera.com/blog/2016/04/benchmarking-apache-parquet-the-allstate-experience/</a></p>
<h2 id="Hortonworks">Hortonworks</h2><p>ANNOUNCING APACHE STORM 1.0.0<br><a href="http://hortonworks.com/blog/announcing-apache-storm-1-0-0/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-apache-storm-1-0-0/</a><br>THE NEXT GENERATION OF HADOOP-BASED SECURITY &amp; DATA GOVERNANCE<br><a href="http://hortonworks.com/blog/the-next-generation-of-hadoop-based-security-data-governance/" target="_blank" rel="external">http://hortonworks.com/blog/the-next-generation-of-hadoop-based-security-data-governance/</a><br>ADVANCED METRICS VISUALIZATION DASHBOARDING WITH APACHE AMBARI<br><a href="http://hortonworks.com/blog/advanced-metrics-visualization-dashboarding-apache-ambari/" target="_blank" rel="external">http://hortonworks.com/blog/advanced-metrics-visualization-dashboarding-apache-ambari/</a><br>STREAMLINING APACHE HADOOP OPERATIONS<br><a href="http://hortonworks.com/blog/streamlining-apache-hadoop-operations/" target="_blank" rel="external">http://hortonworks.com/blog/streamlining-apache-hadoop-operations/</a><br>THE NEXT MARKET LEADERS WILL POWER THEIR BUSINESSES FROM IOAT DATA SOURCES<br><a href="http://hortonworks.com/blog/next-market-leaders-will-power-businesses-ioat-data-sources/" target="_blank" rel="external">http://hortonworks.com/blog/next-market-leaders-will-power-businesses-ioat-data-sources/</a></p>
<h2 id="Databricks">Databricks</h2><p>Preview of Apache Spark 2.0 now on Databricks Community Edition<br><a href="https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html" target="_blank" rel="external">https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html</a><br>Spark Trending in the Stack Overflow Survey<br><a href="https://databricks.com/blog/2016/03/22/spark-trending-in-the-stack-overflow-survey.html" target="_blank" rel="external">https://databricks.com/blog/2016/03/22/spark-trending-in-the-stack-overflow-survey.html</a><br><a href="http://stackoverflow.com/research/developer-survey-2016" target="_blank" rel="external">http://stackoverflow.com/research/developer-survey-2016</a><br>Continuous Integration and Delivery of Spark Applications at Metacog<br><a href="https://databricks.com/blog/2016/04/06/continuous-integration-and-delivery-of-spark-applications-at-metacog.html" target="_blank" rel="external">https://databricks.com/blog/2016/04/06/continuous-integration-and-delivery-of-spark-applications-at-metacog.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Apache_Storm">Apache Storm</h2><p>Storm发布1.0.0版本，关键特性：</p>
<ul>
<li>Developer Productivity<br><strong> New Storm Connectors
</strong]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之2016Q1]]></title>
    <link href="http://navigating.github.io/2016/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B2016Q1/"/>
    <id>http://navigating.github.io/2016/大数据动态之2016Q1/</id>
    <published>2016-04-01T02:41:31.000Z</published>
    <updated>2016-04-18T13:45:59.660Z</updated>
    <content type="html"><![CDATA[<h2 id="Cloudera">Cloudera</h2><h2 id="Hortonworks">Hortonworks</h2><p>Top Ten Blogs from 2015<br><a href="http://hortonworks.com/blog/top-ten-blogs-from-2015/" target="_blank" rel="external">http://hortonworks.com/blog/top-ten-blogs-from-2015/</a><br>Best practices in HDFS authorization with Apache Ranger<br><a href="http://hortonworks.com/blog/best-practices-in-hdfs-authorization-with-apache-ranger/" target="_blank" rel="external">http://hortonworks.com/blog/best-practices-in-hdfs-authorization-with-apache-ranger/</a><br>Community Choice Winner Blog: Advanced Execution Visualization of Spark jobs<br><a href="http://hortonworks.com/blog/community-choice-winner-blog-advanced-execution-visualization-spark-jobs/" target="_blank" rel="external">http://hortonworks.com/blog/community-choice-winner-blog-advanced-execution-visualization-spark-jobs/</a><br>Delivering an Advanced Analytics Platform for Media Management with Arkena<br><a href="http://hortonworks.com/blog/delivering-an-advanced-analytics-platform-for-media-management-with-arkena/" target="_blank" rel="external">http://hortonworks.com/blog/delivering-an-advanced-analytics-platform-for-media-management-with-arkena/</a><br>Top 3 articles for every Hadoop Developer<br><a href="http://hortonworks.com/blog/it-has-a-been-a-busy-week-on-hcc-here-is-the-hot-content-for-this-week-based-on-community-activity-and-votes/" target="_blank" rel="external">http://hortonworks.com/blog/it-has-a-been-a-busy-week-on-hcc-here-is-the-hot-content-for-this-week-based-on-community-activity-and-votes/</a><br>Ambari Metrics - Switching from Embedded to Distributed Mode Fails<br><a href="https://community.hortonworks.com/questions/17421/ambari-metrics-switching-from-embedded-to-distribu.html" target="_blank" rel="external">https://community.hortonworks.com/questions/17421/ambari-metrics-switching-from-embedded-to-distribu.html</a><br>Top 3 articles for every Hadoop Developer<br><a href="http://hortonworks.com/blog/top-3-articles-that-ever-hadoop-developer-should-read-week-072016/" target="_blank" rel="external">http://hortonworks.com/blog/top-3-articles-that-ever-hadoop-developer-should-read-week-072016/</a><br>Windowing and State checkpointing in Apache Storm<br><a href="http://hortonworks.com/blog/storm-support-windowing-state/" target="_blank" rel="external">http://hortonworks.com/blog/storm-support-windowing-state/</a><br><a href="https://community.hortonworks.com/articles/14171/windowing-and-state-checkpointing-in-apache-storm.html" target="_blank" rel="external">https://community.hortonworks.com/articles/14171/windowing-and-state-checkpointing-in-apache-storm.html</a><br>Hadoop All Grown Up<br><a href="http://hortonworks.com/blog/hadoop-all-grown-up/" target="_blank" rel="external">http://hortonworks.com/blog/hadoop-all-grown-up/</a><br>Top 3 articles for every Hadoop Developer 2<br><a href="http://hortonworks.com/blog/top-3-articles-every-hadoop-developer-2/" target="_blank" rel="external">http://hortonworks.com/blog/top-3-articles-every-hadoop-developer-2/</a><br>Hive on Tez Performance Tuning – Determining Reducer Counts<br><a href="https://community.hortonworks.com/articles/22419/hive-on-tez-performance-tuning-determining-reducer.html" target="_blank" rel="external">https://community.hortonworks.com/articles/22419/hive-on-tez-performance-tuning-determining-reducer.html</a><br>What is the Hortonworks recommendation on Swap usage?<br><a href="https://community.hortonworks.com/questions/22548/what-is-the-hortonworks-recommendation-on-swap-usa.html" target="_blank" rel="external">https://community.hortonworks.com/questions/22548/what-is-the-hortonworks-recommendation-on-swap-usa.html</a><br>Top 3 articles for every Hadoop Developer<br><a href="http://hortonworks.com/blog/top-3-articles-every-hadoop-developer/" target="_blank" rel="external">http://hortonworks.com/blog/top-3-articles-every-hadoop-developer/</a><br>Install Apache Hawq on HDP 2.3.4<br><a href="https://community.hortonworks.com/articles/20420/install-apache-hawq-on-hdp-234.html" target="_blank" rel="external">https://community.hortonworks.com/articles/20420/install-apache-hawq-on-hdp-234.html</a><br>Top 3 articles for every Hadoop Developer<br><a href="http://hortonworks.com/blog/enter-title-here/" target="_blank" rel="external">http://hortonworks.com/blog/enter-title-here/</a><br>How to remove risk disks from Hadoop cluster ?<br><a href="https://community.hortonworks.com/questions/18593/how-to-remove-risk-disks-from-hadoop-cluster.html" target="_blank" rel="external">https://community.hortonworks.com/questions/18593/how-to-remove-risk-disks-from-hadoop-cluster.html</a><br><a href="https://community.hortonworks.com/articles/3131/replacing-disk-on-datanode-hosts.html" target="_blank" rel="external">https://community.hortonworks.com/articles/3131/replacing-disk-on-datanode-hosts.html</a><br>Capacity scheduler queue mapping while doAs disabled<br><a href="https://community.hortonworks.com/questions/18639/capacity-scheduler-queue-mapping-while-doas-disabl.html" target="_blank" rel="external">https://community.hortonworks.com/questions/18639/capacity-scheduler-queue-mapping-while-doas-disabl.html</a><br>Hortonworks DataFlow 1.2 Released<br><a href="http://hortonworks.com/blog/hortonworks-dataflow-1-2-released/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-dataflow-1-2-released/</a><br>Top 3 articles for every Hadoop Developer<br><a href="http://hortonworks.com/blog/top-3-articles-every-hadoop-developer-3/" target="_blank" rel="external">http://hortonworks.com/blog/top-3-articles-every-hadoop-developer-3/</a><br>Quickly enable SSL encryption for Hadoop components in HDP Sandbox<br><a href="https://community.hortonworks.com/articles/22756/quickly-enable-ssl-encryption-for-hadoop-component.html" target="_blank" rel="external">https://community.hortonworks.com/articles/22756/quickly-enable-ssl-encryption-for-hadoop-component.html</a><br>Hortonworks HDP and SAS Event Stream Processing together, using YARN<br><a href="http://hortonworks.com/blog/hortonworks-hdp-and-sas-event-stream-processing-together-using-yarn/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-hdp-and-sas-event-stream-processing-together-using-yarn/</a></p>
<h2 id="MapR">MapR</h2><p>Apache Spark as a Distributed SQL Engine<br><a href="https://www.mapr.com/blog/apache-spark-distributed-sql-engine" target="_blank" rel="external">https://www.mapr.com/blog/apache-spark-distributed-sql-engine</a><br>Apache Flink GA - Planning for the Future<br><a href="https://www.mapr.com/blog/apache-flink-ga-planning-future" target="_blank" rel="external">https://www.mapr.com/blog/apache-flink-ga-planning-future</a><br>How to Log in Apache Spark<br><a href="https://www.mapr.com/blog/how-log-apache-spark" target="_blank" rel="external">https://www.mapr.com/blog/how-log-apache-spark</a><br>Streaming in the Extreme<br><a href="https://www.mapr.com/blog/streaming-extreme" target="_blank" rel="external">https://www.mapr.com/blog/streaming-extreme</a><br>Secondary Indexing for MapR-DB using Elasticsearch<br><a href="https://www.mapr.com/blog/secondary-indexing-mapr-db-using-elasticsearch" target="_blank" rel="external">https://www.mapr.com/blog/secondary-indexing-mapr-db-using-elasticsearch</a><br>Top 10 Most Popular MapR Blog Posts of 2015<br><a href="https://www.mapr.com/blog/top-10-most-popular-mapr-blog-posts-2015" target="_blank" rel="external">https://www.mapr.com/blog/top-10-most-popular-mapr-blog-posts-2015</a><br>Architecture Matters for Production Success<br><a href="https://www.mapr.com/why-hadoop/why-mapr/architecture-matters" target="_blank" rel="external">https://www.mapr.com/why-hadoop/why-mapr/architecture-matters</a><br>Spark Data Source API: Extending Our Spark SQL Query Engine<br><a href="https://www.mapr.com/blog/spark-data-source-api-extending-our-spark-sql-query-engine" target="_blank" rel="external">https://www.mapr.com/blog/spark-data-source-api-extending-our-spark-sql-query-engine</a><br>What Will You Do in 2016? Apache Spark, Kafka, Drill and More<br><a href="https://www.mapr.com/blog/what-will-you-do-2016-apache-spark-kafka-drill-and-more" target="_blank" rel="external">https://www.mapr.com/blog/what-will-you-do-2016-apache-spark-kafka-drill-and-more</a><br>A Brief Overview of Performance Enhancements in Apache Drill 1.4<br><a href="https://www.mapr.com/blog/brief-overview-performance-enhancements-apache-drill-14" target="_blank" rel="external">https://www.mapr.com/blog/brief-overview-performance-enhancements-apache-drill-14</a></p>
<h1 id="Databricks">Databricks</h1><p>Announcing Spark 1.6<br><a href="https://databricks.com/blog/2016/01/04/announcing-spark-1-6.html" target="_blank" rel="external">https://databricks.com/blog/2016/01/04/announcing-spark-1-6.html</a><br>Introducing Spark Datasets<br><a href="https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html" target="_blank" rel="external">https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html</a><br>Spark 2015 Year In Review<br><a href="https://databricks.com/blog/2016/01/05/spark-2015-year-in-review.html" target="_blank" rel="external">https://databricks.com/blog/2016/01/05/spark-2015-year-in-review.html</a><br>Deep Learning with Spark and TensorFlow<br><a href="https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html" target="_blank" rel="external">https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html</a><br>Faster Stateful Stream Processing in Spark Streaming<br><a href="https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-spark-streaming.html" target="_blank" rel="external">https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-spark-streaming.html</a><br>An Illustrated Guide to Advertising Analytics<br><a href="https://databricks.com/blog/2016/02/02/an-illustrated-guide-to-advertising-analytics.html" target="_blank" rel="external">https://databricks.com/blog/2016/02/02/an-illustrated-guide-to-advertising-analytics.html</a><br>Introducing Databricks Community Edition: Apache Spark for All<br><a href="https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html" target="_blank" rel="external">https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html</a><br>Introducing Databricks Dashboards<br><a href="https://databricks.com/blog/2016/02/17/introducing-databricks-dashboards.html" target="_blank" rel="external">https://databricks.com/blog/2016/02/17/introducing-databricks-dashboards.html</a><br>Introducing GraphFrames<br><a href="https://databricks.com/blog/2016/03/03/introducing-graphframes.html" target="_blank" rel="external">https://databricks.com/blog/2016/03/03/introducing-graphframes.html</a><br>On-Time Flight Performance with GraphFrames for Apache Spark<br><a href="https://databricks.com/blog/2016/03/16/on-time-flight-performance-with-spark-graphframes.html" target="_blank" rel="external">https://databricks.com/blog/2016/03/16/on-time-flight-performance-with-spark-graphframes.html</a><br>Announcing New Databricks APIs for Faster Production Spark Application Deployment<br><a href="https://databricks.com/blog/2016/03/30/announcing-new-databricks-apis-for-faster-production-spark-application-deployment.html" target="_blank" rel="external">https://databricks.com/blog/2016/03/30/announcing-new-databricks-apis-for-faster-production-spark-application-deployment.html</a><br>Introducing our new eBook: Apache Spark Analytics Made Simple<br><a href="https://databricks.com/blog/2016/03/31/introducing-our-new-ebook-apache-spark-analytics-made-simple.html" target="_blank" rel="external">https://databricks.com/blog/2016/03/31/introducing-our-new-ebook-apache-spark-analytics-made-simple.html</a></p>
<p>参考</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Cloudera">Cloudera</h2><h2 id="Hortonworks">Hortonworks</h2><p>Top Ten Blogs from 2015<br><a href="http://hortonworks.com/blog/top-t]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之2015Q4]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B2015Q4/"/>
    <id>http://navigating.github.io/2015/大数据动态之2015Q4/</id>
    <published>2016-01-01T02:41:31.000Z</published>
    <updated>2016-04-18T13:45:46.872Z</updated>
    <content type="html"><![CDATA[<h2 id="Cloudera">Cloudera</h2><h2 id="Hortonworks">Hortonworks</h2><p>Hortonworks DataFlow 1.1 Released<br><a href="http://hortonworks.com/blog/hortonworks-dataflow-1-1-released/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-dataflow-1-1-released/</a></p>
<h2 id="MapR">MapR</h2><p>Top 10 Big Data Trends in 2016 for Financial Services<br><a href="https://www.mapr.com/blog/top-10-big-data-trends-2016-financial-services" target="_blank" rel="external">https://www.mapr.com/blog/top-10-big-data-trends-2016-financial-services</a><br>Apache Spark vs. Apache Drill<br><a href="https://www.mapr.com/blog/apache-spark-vs-apache-drill" target="_blank" rel="external">https://www.mapr.com/blog/apache-spark-vs-apache-drill</a><br>Turning Data Into Value with Hadoop and Spark – Infographic<br><a href="https://www.mapr.com/blog/turning-data-value-hadoop-and-spark-infographic" target="_blank" rel="external">https://www.mapr.com/blog/turning-data-value-hadoop-and-spark-infographic</a><br>Apache Apex: OSS Incubator Project for Batch and Stream Processing<br><a href="https://www.mapr.com/blog/apache-apex-oss-incubator-project-batch-and-stream-processing" target="_blank" rel="external">https://www.mapr.com/blog/apache-apex-oss-incubator-project-batch-and-stream-processing</a><br>Mesos and YARN: A tale of two clusters<br><a href="https://www.mapr.com/blog/mesos-and-yarn-tale-two-clusters" target="_blank" rel="external">https://www.mapr.com/blog/mesos-and-yarn-tale-two-clusters</a><br>Comparing SQL Functions and Performance with Apache Spark and Apache Drill<br><a href="https://www.mapr.com/blog/comparing-sql-functions-and-performance-apache-spark-and-apache-drill" target="_blank" rel="external">https://www.mapr.com/blog/comparing-sql-functions-and-performance-apache-spark-and-apache-drill</a><br>MapReduce Design Patterns Implemented in Apache Spark<br><a href="https://www.mapr.com/blog/mapreduce-design-patterns-implemented-apache-spark" target="_blank" rel="external">https://www.mapr.com/blog/mapreduce-design-patterns-implemented-apache-spark</a><br>Introduction to Apache Spark with Examples and Use Cases<br><a href="https://www.mapr.com/blog/introduction-apache-spark-examples-and-use-cases" target="_blank" rel="external">https://www.mapr.com/blog/introduction-apache-spark-examples-and-use-cases</a><br>Distributed Stream and Graph Processing with Apache Flink<br><a href="https://www.mapr.com/blog/distributed-stream-and-graph-processing-apache-flink" target="_blank" rel="external">https://www.mapr.com/blog/distributed-stream-and-graph-processing-apache-flink</a><br>A Quick Guide to Spark Streaming<br><a href="https://www.mapr.com/blog/quick-guide-spark-streaming" target="_blank" rel="external">https://www.mapr.com/blog/quick-guide-spark-streaming</a></p>
<p>参考</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Cloudera">Cloudera</h2><h2 id="Hortonworks">Hortonworks</h2><p>Hortonworks DataFlow 1.1 Released<br><a href="http://hortonworks.com/]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[JVM监控与调优]]></title>
    <link href="http://navigating.github.io/2015/JVM%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98/"/>
    <id>http://navigating.github.io/2015/JVM监控与调优/</id>
    <published>2015-11-06T07:53:46.000Z</published>
    <updated>2015-11-10T08:18:37.678Z</updated>
    <content type="html"><![CDATA[<h2 id="题外话">题外话</h2><p>本文当前的范围是Java 8之前的虚拟机，因为Java 8之后虚拟机的架构有比较大的调整，存储类的元数据信息的永久代被删除了，而是放在虚拟机的元空间。<br>JDK 7及之前的版本中永久代有如下的特点：</p>
<ol>
<li>永久代和堆在内存分配上是相连的；</li>
<li>永久代的垃圾回收和老年代的垃圾回收是绑定的，一旦其中一个区域被占满，这两个区都要进行垃圾回收；</li>
<li>永久代一段连续的内存空间，在32位机器默认的永久代的大小为64M，64位的机器则为85M；当然，在JVM启动之前可以通过设置-XX:MaxPermSize的值来控制永久代的大小；</li>
</ol>
<h2 id="Java虚拟机">Java虚拟机</h2><p>Java虚拟机的架构如下图，其中和性能调优相关的组件有三个：Heap，JIT compiler 和 Garbage Collector。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_001.PNG" alt="这是一张图片"></p>
<p>针对内存堆heap中创建的Java对象，采用的是垃圾回收机制进行处理。垃圾回收在Oracle官网叫 Automatic Garbage Collection，其目的寻找确定堆内存中哪些对象在使用，哪些不在使用，并且删除销毁那些不在使用的对象。<br>垃圾回收分为三步：</p>
<ol>
<li>标记，Marking<br>这个阶段识别出哪些对象正在使用，哪些对象不再使用，不再使用的对象标记为可回收的对象，在使用的对象标记为不可回收。在标记阶段需要对所有的对象进行全扫描来做决策。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_003.PNG" alt="这是一张图片"></li>
<li>清除，Normal Deletion<br>这个阶段移除那些没有被引用的Java对象，并释放内存空间。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_004.PNG" alt="这是一张图片"></li>
<li>压缩，Deletion with Compacting<br>为了更好的性能，需要对不可回收依然使用的对象进行压缩，就是把这一类对象迁移在一起，使得新内存的分配变得的更容易和更快。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_005.PNG" alt="这是一张图片"></li>
</ol>
<h2 id="分代垃圾回收(Generational_Garbage_Collection)">分代垃圾回收(Generational Garbage Collection)</h2><p>由于标记和压缩堆内存中所有的对象是十分低效的，并且随着越来越多的对象被分配，垃圾回收的时间也会变得越来越长，因此引入引入了如下的回收策略：按着对象存活的时间窗口采用不同的垃圾回收机制，即是 Minor collections 和 Major collections。如下图：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_006.gif" alt="这是一张图片"><br>其中Y轴标识已分配的字节数，X轴标识随着时间已经被分配内存并且处于使用状态的字节数的情况。从这张图可以看出，只有很少的对象能够长时间需要保留下来，大部分对象只有很短的生命周期。</p>
<p>Java Hotspot Heap结构：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_002.PNG" alt="这是一张图片"><br>Hotspot内存由三部分组成：</p>
<ol>
<li>持久代，Permanent Generation：存储类和对象元数据的数据的地方，包括类的层级信息，方法数据和方法信息（如字节码，栈和变量大小），运行时常量池，已确定的符号引用和虚方法表。</li>
<li>年轻代，Young Generation：分为三个区，一个Eden区，两个Survivor区。新生代主要是存放新生成的Java对象，新生代的垃圾回收称为 minor garbage collection。</li>
<li>年老代，Old Genration： 即是Tenured区，存放在年轻代经过多次垃圾回收依然存活的对象的区域，年老代的垃圾回收称为 major garbage collection。</li>
</ol>
<h2 id="垃圾回收触发时机">垃圾回收触发时机</h2><p>Minor Collection<br>    在Eden空间已满，新对象申请空间失败时，就会触发Minor Collection，对Eden区域进行GC，清除非存活对象，并把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。这种方式的GC是对年轻代的Eden区进行，不会影响到年老代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。因而，一般在这里需要使用速度快、效率高的算法，使Eden去能尽快空闲出来。</p>
<p>Major Collection(Full GC)<br>    对整个堆进行整理，包括Young、Tenured和Perm。Major Collection因为需要对整个对进行回收，所以比Minor Collection要慢，因此应该尽可能减少Full GC的次数。在对JVM调优的过程中，很大一部分工作就是对于Major Collection的调节。有如下原因可能导致Full GC：</p>
<ol>
<li>年老代（Tenured）被写满;</li>
<li>持久代（Perm）被写满;</li>
<li>System.gc()被显示调用;</li>
<li>上一次GC之后Heap的各域分配策略动态变化;</li>
</ol>
<h2 id="Garbage_Collector">Garbage Collector</h2><p>经过发展，Java已有如下的垃圾回收器：</p>
<ol>
<li><p>Serial收集器/SerialOld收集器<br>Serial收集器/Serial Old收集器，是单线程的，使用“复制”算法。当它工作时，必须暂停其它所有工作线程。特点：简单而高效。对于运行在Client模式下的虚拟机来说是一个很好的选择。<br>串行收集器并不是只能使用一个CPU进行收集，而是当JVM需要进行垃圾回收的时候，需要中断所有的用户线程，知道它回收结束为止，因此又号称“Stop The World” 的垃圾回收器。<br>Serial收集器默认新旧生代的回收器搭配为Serial+ SerialOld</p>
</li>
<li><p>ParNew收集器<br>ParNew收集器其实就是多线程版本的Serial收集器，同样有<br>Stop The World的问题，他是多CPU模式下的首选回收器（该回收器在单CPU的环境下回收效率远远低于Serial收集器，所以一定要注意场景哦），也是Server模式下默认的新生代收集器。除了Serial收集器外，目前只有它能与CMS收集器配合工作。</p>
</li>
<li><p>ParallelScavenge/ParallelOld收集器<br>ParallelScavenge又被称为是吞吐量优先的收集器，也是使用“复制”算法的、并行的多线程收集器。这些都和ParNew收集器一样。但它关注的是吞吐量（CPU用于运行用户代码的时间与CPU总消耗时间的比值），而其它收集器（Serial/Serial Old、ParNew、CMS）关注的是垃圾收集时用户线程的停顿时间。<br>Parallel Old收集器是Parallel Scavenge收集器的老年代版本。</p>
</li>
<li><p>CMS<br>CMS(Concurrent Mark Sweep）)又称响应时间优先(最短回收停顿)的回收器，使用并发模式回收垃圾，使用”标记-清除“算法，CMS对CPU是非常敏感的，它的回收线程数=（CPU+3）/4，因此当CPU是2核的实惠，回收线程将占用的CPU资源的50%，而当CPU核心数为4时仅占用25%。<br>CMS收集器分4个步骤进行垃圾收集工作：<br>a. 初始标记(CMS initial mark)<br>b. 并发标记(CMS concurrent mark)<br>c. 重新标记(CMS remark)<br>d. 并发清除(CMS concurrent sweep)</p>
</li>
<li><p>GarbageFirst(G1)<br>G1（Garbage First）收集器，基于“标记-整理”算法，可以非常精确地控制停顿。其实这是一个新的垃圾回收器，既可以回收新生代也可以回收旧生代，SunHotSpot 1.6u14以上EarlyAccess版本加入了这个回收器</p>
</li>
</ol>
<h2 id="监控命令">监控命令</h2><p>JDK自带的性能调优监控工具，包括：VisualVM、jConsole、jps、jstack、jmap、jhat、jstat、hprof等。<br>jstack 主要用于查看Java进程内的线程堆栈信息。<br>例子：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># jps</span></span><br><span class="line"><span class="number">6923</span> HelloWorld</span><br><span class="line"><span class="number">14009</span> Jps</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># jstack -m 6923</span></span><br></pre></td></tr></table></figure></p>
<p>jmap 主要用于查看堆内存的使用情况。<br>例子：<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"># jmap -heap 6923 </span><br><span class="line">Attaching to process ID 6923, please wait...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.</span><br><span class="line">JVM version is 24.65-b04</span><br><span class="line"></span><br><span class="line">using parallel threads in the new generation.</span><br><span class="line">using thread-local object allocation.</span><br><span class="line">Concurrent Mark-Sweep GC</span><br><span class="line"></span><br><span class="line">Heap Configuration:</span><br><span class="line">   MinHeapFreeRatio = 40</span><br><span class="line">   MaxHeapFreeRatio = 70</span><br><span class="line">   MaxHeapSize      = <span class="number">25769803776</span> (<span class="number">24576.0</span>MB)</span><br><span class="line">   NewSize          = <span class="number">2147483648</span> (2048.0MB)</span><br><span class="line">   MaxNewSize       = <span class="number">2147483648</span> (2048.0MB)</span><br><span class="line">   OldSize          = <span class="number">5439488</span> (5.1875MB)</span><br><span class="line">   NewRatio         = 2</span><br><span class="line">   SurvivorRatio    = 8</span><br><span class="line">   PermSize         = <span class="number">21757952</span> (20.75MB)</span><br><span class="line">   MaxPermSize      = <span class="number">85983232</span> (82.0MB)</span><br><span class="line">   G1HeapRegionSize = 0 (0.0MB)</span><br><span class="line"></span><br><span class="line">Heap Usage:</span><br><span class="line">New Generation (Eden + 1 Survivor Space):</span><br><span class="line">   capacity = <span class="number">1932787712</span> (1843.25MB)</span><br><span class="line">   used     = <span class="number">1216988784</span> (<span class="number">1160.61094</span><span class="number">66552734</span>MB)</span><br><span class="line">   free     = <span class="number">715798928</span> (<span class="number">682.63905</span><span class="number">33447266</span>MB)</span><br><span class="line">   <span class="number">62.96546570</span>759655% used</span><br><span class="line">Eden Space:</span><br><span class="line">   capacity = <span class="number">1718091776</span> (1638.5MB)</span><br><span class="line">   used     = <span class="number">1210401768</span> (<span class="number">1154.32907</span><span class="number">86743164</span>MB)</span><br><span class="line">   free     = <span class="number">507690008</span> (<span class="number">484.1709213</span>256836MB)</span><br><span class="line">   <span class="number">70.45035573</span>233545% used</span><br><span class="line">From Space:</span><br><span class="line">   capacity = <span class="number">214695936</span> (204.75MB)</span><br><span class="line">   used     = <span class="number">6587016</span> (<span class="number">6.28186798</span><span class="number">0957031</span>MB)</span><br><span class="line">   free     = <span class="number">208108920</span> (<span class="number">198.4681320190</span>4297MB)</span><br><span class="line">   <span class="number">3.06806738</span><span class="number">9966804</span>% used</span><br><span class="line">To Space:</span><br><span class="line">   capacity = <span class="number">214695936</span> (204.75MB)</span><br><span class="line">   used     = 0 (0.0MB)</span><br><span class="line">   free     = <span class="number">214695936</span> (204.75MB)</span><br><span class="line">   0.0% used</span><br><span class="line">concurrent mark-sweep generation:</span><br><span class="line">   capacity = <span class="number">23622320128</span> (<span class="number">22528.0</span>MB)</span><br><span class="line">   used     = <span class="number">9615124536</span> (<span class="number">9169.69731</span><span class="number">9030762</span>MB)</span><br><span class="line">   free     = <span class="number">14007195592</span> (<span class="number">13358.30268</span><span class="number">0969238</span>MB)</span><br><span class="line">   <span class="number">40.70355699</span><span class="number">1436265</span>% used</span><br><span class="line">Perm Generation:</span><br><span class="line">   capacity = <span class="number">64733184</span> (<span class="number">61.734375</span>MB)</span><br><span class="line">   used     = <span class="number">39187912</span> (<span class="number">37.372505187</span>98828MB)</span><br><span class="line">   free     = <span class="number">25545272</span> (<span class="number">24.36186981</span>201172MB)</span><br><span class="line">   <span class="number">60.53759382</span><span class="number">5139204</span>% used</span><br><span class="line"></span><br><span class="line">11258 interned Strings occupying <span class="number">1021928</span> bytes.</span><br></pre></td></tr></table></figure></p>
<p>jstat 主要用于查看Java进程的统计信息。</p>
<p>例子1：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># <span class="tag">jstat</span> <span class="tag">-gc</span> 6923 2000 200000</span><br><span class="line"> <span class="tag">S0C</span>    <span class="tag">S1C</span>    <span class="tag">S0U</span>    <span class="tag">S1U</span>      <span class="tag">EC</span>       <span class="tag">EU</span>        <span class="tag">OC</span>         <span class="tag">OU</span>       <span class="tag">PC</span>     <span class="tag">PU</span>    <span class="tag">YGC</span>     <span class="tag">YGCT</span>    <span class="tag">FGC</span>    <span class="tag">FGCT</span>     <span class="tag">GCT</span>   </span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span> 83598<span class="class">.2</span> 39179<span class="class">.8</span> 1677824<span class="class">.0</span> 1677824<span class="class">.0</span> 23068672<span class="class">.0</span> 10786771<span class="class">.2</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21541 11322<span class="class">.520</span>   6     35<span class="class">.315</span> 11357<span class="class">.835</span></span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span>  0<span class="class">.0</span>   48833<span class="class">.8</span> 1677824<span class="class">.0</span> 502293<span class="class">.9</span> 23068672<span class="class">.0</span> 10787056<span class="class">.8</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21541 11325<span class="class">.066</span>   6     35<span class="class">.315</span> 11360<span class="class">.380</span></span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span>  0<span class="class">.0</span>   48833<span class="class">.8</span> 1677824<span class="class">.0</span> 1013386<span class="class">.7</span> 23068672<span class="class">.0</span> 10787056<span class="class">.8</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21541 11325<span class="class">.066</span>   6     35<span class="class">.315</span> 11360<span class="class">.380</span></span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span>  0<span class="class">.0</span>   48833<span class="class">.8</span> 1677824<span class="class">.0</span> 1515312<span class="class">.1</span> 23068672<span class="class">.0</span> 10787056<span class="class">.8</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21541 11325<span class="class">.066</span>   6     35<span class="class">.315</span> 11360<span class="class">.380</span></span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span> 79664<span class="class">.9</span>  0<span class="class">.0</span>   1677824<span class="class">.0</span> 363216<span class="class">.4</span> 23068672<span class="class">.0</span> 10787062<span class="class">.6</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21542 11325<span class="class">.180</span>   6     35<span class="class">.315</span> 11360<span class="class">.494</span></span><br></pre></td></tr></table></figure></p>
<p>显示结果标题栏字段含义：<br>S0C：Survivor 0区容量(Capacity)。<br>S1C：Survivor 1区容量(Capacity)。<br>S0U：Survivor 0区使用量(Used)。<br>S1U：Survivor 1区使用量(Used)。<br>EC：  Eden区容量。<br>EU：  Eden区使用量。<br>OC：  Old区容量。<br>OU：  Old区使用量。<br>PC：  Perm区容量。<br>PU：  Perm区使用量。<br>YGC： Young GC次数。<br>YGCT：Young GC耗时。<br>FGC： Full GC次数。<br>FGCT：Full GC耗时。<br>GCT： GC总耗时。</p>
<p>例子2：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># <span class="tag">jstat</span> <span class="tag">-gcutil</span> 6923 2000 1000</span><br><span class="line">  <span class="tag">S0</span>     <span class="tag">S1</span>     <span class="tag">E</span>      <span class="tag">O</span>      <span class="tag">P</span>     <span class="tag">YGC</span>     <span class="tag">YGCT</span>    <span class="tag">FGC</span>    <span class="tag">FGCT</span>     <span class="tag">GCT</span>   </span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.36</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.36</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.36</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.39</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.42</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.80</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.80</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  71<span class="class">.04</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  71<span class="class">.17</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br></pre></td></tr></table></figure></p>
<h2 id="GC_日志输出参数配置">GC 日志输出参数配置</h2><ul>
<li>打开 -verbose:gc 开关可显示GC的操作内容，包括最忙和最空闲收集行为发生的时间、收集前后的内存大小、收集需要的时间等。</li>
<li>打开 -xx:+printGCdetails 开关，可以详细了解GC中的变化。</li>
<li>打开 -XX:+PrintGCTimeStamps 开关，可以了解这些垃圾收集发生的时间，自JVM启动以后以秒计量。</li>
<li>打开 -xx:+PrintHeapAtGC 开关了解堆的更详细的信息。</li>
<li>打开 -XX:+PrintTenuringDistribution 开关了解获得使用期的对象权。</li>
<li>打开 -Xloggc:/var/log/gclogs/gc.log gc日志产生的路径</li>
<li>打开 -XX:+PrintGCApplicationStoppedTime 输出GC造成应用暂停的时间</li>
<li>打开 -XX:+PrintGCDateStamps GC发生的时间信息</li>
</ul>
<h2 id="GC_日志输出样例">GC 日志输出样例</h2><p>Minor GC日志：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2015<span class="tag">-05-08T15</span><span class="pseudo">:50</span><span class="pseudo">:10</span><span class="class">.113</span>+0800: 5<span class="class">.930</span>: <span class="attr_selector">[GC2015-05-08T15:50:10.113+0800: 5.930: [ParNew: 174706K-&gt;16932K(184320K), 0.0309770 secs]</span> 201085<span class="tag">K-</span>&gt;43310<span class="tag">K</span>(1028096<span class="tag">K</span>), 0<span class="class">.0311100</span> <span class="tag">secs</span>] <span class="attr_selector">[Times: user=0.14 sys=0.00, real=0.03 secs]</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>表示发生一次Minor GC，ParNew是新生代的gc算法，174706K表示Eden区的存活对象的内存总和，16932K表示回收后的存活对象的内存总和，184320K是整个eden区的内存总和。0.0309770 secs表示minor gc花费的时间。</li>
<li>201085K-&gt;43310K(1028096K) 表明这个JVM Heap从 201085K 降低到了 43310K。</li>
<li>[Times: user=0.14 sys=0.00, real=0.03 secs]表明这次GC的user time是0.14，而real time是0.03秒；( user/sys/real 的解释参见：<a href="http://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1" target="_blank" rel="external">http://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1</a> )</li>
</ul>
<p>Full GC日志：<br><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">08</span>T17:<span class="number">31</span>:<span class="number">19.816</span>+<span class="number">0800</span>: <span class="number">8317.639</span>: [<span class="keyword">Full</span> GC2015-<span class="number">04</span>-<span class="number">08</span>T17:<span class="number">31</span>:<span class="number">19.816</span>+<span class="number">0800</span>: <span class="number">8317.639</span>: [CMS: <span class="number">603725</span><span class="keyword">K</span>-&gt;<span class="number">464743</span><span class="keyword">K</span>(<span class="number">843776</span><span class="keyword">K</span>), <span class="number">0.6577700</span> secs] <span class="number">788045</span><span class="keyword">K</span>-&gt;<span class="number">464743</span><span class="keyword">K</span>(<span class="number">1028096</span><span class="keyword">K</span>), [CMS Perm : <span class="number">40447</span><span class="keyword">K</span>-&gt;<span class="number">40446</span><span class="keyword">K</span>(<span class="number">67100</span><span class="keyword">K</span>)], <span class="number">0.6579650</span> secs] [<span class="keyword">Times</span>: user=<span class="number">0.54</span> sys=<span class="number">0.00</span>, real=<span class="number">0.65</span> secs]</span><br></pre></td></tr></table></figure></p>
<ul>
<li>最前面的数字 8317.639 代表GC发生的时间，是从Java虚拟机启动以来经过的秒数；</li>
<li>表示发生了一次Full GC，有Full说明发生了Stop-The-World，，如果是调用system.gc()方法所触发的收集，将显示(System)；</li>
<li>整个JVM都停顿了 0.6577700 秒，输出说明同上。只是 CMS: 603725K-&gt;464743K(843776K), 0.6577700 secs 表示的是Old区。788045K-&gt;464743K(1028096K) 表示的是整个Heap区。</li>
<li>CMS Perm表示GC发生的区域，名称是由收集器决定的。</li>
</ul>
<h2 id="参考：">参考：</h2><p><a href="http://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/generations.html" target="_blank" rel="external">http://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/generations.html</a><br><a href="http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html" target="_blank" rel="external">http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html</a><br><a href="http://www.oracle.com/technetwork/java/gc-tuning-5-138395.html" target="_blank" rel="external">http://www.oracle.com/technetwork/java/gc-tuning-5-138395.html</a><br><a href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" target="_blank" rel="external">http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html</a><br><a href="http://www.infoq.com/cn/articles/Java-PERMGEN-Removed" target="_blank" rel="external">http://www.infoq.com/cn/articles/Java-PERMGEN-Removed</a><br><a href="http://docs.oracle.com/cd/E21764_01/web.1111/e13814/jvm_tuning.htm#PERFM169" target="_blank" rel="external">http://docs.oracle.com/cd/E21764_01/web.1111/e13814/jvm_tuning.htm#PERFM169</a><br><a href="http://blog.csdn.net/ning109314/article/details/10411495" target="_blank" rel="external">http://blog.csdn.net/ning109314/article/details/10411495</a><br><a href="http://jbutton.iteye.com/blog/1569746" target="_blank" rel="external">http://jbutton.iteye.com/blog/1569746</a><br><a href="http://buddie.iteye.com/blog/1824937" target="_blank" rel="external">http://buddie.iteye.com/blog/1824937</a><br><a href="http://www.cnblogs.com/ggjucheng/p/3977384.html" target="_blank" rel="external">http://www.cnblogs.com/ggjucheng/p/3977384.html</a><br><a href="http://blog.csdn.net/historyasamirror/article/details/6233007" target="_blank" rel="external">http://blog.csdn.net/historyasamirror/article/details/6233007</a><br><a href="http://sargeraswang.com/blog/2014/02/03/la-ji-shou-ji-qi-yu-nei-cun-fen-pei-ce-lue/" target="_blank" rel="external">http://sargeraswang.com/blog/2014/02/03/la-ji-shou-ji-qi-yu-nei-cun-fen-pei-ce-lue/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="题外话">题外话</h2><p>本文当前的范围是Java 8之前的虚拟机，因为Java 8之后虚拟机的架构有比较大的调整，存储类的元数据信息的永久代被删除了，而是放在虚拟机的元空间。<br>JDK 7及之前的版本中永久代有如下的特点：</p>
<ol>
<li>永]]>
    </summary>
    
      <category term="JVM" scheme="http://navigating.github.io/tags/JVM/"/>
    
      <category term="Java" scheme="http://navigating.github.io/tags/Java/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201509]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201509/"/>
    <id>http://navigating.github.io/2015/大数据动态之201509/</id>
    <published>2015-10-16T02:41:31.000Z</published>
    <updated>2015-10-16T02:43:52.651Z</updated>
    <content type="html"><![CDATA[<p>Apache Kylin<br>Apache Kylin v1.0 发布，分布式分析引擎<br><a href="http://www.oschina.net/news/65938/apache-kylin-1-0-released" target="_blank" rel="external">http://www.oschina.net/news/65938/apache-kylin-1-0-released</a> </p>
<p>Apache Calcite<br>Apache Calcite：Hadoop中新型大数据查询引擎<br><a href="http://www.infoq.com/cn/articles/new-big-data-hadoop-query-engine-apache-calcite" target="_blank" rel="external">http://www.infoq.com/cn/articles/new-big-data-hadoop-query-engine-apache-calcite</a> </p>
<p>Apache Flink<br><a href="http://flink.apache.org/" target="_blank" rel="external">http://flink.apache.org/</a> </p>
<p>Cloudera<br>新的快数据存储Hadoop组件，Kudu:<br><a href="http://blog.cloudera.com/blog/2015/09/kudu-new-apache-hadoop-storage-for-fast-analytics-on-fast-data/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/kudu-new-apache-hadoop-storage-for-fast-analytics-on-fast-data/</a><br>Hadoop细粒度的安全增强组件，RecordService：<br><a href="http://blog.cloudera.com/blog/2015/09/recordservice-for-fine-grained-security-enforcement-across-the-hadoop-ecosystem/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/recordservice-for-fine-grained-security-enforcement-across-the-hadoop-ecosystem/</a><br>HDFS Erasure Coding特性<br><a href="http://blog.cloudera.com/blog/2015/09/introduction-to-hdfs-erasure-coding-in-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/introduction-to-hdfs-erasure-coding-in-apache-hadoop/</a><br>Spark测试基础库<br><a href="http://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/</a><br>如何使用Impala对非结构化数据进行分析：<br><a href="http://blog.cloudera.com/blog/2015/09/how-to-prepare-unstructured-data-in-impala-for-analysis/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/how-to-prepare-unstructured-data-in-impala-for-analysis/</a><br>BI场景下Impala测试结果<br><a href="http://blog.cloudera.com/blog/2015/09/how-impala-scales-for-business-intelligence-new-test-results/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/how-impala-scales-for-business-intelligence-new-test-results/</a><br>揭秘Apache Hadoop YARN:<br><a href="http://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/</a><br><a href="http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-users/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-users/</a><br><a href="http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-operators/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-operators/</a><br>Impala支持shell执行的动态进度报告(Impala’s debug webpages (http:::25000))：<br><a href="http://blog.cloudera.com/blog/2015/09/dynamic-progress-reports-in-the-impala-shell/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/dynamic-progress-reports-in-the-impala-shell/</a><br>Cloudera One Platform:<br><a href="http://vision.cloudera.com/one-platform/" target="_blank" rel="external">http://vision.cloudera.com/one-platform/</a> </p>
<p>Hortonworks<br>Microsoft Azure HDInsight对Ubuntu Linux支持，可以支持到Hadoop 2.6：<br><a href="http://hortonworks.com/blog/microsoft-azure-hdinsight-on-linux-choice/" target="_blank" rel="external">http://hortonworks.com/blog/microsoft-azure-hdinsight-on-linux-choice/</a><br>HDP 2.3 Sandbox在Microsoft Azure Gallery上线：<br><a href="http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/</a><br>Impala与Hive性能对比<br><a href="http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/" target="_blank" rel="external">http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/</a><br>HDP迁移案例：<br><a href="http://hortonworks.com/blog/migration-to-hdp-as-easy-as-1-2-3-without-downtime-or-disruption/" target="_blank" rel="external">http://hortonworks.com/blog/migration-to-hdp-as-easy-as-1-2-3-without-downtime-or-disruption/</a> </p>
<p>MapR<br>Spark on YARN资源分配配置<br><a href="https://www.mapr.com/blog/resource-allocation-configuration-spark-yarn#.VfoRrSWqqko" target="_blank" rel="external">https://www.mapr.com/blog/resource-allocation-configuration-spark-yarn#.VfoRrSWqqko</a><br><a href="https://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="external">https://spark.apache.org/docs/latest/running-on-yarn.html</a><br><a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="external">https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation</a><br>SAP HANA与Mapr DP混合架构<br><a href="https://www.mapr.com/blog/sap-hana-vora-and-mapr-data-platform#.VfoRsCWqqko" target="_blank" rel="external">https://www.mapr.com/blog/sap-hana-vora-and-mapr-data-platform#.VfoRsCWqqko</a><br>Spark Streaming with HBase<br><a href="https://www.mapr.com/blog/spark-streaming-hbase#.VfoR0SWqqko" target="_blank" rel="external">https://www.mapr.com/blog/spark-streaming-hbase#.VfoR0SWqqko</a><br>MapR对Docker的支持<br><a href="https://www.mapr.com/blog/how-create-instant-mapr-clusters-docker#.VfoR2CWqqko" target="_blank" rel="external">https://www.mapr.com/blog/how-create-instant-mapr-clusters-docker#.VfoR2CWqqko</a><br><a href="https://www.mapr.com/blog/my-experience-running-docker-containers-on-mesos#.Vfoc_yWqqkp" target="_blank" rel="external">https://www.mapr.com/blog/my-experience-running-docker-containers-on-mesos#.Vfoc_yWqqkp</a> </p>
<p>Databricks<br>Spark Survey 2015调查结果：<br><a href="https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html</a><br>Spark代码调试：实时进度条和Spark UI<br><a href="https://databricks.com/blog/2015/09/23/easier-spark-code-debugging-real-time-progress-bar-and-spark-ui-integration-in-databricks.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/23/easier-spark-code-debugging-real-time-progress-bar-and-spark-ui-integration-in-databricks.html</a><br>新版本Spark 1.5上LDA算法的性能提升：<br><a href="https://databricks.com/blog/2015/09/22/large-scale-topic-modeling-improvements-to-lda-on-spark.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/22/large-scale-topic-modeling-improvements-to-lda-on-spark.html</a><br>Spark 1.5 DataFrame API:<br><a href="https://databricks.com/blog/2015/09/16/spark-1-5-dataframe-api-highlights-datetimestring-handling-time-intervals-and-udafs.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/16/spark-1-5-dataframe-api-highlights-datetimestring-handling-time-intervals-and-udafs.html</a><br>Spark 1.5发布，在性能、可用性、运维、Data Science API等方面有重大改进：<br><a href="https://databricks.com/blog/2015/09/09/announcing-spark-1-5.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/09/announcing-spark-1-5.html</a><br><a href="http://www.csdn.net/article/2015-09-29/2825825" target="_blank" rel="external">http://www.csdn.net/article/2015-09-29/2825825</a><br><a href="http://www.csdn.net/article/2015-09-10/2825669" target="_blank" rel="external">http://www.csdn.net/article/2015-09-10/2825669</a> </p>
<p>MongoDB<br>MongoDB性能优化五个简单步骤：<br><a href="http://www.csdn.net/article/2015-09-30/2825833" target="_blank" rel="external">http://www.csdn.net/article/2015-09-30/2825833</a><br>MongoDB开发版本3.1.8发布<br><a href="http://www.csdn.net/article/2015-09-17/2825734" target="_blank" rel="external">http://www.csdn.net/article/2015-09-17/2825734</a><br>分布式文档数据库MongoDB开发版本3.1.7发布<br><a href="http://www.csdn.net/article/2015-09-01/2825599-mongodb-317-is-released" target="_blank" rel="external">http://www.csdn.net/article/2015-09-01/2825599-mongodb-317-is-released</a> </p>
<p>参考<br>逆水行舟，看前行中的Spark<br><a href="http://www.csdn.net/article/2015-09-21/2825754" target="_blank" rel="external">http://www.csdn.net/article/2015-09-21/2825754</a><br>微店的大数据平台建设实践与探讨<br><a href="http://www.csdn.net/article/2015-09-21/2825756" target="_blank" rel="external">http://www.csdn.net/article/2015-09-21/2825756</a><br>打造数据驱动的组织：第二年<br><a href="http://zhuanlan.zhihu.com/donglaoshi/20205116" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20205116</a><br>揭开 Growth Hacking 的神秘面纱（上篇）<br><a href="http://zhuanlan.zhihu.com/qinchao/20190015" target="_blank" rel="external">http://zhuanlan.zhihu.com/qinchao/20190015</a><br>京东大数据基础架构和实践—王彦明<br><a href="http://share.csdn.net/slides/9138" target="_blank" rel="external">http://share.csdn.net/slides/9138</a><br>京东数据仓库海量数据交换工具—张侃<br><a href="http://share.csdn.net/slides/9137" target="_blank" rel="external">http://share.csdn.net/slides/9137</a><br>京东大数据分析与创新应用<br><a href="http://share.csdn.net/slides/9139" target="_blank" rel="external">http://share.csdn.net/slides/9139</a><br>LinkedIn架构这十年<br><a href="http://engineering.linkedin.com/architecture/brief-history-scaling-linkedin" target="_blank" rel="external">http://engineering.linkedin.com/architecture/brief-history-scaling-linkedin</a><br><a href="http://colobu.com/2015/07/24/brief-history-scaling-linkedin/" target="_blank" rel="external">http://colobu.com/2015/07/24/brief-history-scaling-linkedin/</a><br>LinkedIn是如何优化Kafka的<br><a href="http://www.infoq.com/cn/articles/linkedIn-improving-kafka" target="_blank" rel="external">http://www.infoq.com/cn/articles/linkedIn-improving-kafka</a><br><a href="http://engineering.linkedin.com/apache-kafka/how-we%E2%80%99re-improving-and-advancing-kafka-linkedin" target="_blank" rel="external">http://engineering.linkedin.com/apache-kafka/how-we%E2%80%99re-improving-and-advancing-kafka-linkedin</a><br>阿里CDN从自建到服务<br><a href="http://share.csdn.net/slides/8319" target="_blank" rel="external">http://share.csdn.net/slides/8319</a><br>系统架构设计-负载均衡和高可用<br><a href="http://share.csdn.net/slides/12338" target="_blank" rel="external">http://share.csdn.net/slides/12338</a><br>OSTC2015-朱照远（叔度）阿里开源经验分享<br><a href="http://share.csdn.net/slides/13730" target="_blank" rel="external">http://share.csdn.net/slides/13730</a><br>Voidbox<br><a href="http://dongxicheng.org/mapreduce-nextgen/voidbox-docker-on-hadoop-hulu/" target="_blank" rel="external">http://dongxicheng.org/mapreduce-nextgen/voidbox-docker-on-hadoop-hulu/</a><br>深入理解Spark Streaming执行模型<br><a href="http://www.csdn.net/article/2015-09-13/2825689" target="_blank" rel="external">http://www.csdn.net/article/2015-09-13/2825689</a><br>Apache Spark 1.5新特性介绍<br><a href="http://www.csdn.net/article/2015-09-10/2825669" target="_blank" rel="external">http://www.csdn.net/article/2015-09-10/2825669</a><br>盘点大数据生态圈，那些繁花似锦的开源项目<br><a href="http://www.csdn.net/article/2015-09-11/2825674" target="_blank" rel="external">http://www.csdn.net/article/2015-09-11/2825674</a><br>Redis整合Spring项目搭建实例<br><a href="http://www.csdn.net/article/2015-09-01/2825600" target="_blank" rel="external">http://www.csdn.net/article/2015-09-01/2825600</a><br>MongoDB开发版本3.1.8发布<br><a href="http://www.csdn.net/article/2015-09-17/2825734" target="_blank" rel="external">http://www.csdn.net/article/2015-09-17/2825734</a><br>分布式并行数据库将在 OLTP 领域促进去“Oracle”<br><a href="http://www.csdn.net/article/2015-09-11/2825678" target="_blank" rel="external">http://www.csdn.net/article/2015-09-11/2825678</a><br>Gartner 2015新兴技术发展周期简评：大数据实用化、机器学习崛起<br><a href="http://www.csdn.net/article/2015-09-06/2825620" target="_blank" rel="external">http://www.csdn.net/article/2015-09-06/2825620</a><br>Hortonworks收购Onyara，启动数据流自动化<br><a href="http://www.csdn.net/article/2015-09-02/2825612" target="_blank" rel="external">http://www.csdn.net/article/2015-09-02/2825612</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Apache Kylin<br>Apache Kylin v1.0 发布，分布式分析引擎<br><a href="http://www.oschina.net/news/65938/apache-kylin-1-0-released" target="_blank" rel]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《Impala vs. Hive Performance Benchmark》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8AImpala-vs-Hive-Performance-Benchmark%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《Impala-vs-Hive-Performance-Benchmark》/</id>
    <published>2015-09-24T04:56:08.000Z</published>
    <updated>2015-09-24T04:59:01.109Z</updated>
    <content type="html"><![CDATA[<p>原文：<a href="http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/" target="_blank" rel="external">http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/</a><br>学习如下：</p>
<p>本文是Yahoo! JAPAN针对自己的场景需求进行设计书选型，对Impala和Hive(Tez on YARN)所做的评测。<br>场景数据和要求：</p>
<ol>
<li>数据格式为 Text 或者 gz ;</li>
<li>每天新增数据文件为10G，数据记录为13亿行；</li>
<li>数据留存(retention)周期为13个月，共有数据6000G，共有4500亿行；</li>
<li>每个小时需要生成 15000 个报表(reporting)；</li>
<li>查询条件包含少量的的 grouping ，grouping的条件主要是地区(region)或者性别(gender)；</li>
<li>没有过滤条件查询；</li>
<li>绝大部分基于时间的报表(report)生成都是周期性的，除了小时报表，还有天报表和周报表；</li>
</ol>
<p>技术选型：</p>
<ol>
<li>Cloudera Impala，没提到所评估的版本，估计为最近的版本。当前Impala的最新版本为。</li>
<li>Hortonworks HDP 2.2, Apache Hive-0.14, Apache Tez。</li>
</ol>
<p>考虑Impala</p>
<ol>
<li>Imapa查询耗时比较少，一般在几秒到几十秒之间；</li>
<li>当一次查询的响应时间为15秒，每小时能够执行240次查询；</li>
<li>针对单位时间内处理量的不断增加，需要考虑通过增加单位时间内的并行查询数量来提升查询的数量；使用Impala遇到的问题是，随着查询并行度的增加，Impala查询的响应时间线性增加；因此，在每天数据更新之后Impala无法自动处理完成所有的批量查询。</li>
</ol>
<p>考虑Hive和Tez on YARN<br>Hadoop-2.x, YARN有更好粒度的并行执行控制，Tez引擎能大幅度的降低MapReduce的延时。<br>YARN和Tez大幅度的增加处理能力，每小时需要处理超过15000个任务。之前的Hadoop 1.0集群，每天已经能够处理100000个任务。</p>
<p>测试验证</p>
<p>测试条件</p>
<ol>
<li>计划模拟真实业务场景测试执行近2000个SQL；</li>
<li>绝大部分查询返回的结果少于 1000 行数据；</li>
<li>少数查询返回的结果超过100000行数据;</li>
<li>执行近2000个SQL的并发请求为32；</li>
</ol>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/HiveImpala_001.png" alt="这是一张图片"></p>
<p>测试结果</p>
<ol>
<li>Hive请求的大部分请求在20秒以内返回，随着返回结果集数据的增加查询时间也随之增加，最长返回时间为70秒；</li>
<li>Impala请求的大部分请求返回在30秒～60秒之间，最长返回时间为10分钟，随着返回结果集数据的增加查询时间大幅度显著的增加；</li>
<li>在一些低负载(low load)条件的查询中，Impala能够达到毫秒(milliseconds)级返回；如果没有SQL并行化的处理需求，Impala是有效的选择。</li>
<li>对于要求批处理，并且SQL并行化是必须的场景中，Hive on Tez是更好的选择。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/HiveImpala_002.png" alt="这是一张图片"></p>
<p>Hive的并发性<br>对于单个的HiveServer2实例，我们测试验证多少个并行查询可以被执行，多少并行度的处理是最有效的。<br>我们以16作为SQL并发执行的提升倍数，衡量的指标是SQL执行的处理时间。<br>在并发达到64之前查询的吞吐量快速增加，在这点之后开始下降，因此在当前环境下64是最好的并发数。</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/HiveImpala_003.png" alt="这是一张图片"></p>
<p>结论<br>对于Hive on Tez，单个SQL执行时间一般为15秒，会随着并行度的提升而增加。(并行度的限制主要依赖于集群的大小和性能。)<br>在低负载状态下Impala有非常快的响应时间，但并不适合于SQL并行度非常高的场景。<br>最后的结论是测试者最后选择了Hive on Tez，因为测试者的场景是每小时至少处理15000个SQL请求。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>原文：<a href="http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/" target="_blank" rel="external">http://hortonworks.com/blog]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="http://navigating.github.io/tags/Hive/"/>
    
      <category term="Impala" scheme="http://navigating.github.io/tags/Impala/"/>
    
      <category term="Tez" scheme="http://navigating.github.io/tags/Tez/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hortonworks HDP 2.3.0]]></title>
    <link href="http://navigating.github.io/2015/Hortonworks-HDP-2-3-0/"/>
    <id>http://navigating.github.io/2015/Hortonworks-HDP-2-3-0/</id>
    <published>2015-09-22T02:23:39.000Z</published>
    <updated>2015-09-22T02:26:21.433Z</updated>
    <content type="html"><![CDATA[<p>HDP 2.3 相对于 HDP 2.2.6</p>
<ol>
<li>HBase版本变化比较大，HBase 1.1.1；</li>
<li>新增加了一些组件；</li>
<li>Ambari在配置、监控等方面有较大的升级；</li>
</ol>
<p>新增加的组件：</p>
<pre><code>1. <span class="tag">Apache</span> <span class="tag">Atlas</span> 0<span class="class">.5</span><span class="class">.0</span>
2. <span class="tag">Apache</span> <span class="tag">Calcite</span> 1<span class="class">.2</span><span class="class">.0</span>
3. <span class="tag">Apache</span> <span class="tag">Solr</span> 5<span class="class">.2</span><span class="class">.1</span>
4. <span class="tag">Cascading</span> 3<span class="class">.0</span><span class="class">.1</span>
5. <span class="tag">Cloudbreak</span> 1<span class="class">.0</span>
6. <span class="tag">SmartSense</span>
</code></pre><p>版本升级的组件：</p>
<pre><code>1. <span class="tag">Apache</span> <span class="tag">Ambari</span> 2<span class="class">.1</span>
2. <span class="tag">Apache</span> <span class="tag">Hadoop</span> 2<span class="class">.7</span><span class="class">.1</span>
3. <span class="tag">Apache</span> <span class="tag">HBase</span> 1<span class="class">.1</span><span class="class">.1</span>
4. <span class="tag">Apache</span> <span class="tag">Spark</span> 1<span class="class">.3</span><span class="class">.1</span>
5. <span class="tag">Apache</span> <span class="tag">Hive</span> 1<span class="class">.2</span><span class="class">.1</span>
6. <span class="tag">Apache</span> <span class="tag">Kafka</span> 0<span class="class">.8</span><span class="class">.2</span>
7. <span class="tag">Apache</span> <span class="tag">Phoenix</span> 4<span class="class">.4</span><span class="class">.0</span>
8. <span class="tag">Apache</span> <span class="tag">Pig</span> 0<span class="class">.15</span><span class="class">.0</span>
9. <span class="tag">Apache</span> <span class="tag">Sqoop</span> 1<span class="class">.4</span><span class="class">.6</span>
10. <span class="tag">Apache</span> <span class="tag">Oozie</span> 4<span class="class">.2</span><span class="class">.0</span>
11. <span class="tag">Apache</span> <span class="tag">Knox</span> 0<span class="class">.6</span><span class="class">.0</span>
12. <span class="tag">Apache</span> <span class="tag">Ranger</span> 0<span class="class">.5</span><span class="class">.0</span>
13. <span class="tag">Apache</span> <span class="tag">Falcon</span> 0<span class="class">.6</span><span class="class">.1</span>
14. <span class="tag">Slider</span> 0<span class="class">.80</span><span class="class">.0</span>
15. <span class="tag">Tez</span> 0<span class="class">.7</span><span class="class">.0</span>
16. <span class="tag">Storm</span> 0<span class="class">.10</span><span class="class">.0</span>
</code></pre><p>Ambari 2.1新特性：</p>
<pre><code><span class="bullet">1. </span>通过Ambari参数配置UI优化；
<span class="bullet">2. </span>各项服务Metrics可以进行监控指标添加，通过添加Widgets就可以；
<span class="bullet">3. </span>支持Hive, Pig, Files, Capacity Scheduler的User Views界面，可以通过UI执行Hive、Pig语句；
<span class="bullet">4. </span>配置 Capacity Scheduler 支持图形化了，更加方便；
<span class="bullet">5. </span>支持 SQL User View；
<span class="bullet">6. </span>支持机架感应(Rack Awareness)配置：通过Ambari来配置；
<span class="bullet">7. </span>支持 Cloudbreak；
<span class="bullet">8. </span>支持 SmartSense；
</code></pre><p>系统要求：</p>
<pre><code>1. <span class="tag">Red</span> <span class="tag">Hat</span> <span class="tag">Enterprise</span> <span class="tag">Linux</span> (<span class="tag">RHEL</span>) <span class="tag">v6</span><span class="class">.x</span> 或者 <span class="tag">Red</span> <span class="tag">Hat</span> <span class="tag">Enterprise</span> <span class="tag">Linux</span> (<span class="tag">RHEL</span>) <span class="tag">v7</span><span class="class">.x</span>
2. <span class="tag">Oracle</span> <span class="tag">JDK</span> 1<span class="class">.8</span> 64<span class="tag">-bit</span> (<span class="tag">minimum</span> <span class="tag">JDK</span> 1<span class="class">.8_40</span>) (<span class="tag">default</span>) 或者 <span class="tag">Oracle</span> <span class="tag">JDK</span> 1<span class="class">.7</span> 64<span class="tag">-bit</span> (<span class="tag">minimum</span> <span class="tag">JDK</span> 1<span class="class">.7_67</span>)
</code></pre><p>参考：<br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_RelNotes/content/ch_relnotes_v230.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_RelNotes/content/ch_relnotes_v230.html</a><br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>HDP 2.3 相对于 HDP 2.2.6</p>
<ol>
<li>HBase版本变化比较大，HBase 1.1.1；</li>
<li>新增加了一些组件；</li>
<li>Ambari在配置、监控等方面有较大的升级；</li>
</ol>
<p>新增加的组件：</p>]]>
    </summary>
    
      <category term="Ambari" scheme="http://navigating.github.io/tags/Ambari/"/>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Oozie：入门概述]]></title>
    <link href="http://navigating.github.io/2015/Oozie%EF%BC%9A%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0/"/>
    <id>http://navigating.github.io/2015/Oozie：入门概述/</id>
    <published>2015-09-18T15:24:54.000Z</published>
    <updated>2015-09-18T15:40:18.791Z</updated>
    <content type="html"><![CDATA[<h3 id="Oozie能做什么(What_Oozie_Does)">Oozie能做什么(What Oozie Does)</h3><p>Oozie是一个Java Web应用，用于Apache Hadoop的任务(jobs)调度。Oozie顺序的合并多个任务(jobs)成为一个可工作的逻辑单元。其主要是集成了Hadoop技术栈，包括YARN等，支持Apache MapReduce, Apache Pig, Apache Hive, Apache Sqoop等。Oozie使得用户能够通过Java应用或者Shell脚本的方式调度任务。<br>Oozie任务有两种基本类型</p>
<pre><code>* Oozie Workflow jobs：这种任务是一个有向无环图(DAG, <span class="keyword">Direct</span> Acyclical Graph)，并按着规则顺序的执行，即上一个<span class="keyword">Action</span>运行完成后才能运行下一个<span class="keyword">Action</span>。所以其经常不得不等待。
* Oozie Coordinator jobs：这种任务是重复性的工作流，一般被时间或者数据达到可用会被触发。
</code></pre><p>Oozie Bundle提供一个复合的方式，将多个Workflow jobs和Coordinator jobs打包合并在一起并能对它们的生命周期进行管理。</p>
<h3 id="Oozie如何工作(How_Oozie_works)">Oozie如何工作(How Oozie works)</h3><p>一个Oozie Workflow是一系列编排成有向无环图（DAG）的Action集合。控制节点定义job时间,设置开始和结束workflow的规则(rules)。这样，Oozie通过decision，fork和join节点控制工作流的执行路径。Action节点触发任务的执行。<br>Oozie触发工作流的action操作，实际上由Hadoop MapReduce去执行。Oozie利用Hadoop技术栈来均衡负载和处理失败。<br>Oozie通过回调(callback)和轮询(polling)来检测任务的是否完成。当Oozie开始一个任务(task)，它的提供了一个唯一的可以回调的HTTP URL，当这个任务完成的时候就通知这个URL。如果任务失败就调用回调URL，Oozie可以设置任务完成。<br>经常有这种需求，在规则的时间间隔内运行Oozie workflow，处理那些无法预期的有效数据或者时间。在这些情况下，Oozie Coordinator允许你根据、时间或者事件的条件对工作流触发的时机进行建模。在这些条件得到满足之后，工作流任务就就开始启动。<br>Oozie Coordinator也可以管理多个工作流，是依赖于子工作流的输出结果。子工作流程的输出将会成为下一个工作流的输入。这条链被称为“数据应用管道”(data application pipeline)。</p>
<h3 id="工作流定义">工作流定义</h3><p>定义一个Oozie工作流，两个配置文件是必须的，job.properties和workflow.xml。<br>job.properties的环境变量如下：<br>nameNode                        hdfs://mycluster:8020                HDFS地址<br>jobTracker                        localhost:8034                        jobTracker地址<br>queueName                        default                                Oozie队列<br>examplesRoot                    examples                            全局目录<br>oozie.usr.system.libpath        true                                是否加载用户的lib库<br>oozie.libpath                    share/lib/user                        用户lib库<br>oozie.wf.application.path        ${nameNode}/user/${user.name}/        Oozie流程所在的HDFS地址</p>
<p>workflow.xml示例如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span><br><span class="line">  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">  distributed with this work for additional information</span><br><span class="line">  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">  to you under the Apache License, Version 2.0 (the</span><br><span class="line">  "License"); you may not use this file except in compliance</span><br><span class="line">  with the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">       http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an "AS IS" BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License.</span><br><span class="line">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">workflow-app</span> <span class="attribute">xmlns</span>=<span class="value">"uri:oozie:workflow:0.2"</span> <span class="attribute">name</span>=<span class="value">"map-reduce-wf"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">start</span> <span class="attribute">to</span>=<span class="value">"mr-node"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">action</span> <span class="attribute">name</span>=<span class="value">"mr-node"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">map-reduce</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="title">job-tracker</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="title">name-node</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">prepare</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">delete</span> <span class="attribute">path</span>=<span class="value">"$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">prepare</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.mapper.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.oozie.example.SampleMapper<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.reducer.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.oozie.example.SampleReducer<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.map.tasks<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.input.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/text<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.output.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">map-reduce</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">ok</span> <span class="attribute">to</span>=<span class="value">"end"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">error</span> <span class="attribute">to</span>=<span class="value">"fail"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">action</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">kill</span> <span class="attribute">name</span>=<span class="value">"fail"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">message</span>&gt;</span>Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="title">message</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">kill</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">end</span> <span class="attribute">name</span>=<span class="value">"end"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="实战命令">实战命令</h3><p>上传example目录到hdfs用户oozie根目录(/user/oozie)下：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su - oozie</span><br><span class="line">cd <span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>oozie-server/doc</span><br><span class="line">hdfs dfs -put example example</span><br></pre></td></tr></table></figure></p>
<p>启动任务命令：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie <span class="string">http:</span><span class="comment">//localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run</span></span><br></pre></td></tr></table></figure></p>
<p>停止任务命令：<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie http://localhost:11000/oozie -kill <span class="number">0000002-150</span><span class="number">914143759473</span>-oozie-oozi-W</span><br></pre></td></tr></table></figure></p>
<h3 id="Oozie_Web_UI效果图：">Oozie Web UI效果图：</h3><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_001.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_002.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_003.JPG" alt="这是一张图片"></p>
<p>参考：<br><a href="http://hortonworks.com/hadoop/oozie/" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/</a><br><a href="http://oozie.apache.org/" target="_blank" rel="external">http://oozie.apache.org/</a><br><a href="http://hortonworks.com/hadoop/oozie/#blog" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/#blog</a><br><a href="http://hortonworks.com/hadoop/oozie/#forums" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/#forums</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/</a><br><a href="https://github.com/yahoo/oozie" target="_blank" rel="external">https://github.com/yahoo/oozie</a><br>书籍：<br>《Apache Oozie: The Workflow Scheduler for Hadoop》<br><a href="http://book.douban.com/subject/26348732/" target="_blank" rel="external">http://book.douban.com/subject/26348732/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="Oozie能做什么(What_Oozie_Does)">Oozie能做什么(What Oozie Does)</h3><p>Oozie是一个Java Web应用，用于Apache Hadoop的任务(jobs)调度。Oozie顺序的合并多个任务(jobs)成为一个]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Oozie" scheme="http://navigating.github.io/tags/Oozie/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据技术百度指数201508]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%99%BE%E5%BA%A6%E6%8C%87%E6%95%B0201508/"/>
    <id>http://navigating.github.io/2015/大数据技术百度指数201508/</id>
    <published>2015-09-16T06:17:00.000Z</published>
    <updated>2015-09-16T09:35:47.738Z</updated>
    <content type="html"><![CDATA[<p>关于大数据技术点的搜索指数，这里只关注一下百度指数的结果。</p>
<ol>
<li>当前最热的依次是：Hadoop、Redis、MongoDB、Spark、Storm。可以看到国内Redis、MongoDB的用户很多，有时候比HBase都热，可见热度之高。</li>
<li>从趋势来看，Spark是最强劲的，Hadoop、Redis表现都很不错。</li>
<li>从搜索热词看，当前主要表现在入门介绍、安装、教程的需求量非常大。对于使用中的问题、优化议题还不多，对于监控更少。</li>
<li>当前搜索热词来源Top5的城市：北京、上海、深圳、广州、南京。北京、珠三角、长三角，同时整体上中部IT发展的相对不错。</li>
<li>用户人群的年龄主要是30～39之间，几乎达到50%，其次是20～29，几乎40%，其他年龄段的人很少。</li>
</ol>
<p>首先是意外的收获，就是发现Redis和MongoDB在国内这么火，热词竟然超过了Hadoop了，也许是两个比较简单易用，不像Hadoop如今已经发展成为了一个大家族了。<br>第二个意外是，发现Cloudera、Hortonworks、CHD、HDP尽然不是指数热词，看来一般印象的大数据技术等于Hadoop真的是偏见啊。<br>第三个意外，发现中部城市IT整体发展的不错，除了上海、南京，还包括：成都、重庆、武汉、长沙、西安、郑州。</p>
<p>趋势研究<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_01.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_02.JPG" alt="这是一张图片"></p>
<p>需求图谱<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_03.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_04.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_05.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_06.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_07.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_08.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_09.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_10.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_11.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_12.JPG" alt="这是一张图片"></p>
<p>人群画像<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_13.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_14.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_15.JPG" alt="这是一张图片"></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>关于大数据技术点的搜索指数，这里只关注一下百度指数的结果。</p>
<ol>
<li>当前最热的依次是：Hadoop、Redis、MongoDB、Spark、Storm。可以看到国内Redis、MongoDB的用户很多，有时候比HBase都热，可见热度之高。</li>
<l]]>
    </summary>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Redis" scheme="http://navigating.github.io/tags/Redis/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="Storm" scheme="http://navigating.github.io/tags/Storm/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201508]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201508/"/>
    <id>http://navigating.github.io/2015/大数据动态之201508/</id>
    <published>2015-09-07T08:13:04.000Z</published>
    <updated>2015-10-16T02:44:00.973Z</updated>
    <content type="html"><![CDATA[<p>Cloudera：<br>Cloudera Navigator路线图<br><a href="http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-governance-cloudera-navigator-roadmap/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-governance-cloudera-navigator-roadmap/</a><br>NoSQL性能测试开放标准套件YCSB加入Cloudera实验室项目中<br><a href="http://blog.cloudera.com/blog/2015/08/ycsb-the-open-standard-for-nosql-benchmarking-joins-cloudera-labs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/ycsb-the-open-standard-for-nosql-benchmarking-joins-cloudera-labs/</a><br>Spark在TripAdvisor的机器学习应用案例<br><a href="http://blog.cloudera.com/blog/2015/08/using-apache-spark-for-massively-parallel-nlp-at-tripadvisor/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/using-apache-spark-for-massively-parallel-nlp-at-tripadvisor/</a><br>CDH支持Mesos<br><a href="http://blog.cloudera.com/blog/2015/08/how-to-run-apache-mesos-on-cdh/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/how-to-run-apache-mesos-on-cdh/</a><br>HBase开始支持HBase-Spark模块<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br>Navigator Encrypt开始支持YARN Container安全<br><a href="http://blog.cloudera.com/blog/2015/08/how-to-secure-yarn-containers-with-cloudera-navigator-encrypt/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/how-to-secure-yarn-containers-with-cloudera-navigator-encrypt/</a><br>基于Kafka和HBase的近实时集成架构案例: Santanders<br><a href="http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/</a> </p>
<p>Hortonworks:<br>Microsoft Azure Gallery开始支持HDP 2.3<br><a href="http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/</a><br>Microsoft Azure支持Spark<br><a href="http://hortonworks.com/blog/microsoft-and-hortonworks-do-spark-in-the-cloud/" target="_blank" rel="external">http://hortonworks.com/blog/microsoft-and-hortonworks-do-spark-in-the-cloud/</a><br>Storm的容错Nimbus架构<br><a href="http://hortonworks.com/blog/fault-tolerant-nimbus-in-apache-storm/" target="_blank" rel="external">http://hortonworks.com/blog/fault-tolerant-nimbus-in-apache-storm/</a> </p>
<p>MapR<br>Spark Streaming with HBase<br><a href="https://www.mapr.com/blog/spark-streaming-hbase" target="_blank" rel="external">https://www.mapr.com/blog/spark-streaming-hbase</a><br>Apache Drill Architecture: The Ultimate Guide<br><a href="https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide" target="_blank" rel="external">https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide</a><br>HBase架构深度剖析<br><a href="https://www.mapr.com/blog/in-depth-look-hbase-architecture" target="_blank" rel="external">https://www.mapr.com/blog/in-depth-look-hbase-architecture</a><br>HBase Schema设计指导<br><a href="https://www.mapr.com/blog/guidelines-hbase-schema-design" target="_blank" rel="external">https://www.mapr.com/blog/guidelines-hbase-schema-design</a><br>如何利用Spark进行机器学习的并行与交互处理<br><a href="https://www.mapr.com/blog/parallel-and-iterative-processing-machine-learning-recommendations-spark" target="_blank" rel="external">https://www.mapr.com/blog/parallel-and-iterative-processing-machine-learning-recommendations-spark</a></p>
<p>Databricks<br>Spark 1.5发布，包含Tungsten，其利用代码生成技术和Cache感知算法，大幅度提升运行时的性能：<br><a href="https://databricks.com/blog/2015/08/18/spark-1-5-preview-now-available-in-databricks.html" target="_blank" rel="external">https://databricks.com/blog/2015/08/18/spark-1-5-preview-now-available-in-databricks.html</a><br><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" target="_blank" rel="external">https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html</a></p>
<p>mongoDB<br>mongoDB 2.x版本发布了2个，3.x发布了3个：<br><a href="http://blog.mongodb.org/post/128063809158/mongodb-306-rc2-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/128063809158/mongodb-306-rc2-is-released</a><br><a href="http://blog.mongodb.org/post/127802855483/mongodb-317-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/127802855483/mongodb-317-is-released</a><br><a href="http://blog.mongodb.org/post/126436298628/mongodb-2611-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/126436298628/mongodb-2611-is-released</a><br><a href="http://blog.mongodb.org/post/126436227873/mongodb-306-rc0-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/126436227873/mongodb-306-rc0-is-released</a><br><a href="http://blog.mongodb.org/post/125850939688/mongodb-2611-rc0-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/125850939688/mongodb-2611-rc0-is-released</a> </p>
<p>Redis</p>
<p>参考：<br>NoSQL大数据分类<br><a href="http://www.nosql-database.org/" target="_blank" rel="external">http://www.nosql-database.org/</a><br>Autodesk基于Mesos的通用事件系统架构<br><a href="http://www.csdn.net/article/2015-08-27/2825550" target="_blank" rel="external">http://www.csdn.net/article/2015-08-27/2825550</a><br>QingCloud推出Spark即服务<br><a href="http://mt.sohu.com/20150826/n419752360.shtml" target="_blank" rel="external">http://mt.sohu.com/20150826/n419752360.shtml</a><br>Spark大数据分析框架的核心部件<br><a href="http://my.oschina.net/u/2306127/blog/489024?p=1" target="_blank" rel="external">http://my.oschina.net/u/2306127/blog/489024?p=1</a><br>Hadoop和大数据：60款顶级开源工具<br><a href="http://os.51cto.com/art/201508/487936.htm" target="_blank" rel="external">http://os.51cto.com/art/201508/487936.htm</a><br>【微信分享】QingCloud周小四：Spark学习简谈<br><a href="http://www.csdn.net/article/2015-08-07/2825404" target="_blank" rel="external">http://www.csdn.net/article/2015-08-07/2825404</a><br>【微信分享】李滔：搜狐基于Spark的新闻和广告推荐实战<br><a href="http://www.csdn.net/article/2015-07-31/2825353" target="_blank" rel="external">http://www.csdn.net/article/2015-07-31/2825353</a><br>【微信分享】王团结：七牛是如何搞定每天500亿条日志的<br><a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br>对七牛云存储日志处理的思考<br><a href="http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/" target="_blank" rel="external">http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/</a><br>STORM在线业务实践-集群空闲CPU飙高问题排查<br><a href="http://daiwa.ninja/index.php/2015/07/18/storm-cpu-overload/" target="_blank" rel="external">http://daiwa.ninja/index.php/2015/07/18/storm-cpu-overload/</a><br>Spark与Flink：对比与分析<br><a href="http://www.csdn.net/article/2015-07-16/2825232" target="_blank" rel="external">http://www.csdn.net/article/2015-07-16/2825232</a><br>一共81个，开源大数据处理工具汇总（上）<br><a href="http://www.36dsj.com/archives/24852" target="_blank" rel="external">http://www.36dsj.com/archives/24852</a><br>一共81个，开源大数据处理工具汇总（下）<br><a href="http://home.hylanda.com/show_26_11558.html" target="_blank" rel="external">http://home.hylanda.com/show_26_11558.html</a></p>
<p>总结：</p>
<pre><code><span class="bullet">1. </span>Cloudera和Hortonworks都开始注重数据管理和数据治理，Cloudera是通过增强Cloudera Navigator来实现，Hortonworks通过引入Informatic组件Fabric来实现。
<span class="bullet">2. </span>Spark 1.5发布；
<span class="bullet">3. </span>HBase、Cassandra是Column Families/Wide Column Store；
<span class="bullet">4. </span>MongoDB是Document Store；
<span class="bullet">5. </span>Redis是Key Value/Tuple Store；
<span class="bullet">6. </span>Neo4J是Graph Databases；
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>Cloudera：<br>Cloudera Navigator路线图<br><a href="http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-gov]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[SparkOnHBase(Cloudera)]]></title>
    <link href="http://navigating.github.io/2015/SparkOnHBase-Cloudera/"/>
    <id>http://navigating.github.io/2015/SparkOnHBase-Cloudera/</id>
    <published>2015-08-18T05:35:51.000Z</published>
    <updated>2015-08-18T05:46:25.533Z</updated>
    <content type="html"><![CDATA[<p>2014年2月4日，Cloudera宣布CDH支持Spark，在CDH 4.4中引入Spark 0.9。<br><a href="http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/" target="_blank" rel="external">http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/</a><br>在引入的时候强调了三点：</p>
<pre><code><span class="bullet">1. </span>Machine Learning
<span class="bullet">2. </span>Spark Streaming
<span class="bullet">3. </span>Faster Batch
</code></pre><p>2014年7月，在github上创建了Apache HBase与Spark的集成项目SparkOnHBase<br><a href="http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/" target="_blank" rel="external">http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/</a><br><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="external">https://github.com/cloudera-labs/SparkOnHBase</a><br>当前SparkOnHBase主要集中在这几个方面的功能改进：</p>
<pre><code>1. 在MR的map或者reduce阶段对HBase的全量访问(Full Access)；
2. 支持bulk <span class="operator"><span class="keyword">load</span>；
<span class="number">3.</span> 支持<span class="keyword">get</span>, put, <span class="keyword">delete</span>等bulk操作(bulk operation)；
<span class="number">4.</span> 支持成为<span class="keyword">SQL</span> <span class="keyword">engines</span>。</span>
</code></pre><p>2015年8月SparkOnHBase项目有了里程碑似的进展，被提交到HBase的主干(trunk)上，模块名为HBase-Spark Module，HBASE-13992 。<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br><a href="https://issues.apache.org/jira/browse/HBASE-13992" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-13992</a><br>HBase-Spark module相比于SparkOnHBase在架构上没有什么变化：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Cloudera_Spark_2015_01.png" alt="这是一张图片"><br>在具体实现上当前有三点改进：</p>
<pre><code><span class="bullet">1. </span>使用了全新的HBase 1.0+的API；
<span class="bullet">2. </span>从RDD和DStream functions操作HBase的直接支持；
<span class="bullet">3. </span>简化 foreach 和 map functions；
</code></pre><p>计划工作有两项：</p>
<pre><code><span class="bullet">1. </span>Spark-HBase Module支持bulkload；
<span class="bullet">2. </span>Spark-HBase Module支持Spark DataFrame DataSource；
</code></pre><p><a href="https://issues.apache.org/jira/browse/HBASE-14150" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-14150</a><br><a href="https://issues.apache.org/jira/browse/HBASE-14181" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-14181</a> </p>
<p>实际上集成Spark作为计算引擎的项目还有Hive和Pig：<br><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/spark.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/spark.html</a><br><a href="http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/</a><br><a href="http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/" target="_blank" rel="external">http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/</a> </p>
<p>参考：<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="external">https://github.com/cloudera-labs/SparkOnHBase</a><br><a href="http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>2014年2月4日，Cloudera宣布CDH支持Spark，在CDH 4.4中引入Spark 0.9。<br><a href="http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/" targ]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Cloudera" scheme="http://navigating.github.io/tags/Cloudera/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《Hadoop生态技术在阿里全网商品搜索实战》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8AHadoop%E7%94%9F%E6%80%81%E6%8A%80%E6%9C%AF%E5%9C%A8%E9%98%BF%E9%87%8C%E5%85%A8%E7%BD%91%E5%95%86%E5%93%81%E6%90%9C%E7%B4%A2%E5%AE%9E%E6%88%98%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《Hadoop生态技术在阿里全网商品搜索实战》/</id>
    <published>2015-08-17T04:41:00.000Z</published>
    <updated>2015-08-18T08:55:01.529Z</updated>
    <content type="html"><![CDATA[<p>资料参见文档：<a href="http://wenku.it168.com/d_001428550.shtml" target="_blank" rel="external">http://wenku.it168.com/d_001428550.shtml</a><br>版本:</p>
<pre><code><span class="bullet">1. </span>Hadoop: 基于 Hadoop 2.2 的阿里定制版
<span class="bullet">2. </span>HBase: 基于 HBase 0.94 的阿里定制版
</code></pre><p>部署方式：</p>
<pre><code><span class="bullet">1. </span>服务总数近1000台，分2个集群；
<span class="bullet">2. </span>Hadoop/HBase共同部署；
</code></pre><p>分析：服务器数量可能是2014年初的数据；HBase部署方式可能是RS和DN部署在同一个节点上。<br>服务器配置：</p>
<pre><code><span class="bullet">1. </span>CPU：24/32 Cores
<span class="bullet">2. </span>Memory：48G/96G
<span class="bullet">3. </span>Disk：12 <span class="bullet">* 1T SATA Disk 或者 12 *</span> 2T SATA Disk
</code></pre><p>分析：服务器配置计算能力比较强，内存和磁盘配置都不是很高。<br>大数据组件：</p>
<pre><code><span class="bullet">1. </span>HDFS + YARN
<span class="bullet">2. </span>HBase
<span class="bullet">3. </span>MR
<span class="bullet">4. </span>iStream
<span class="bullet">5. </span>Spark
<span class="bullet">6. </span>HQueue
<span class="bullet">7. </span>Phoenix
<span class="bullet">8. </span>OpenTSDB
<span class="bullet">9. </span>Zookeeper
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_01.JPG" alt="这是一张图片"><br>分析：</p>
<pre><code>* 对于基于HBase的HQueue是一个创新，当前没有看到更多的资料，无法和Kafka对比。(在性能和TPC上可能Kafka更强大，但通过对HBase的复用做出Queue，很赞。)
* iStream是一个Steaming <span class="function_start"><span class="keyword">on</span></span> YARN的产品，从架构上看很类似storm的设计理念。
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_02.JPG" alt="这是一张图片"><br>HBase<br>HBase应用</p>
<pre><code><span class="number">1</span>. <span class="function"><span class="title">Phoenix</span><span class="params">(SQL on HBase)</span></span>
<span class="number">2</span>. <span class="function"><span class="title">OpenTSDB</span><span class="params">(Metrics on HBase)</span></span>
<span class="number">3</span>. <span class="function"><span class="title">HQueue</span><span class="params">(Queue on HBase)</span></span>
</code></pre><p>分析：</p>
<pre><code><span class="bullet">* </span>在HBase集群上运行了Phoenix、OpenTSDB、HQueue三种应用，因此HBase具有作为一种数据存储的基础设施的能力。
</code></pre><p>HBase网页库存储方案</p>
<pre><code>1. 版本从0<span class="class">.25</span>、0<span class="class">.26</span>、0<span class="class">.90</span>、0<span class="class">.92</span>、0<span class="class">.94</span>、0<span class="class">.98</span>逐步升级的。
2. <span class="tag">HBase</span>集群规模从30多台持续升级到300多台。
3. <span class="tag">HBase</span> <span class="tag">Region</span>个数从 1<span class="tag">K</span> 增长到 20<span class="tag">K</span>。
4. 网页数量从 十亿 增长到 百亿。
</code></pre><p>存储业务数据的CF如下：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_05.JPG" alt="这是一张图片"><br>在HBase/Hadoop的I/O上的优化如下：</p>
<pre><code><span class="bullet">1. </span>Compression：Snappy/Gzip
<span class="bullet">2. </span>Block Encoding：Diff
<span class="bullet">3. </span>Block Size：64KB - 1MB
<span class="bullet">4. </span>Block Cache：InMemory
<span class="bullet">5. </span>Bloom Filter：ROW
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_06.JPG" alt="这是一张图片"><br>HBase Coprocessor应用<br>在网页库中使用了三种Coprocessor：</p>
<pre><code><span class="bullet">1. </span>Trace Coprocessor
<span class="bullet">2. </span>Clone Coprocessor
<span class="bullet">3. </span>Incremental Coprocessor
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_07.JPG" alt="这是一张图片"><br>分析：</p>
<pre><code><span class="keyword">*</span> 如果HBase集群就是两个集群中的一个，那么裸存储容量最大为：12 <span class="keyword">*</span> 2T <span class="keyword">*</span> 300 = 7200T = 7.2P，如果考虑到压缩、复制因子、数据冗余、容量冗余，可以存储有效数据约为：8P 数据。 
<span class="keyword">*</span> 平均每台服务器运行的Region个数：20K/300 = 67 个，这个数字比较符合HBase官方推荐的值。
<span class="keyword">*</span> Compression方法用了snappy和gzip两种。CF访问频繁，使用snappy，速度快；Raw CF访问较少，使用gzip，压缩比高。
<span class="keyword">*</span> Block Encoding使用Diff，0.98后改用PrefixTree；
<span class="keyword">*</span> Block Size的大小为 64KB - 1MB 
</code></pre><p>实时处理架构<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_08.JPG" alt="这是一张图片"><br>分析</p>
<pre><code><span class="subst">*</span> Metrics实时采集的流程大约是：HBase <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iStream <span class="subst">-&gt; </span>OpenTSDB <span class="keyword">on</span> HBase
<span class="subst">*</span> 流处理的全流程：HBase <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iStream <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iSearch/iStream
<span class="subst">*</span> 参见前文分析，猜测iStream是一个类似Storm的YARN框架。
</code></pre><p>关于阿里搜索自研的iStream的架构与文档参加如下：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_09.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_10.JPG" alt="这是一张图片"></p>
<p><a href="http://www.infoq.com/cn/news/2014/09/hadoop-alibaba-yarn" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/09/hadoop-alibaba-yarn</a><br><a href="http://club.alibabatech.org/resource_detail.htm?topicId=140" target="_blank" rel="external">http://club.alibabatech.org/resource_detail.htm?topicId=140</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>资料参见文档：<a href="http://wenku.it168.com/d_001428550.shtml" target="_blank" rel="external">http://wenku.it168.com/d_001428550.shtml</a><br>]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="HQueue" scheme="http://navigating.github.io/tags/HQueue/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="iStream" scheme="http://navigating.github.io/tags/iStream/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop发行版(2015第二季)]]></title>
    <link href="http://navigating.github.io/2015/Hadoop%E5%8F%91%E8%A1%8C%E7%89%88(2015%E7%AC%AC%E4%BA%8C%E5%AD%A3)/"/>
    <id>http://navigating.github.io/2015/Hadoop发行版(2015第二季)/</id>
    <published>2015-08-11T14:40:02.000Z</published>
    <updated>2015-08-11T14:56:06.872Z</updated>
    <content type="html"><![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" target="_blank" rel="external">http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support</a><br>其中在最有名为人所知的三家：<br>1.Cloudera<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_1.JPG" alt="这是一张图片"><br>2.Hortonwork<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_2.JPG" alt="这是一张图片"><br>3.MapR<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_3.JPG" alt="这是一张图片"><br>这三个厂商之中，MapR最为封闭；Hortonworks最为开放，产品线全开源，在线文档比较丰富。国内使用Cloudera CDH和Hortonworks的应该是最多的。<br>国内市场当前有两家也非常有竞争力，一家是Huawei，一家是星环科技。<br>4.Huawei FusionInsight<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_7.JPG" alt="这是一张图片"><br>5.星环科技TDH，TDH对Spark的支持据说非常不错的，有良好的性能表现。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_6.JPG" alt="这是一张图片"><br>准实时计算框架/即席查询<br>1.CDH的框架有：Impala + Spark；<br>2.HDP的框架有：Tez + Spark；<br>3.MapR的框架有：Drill + Tez + Spark。<br>关于Spark：<br>2014年大数据最热门的技术路线就是算是Spark了，而且得力于Spark不遗余力的推广和快速成长。Cloudera是最早支持Spark，也是最激进的。下图即是Spark在Cloudera产品线中的定位：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_4.JPG" alt="这是一张图片"><br>实际上基于Hadoop的快速计算框架的发展才刚刚开始，社区中已经有如下几种：<br>1.Spark/Shark<br>2.Hortonworks Tez/Stinger<br>3.Cloudera Impala<br>4.Apache Drill<br>5.Apache Flink<br>6.Apache Nifi<br>7.Facebook Presto</p>
<p>SQL on Hadoop<br>SQL on Hadoop的发展主要是传统的SQL过于强大，人才库非常庞大，从Hadoop出现的第一天就在SQL发力。当前技术路线上更是百花齐放，这里从开源和商业产品来说。<br>Open Source</p>
<pre><code><span class="bullet">1. </span>Apache Hive(Hive on MR)
<span class="bullet">2. </span>Hortonworks Tez/Stinger(Hive on Tez)
<span class="bullet">3. </span>Cloudera Impala
<span class="bullet">4. </span>Shark
<span class="bullet">5. </span>Spark SQL
<span class="bullet">6. </span>Apache Drill - MapR
<span class="bullet">7. </span>Facebook Presto
<span class="bullet">8. </span>Apache Phoenix(on HBase) - Saleforce
<span class="bullet">9. </span>Apache Kylin
<span class="bullet">10. </span>Apache Tajo - (Database Lab, Korea University)
<span class="bullet">11. </span>Cascading Lingual - (Cascading, Optiq)
<span class="bullet">12. </span>Dato (GraphLab) - Dato
</code></pre><p>Commercial</p>
<pre><code><span class="bullet">1. </span>EMC HAWQ
<span class="bullet">2. </span>IBM BigSQL
<span class="bullet">3. </span>TERADATA SQL-H
<span class="bullet">4. </span>Hadapt/HadoopDB
<span class="bullet">5. </span>Transwarp Inceptor
</code></pre><p>在开源领域里面，当前比受追捧的主要是：Hive、Impala、Spark、Phoenix。</p>
<p>参考：<br>SQL on Hadoop开源项目总结<br><a href="http://segmentfault.com/a/1190000002799235" target="_blank" rel="external">http://segmentfault.com/a/1190000002799235</a><br>如何选择满足需求的SQL on Hadoop系统<br><a href="http://www.searchbi.com.cn/showcontent_89816.htm" target="_blank" rel="external">http://www.searchbi.com.cn/showcontent_89816.htm</a><br>2015Hadoop技术峰会演讲速记3： 基于Transwarp Stream和Discover的实时大数据人流密度估计<br><a href="http://www.transwarp.cn/news/detail?id=70" target="_blank" rel="external">http://www.transwarp.cn/news/detail?id=70</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="SQL on Hadoop" scheme="http://navigating.github.io/tags/SQL-on-Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《七牛是如何搞定每天500亿条日志的》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8A%E4%B8%83%E7%89%9B%E6%98%AF%E5%A6%82%E4%BD%95%E6%90%9E%E5%AE%9A%E6%AF%8F%E5%A4%A9500%E4%BA%BF%E6%9D%A1%E6%97%A5%E5%BF%97%E7%9A%84%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《七牛是如何搞定每天500亿条日志的》/</id>
    <published>2015-08-10T06:48:00.000Z</published>
    <updated>2015-08-10T14:12:54.949Z</updated>
    <content type="html"><![CDATA[<p>七牛是如何搞定每天500亿条日志的 <a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/qiniu_01.jpg" alt=""><br>日志处理的大致分为三步：</p>
<pre><code><span class="bullet">1. </span>日志采集，主要是通过Agent和Flume；
<span class="bullet">2. </span>日志流转，主要是通过Kafka；
<span class="bullet">3. </span>日志计算，主要是通过Spark Streaming作为计算引擎；
</code></pre><p>大致的处理流程：</p>
<pre><code><span class="number">1.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>HDFS <span class="subst">-&gt; </span>mongoDB
<span class="number">2.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>Spark <span class="subst">-&gt; </span>mongoDB
<span class="number">3.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>Spark <span class="subst">-&gt; </span>opentsdb 
</code></pre><p>流程3只是见于图上，文字上没有任何提到。<br>在日志采集中，通过Agent将业务应用和日志采集进行了分离，采取了Agent主动来拉的模式。专门强调了Agent 的设计需求：<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">每台机器上会有一个Agent去同步这些日志，这是个典型的队列模型，业务进程在不断的push，Agent在不停的pop。Agent需要有记忆功能，用来保存同步的位置(<span class="command">offset</span>)，这样才尽可能保证数据准确性，但不可能做到完全准确。由于发送数据和保存<span class="command">offset</span>是两个动作，不具有事务性，不可避免的会出现数据不一致性情况，通常是发送成功后保存<span class="command">offset</span>，那么在Agent异常退出或机器断电时可能会造成多余的数据。</span><br><span class="line">在这里，Agent需要足够轻，这主要体现在运维和逻辑两个方面。Agent在每台机器上都会部署，运维成本、接入成本是需要考虑的。Agent不应该有解析日志、过滤、统计等动作，这些逻辑应该给数据消费者。倘若Agent有较多的逻辑，那它是不可完成的，不可避免的经常会有升级变更动作。</span><br></pre></td></tr></table></figure></p>
<p>为什么Agent没有直接将日志发送给Kafka，而是通过Flume来做：<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">具体架构上，Agent并没把数据直接发送到Kafka，在Kafka前面有层由Flume构成的<span class="keyword">forward</span>。这样做有两个原因：</span><br><span class="line"><span class="number">1</span>. Kafka的API对非JVM系的语言支持很不友好，<span class="keyword">forward</span>对外提供更加通用的http接口。</span><br><span class="line"><span class="number">2</span>. <span class="keyword">forward</span>层可以做路由、Kafka topic和Kafka partition key等逻辑，进一步减少Agent端的逻辑。</span><br></pre></td></tr></table></figure></p>
<p>Kafka使用建议<br>1.Topic划分。尽量通过划分Topic分离不同类型的数据；<br>2.Kafka partition数目直接关系整体的吞吐量。3个Partition能够跑满一块磁盘的IO。<br>3.Partition key设计。partition key选择不当，可能会造成数据倾斜。在对数据有顺序性要求才需使用partition key。Kafka的producer sdk在没指定partition key时，在一定时间内只会往一个partition写数据，这种情况下当producer数少于partition数也会造成数据倾斜，可以提高producer数目来解决这个问题。<br>实时计算Spark Streaming<br>1.当前Spark只用作统计，没有进行迭代计算(DAG)。场景比较简单。<br>2.Spark Streaming从Kafka中读数据，统计完结果如mongoDB。可以理解是Spark Streaming + mongoDB的应用。<br>3.Spark Streaming对存储计算结果的数据库tps要求较高。比如有10万个域名需要统计流量，batch interval为10s，每个域名有4个相关统计项，算下来平均是4万 tps，考虑到峰值可能更高，固态硬盘上的mongo也只能抗1万tps，后续我们会考虑用redis来抗这么高的tps。难道Redis能够支持很高的TPS？<br>4.有状态的Task的挑战：有外部状态的task逻辑上不可重入的，当开启speculation参数时候，可能会造成计算的结果不准确。说个简单的例子。这个任务，如果被重做了，会造成落入mongo的结果比实际多。有状态的对象生命周期不好管理，这种对象不可能做到每个task都去new一个。我们的策略是一个JVM内一个对象，同时在代码层面做好并发控制。<br>七牛数据平台规模<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">线上的规模：Flume ＋ Kafka ＋ Spark8台高配机器，日均500亿条数据，峰值80万tps。</span><br></pre></td></tr></table></figure></p>
<p>因此，<br>1.如果是Flume/Kafka/Spark共享同一个物理集群，硬件压力如何？<br>2.如果每条日志 0.1K，那么每天总数据量 50G <em> 0.1K = 5T，每个节点每秒 5T/24</em>3600/8 = 7.23M。 </p>
<p>参考：<br>【微信分享】王团结：七牛是如何搞定每天500亿条日志的<br><a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br>对七牛云存储日志处理的思考<br><a href="http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/" target="_blank" rel="external">http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>七牛是如何搞定每天500亿条日志的 <a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/201]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Flume" scheme="http://navigating.github.io/tags/Flume/"/>
    
      <category term="Kafka" scheme="http://navigating.github.io/tags/Kafka/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《腾讯在Spark上的应用与实践优化》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8A%E8%85%BE%E8%AE%AF%E5%9C%A8Spark%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%E4%BC%98%E5%8C%96%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《腾讯在Spark上的应用与实践优化》/</id>
    <published>2015-08-07T08:28:42.000Z</published>
    <updated>2015-08-07T08:37:53.504Z</updated>
    <content type="html"><![CDATA[<p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.net/detail/happytofly/8637461</a></p>
<p>TDW: Tencent Distributed Data Warehouse，腾讯分布式数据仓库；<br>GAIA：腾讯自研的基于YARN定制化和优化的资源管理系统；<br>Lhoste：腾讯自研的作业的工作流调度系统，类似于Oozie；<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_1.JPG" alt=""></p>
<p>TDW集群规模：</p>
<pre><code><span class="bullet">1. </span>Gaia集群节点数：8000+；
<span class="bullet">2. </span>HDFS的存储空间：150PB+；
<span class="bullet">3. </span>每天新增数据：1PB+；
<span class="bullet">4. </span>每天任务数：1M+；
<span class="bullet">5. </span>每天计算量：10PB+；
</code></pre><p>Spark集群：</p>
<pre><code><span class="bullet">1. </span>Spark部署在Gaia之上，即是Spark on YARN模式，每个节点是 24 cores 和 60G 内存；
<span class="bullet">2. </span>底层存储包括：HDFS、HBase、Hive、MySQL；
<span class="bullet">3. </span>作业类型，包括：ETL、SparkSQL、Machine Learning、Graph Compute、Streaming；
<span class="bullet">4. </span>每天任务数，10K+；
<span class="bullet">5. </span>腾讯从2013年开始引入Spark 0.6，已经使用2年了；
</code></pre><p>Spark的典型应用：</p>
<pre><code><span class="bullet">1. </span>预测用户的广告点击概率；
<span class="bullet">2. </span>计算两个好友间的共同好友数；
<span class="bullet">3. </span>用于ETL的SparkSQL和DAG任务；
</code></pre><p>Case 1: 预测用户的广告点击概率<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_4.JPG" alt=""></p>
<pre><code><span class="number">1</span>. 数据是通过<span class="function"><span class="title">DCT</span><span class="params">(Data Collect Tool)</span></span>推送到HDFS上，然后Spark直接将HDFS数据导入到 RDD&amp;Cache；
<span class="number">2</span>. <span class="number">60</span>次迭代计算的时间为<span class="number">10</span>～<span class="number">15</span>分钟，即每次迭代<span class="number">10</span>～<span class="number">15</span>秒；
</code></pre><p>Case 2: 计算两个好友间的共同好友数</p>
<pre><code>1. 根据shuffle数量来确定partition数量；
2. 尽量使用sort-based shuffle，减少reduce的内存使用；
3. 当连接超时后选择重试来减少executor丢失的概率；
4. 避免executor被YARN给<span class="operator"><span class="keyword">kill</span>掉，设置 spark.yarn.executor.memoryoverhead
<span class="number">5.</span> 执行语句 <span class="keyword">INSERT</span> <span class="keyword">TABLE</span> test_result <span class="keyword">SELECT</span> t3.d, <span class="keyword">COUNT</span>(*) FROＭ( <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> a, b <span class="keyword">FROM</span> join_1 ) t1 <span class="keyword">JOIN</span> （<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> b, c <span class="keyword">FROM</span> join_2 ) t2 <span class="keyword">ON</span> (t1.a = t2.c) <span class="keyword">JOIN</span> (<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c, d <span class="keyword">FROM</span> c, d <span class="keyword">FROM</span> join_3 ) t3 <span class="keyword">ON</span> (t2.b = t3.d) <span class="keyword">GROUP</span> <span class="keyword">BY</span> t3.d 使用Hive需要<span class="number">30</span>分钟，使用SparkSQL需要<span class="number">5</span>分钟；
<span class="number">6.</span> 当有小表时使用broadcase <span class="keyword">join</span>代替Common <span class="keyword">join</span>；
<span class="number">7.</span> 尽量使用ReduceByKey代替GroupByKey；
<span class="number">8.</span> 设置spark.serializer = org.apache.spark.serializer.KryoSerializer；
<span class="number">9.</span> 使用YARN时，设置spark.shuffle.service.enabled = <span class="literal">true</span>；
<span class="number">10.</span> 在早期版本中Spark通过启动参数固定executor的数量，当前支持动态资源扩缩容特性

    * spark.dynamicAllocation.enabled = <span class="literal">true</span>
    * spark.dynamicAllocation.executorIdleTimeout = <span class="number">120</span>
    * spark.dynamicAllocation.schedulerBacklogTimeout = <span class="number">10</span>
    * spark.dynamicAllocation.minExecutors/maxExecutors

<span class="number">11.</span> 当申请固定的executors时且task数大于executor数时，存在着资源的空闲状态。</span>
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_5.JPG" alt=""><br>&lt;完&gt;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
</feed>