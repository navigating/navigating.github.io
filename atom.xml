<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[On The Open Way]]></title>
  <subtitle><![CDATA[自信人生二百年，会当水击三千里！]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://navigating.github.io//"/>
  <updated>2015-11-10T06:18:28.873Z</updated>
  <id>http://navigating.github.io//</id>
  
  <author>
    <name><![CDATA[Steven Xu]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[JVM监控与调优]]></title>
    <link href="http://navigating.github.io/2015/JVM%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98/"/>
    <id>http://navigating.github.io/2015/JVM监控与调优/</id>
    <published>2015-11-06T07:53:46.000Z</published>
    <updated>2015-11-10T06:18:28.873Z</updated>
    <content type="html"><![CDATA[<h2 id="题外话">题外话</h2><p>本文当前的范围是Java 8之前的虚拟机，因为Java 8之后虚拟机的架构有比较大的调整，存储类的元数据信息的永久代被删除了，而是放在虚拟机的元空间。<br>JDK 7及之前的版本中永久代有如下的特点：</p>
<ol>
<li>永久代和堆在内存分配上是相连的；</li>
<li>永久代的垃圾回收和老年代的垃圾回收是绑定的，一旦其中一个区域被占满，这两个区都要进行垃圾回收；</li>
<li>永久代一段连续的内存空间，在32位机器默认的永久代的大小为64M，64位的机器则为85M；当然，在JVM启动之前可以通过设置-XX:MaxPermSize的值来控制永久代的大小；</li>
</ol>
<h2 id="Java虚拟机">Java虚拟机</h2><p>Java虚拟机的架构如下图，其中和性能调优相关的组件有三个：Heap，JIT compiler 和 Garbage Collector。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_001.PNG" alt="这是一张图片"></p>
<p>针对内存堆heap中创建的Java对象，采用的是垃圾回收机制进行处理。垃圾回收在Oracle官网叫 Automatic Garbage Collection，其目的寻找确定堆内存中哪些对象在使用，哪些不在使用，并且删除销毁那些不在使用的对象。<br>垃圾回收分为三步：</p>
<ol>
<li>标记，Marking<br>这个阶段识别出哪些对象正在使用，哪些对象不再使用，不再使用的对象标记为可回收的对象，在使用的对象标记为不可回收。在标记阶段需要对所有的对象进行全扫描来做决策。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_003.PNG" alt="这是一张图片"></li>
<li>清除，Normal Deletion<br>这个阶段移除那些没有被引用的Java对象，并释放内存空间。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_004.PNG" alt="这是一张图片"></li>
<li>压缩，Deletion with Compacting<br>为了更好的性能，需要对不可回收依然使用的对象进行压缩，就是把这一类对象迁移在一起，使得新内存的分配变得的更容易和更快。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_005.PNG" alt="这是一张图片"></li>
</ol>
<h2 id="分代垃圾回收(Generational_Garbage_Collection)">分代垃圾回收(Generational Garbage Collection)</h2><p>由于标记和压缩堆内存中所有的对象是十分低效的，并且随着越来越多的对象被分配，垃圾回收的时间也会变得越来越长，因此引入引入了如下的回收策略：按着对象存活的时间窗口采用不同的垃圾回收机制，即是 Minor collections 和 Major collections。如下图：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_006.gif" alt="这是一张图片"><br>其中Y轴标识已分配的字节数，X轴标识随着时间已经被分配内存并且处于使用状态的字节数的情况。从这张图可以看出，只有很少的对象能够长时间需要保留下来，大部分对象只有很短的生命周期。</p>
<p>Java Hotspot Heap结构：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/JVM_Arch_002.PNG" alt="这是一张图片"><br>Hotspot内存由三部分组成：</p>
<ol>
<li>持久代，Permanent Generation：存储类和对象元数据的数据的地方，包括类的层级信息，方法数据和方法信息（如字节码，栈和变量大小），运行时常量池，已确定的符号引用和虚方法表。</li>
<li>年轻代，Young Generation：分为三个区，一个Eden区，两个Survivor区。新生代主要是存放新生成的Java对象，新生代的垃圾回收称为 minor garbage collection。</li>
<li>年老代，Old Genration： 即是Tenured区，存放在年轻代经过多次垃圾回收依然存活的对象的区域，年老代的垃圾回收称为 major garbage collection。</li>
</ol>
<h2 id="垃圾回收触发时机">垃圾回收触发时机</h2><p>Minor Collection<br>    在Eden空间已满，新对象申请空间失败时，就会触发Minor Collection，对Eden区域进行GC，清除非存活对象，并把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。这种方式的GC是对年轻代的Eden区进行，不会影响到年老代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。因而，一般在这里需要使用速度快、效率高的算法，使Eden去能尽快空闲出来。</p>
<p>Major Collection(Full GC)<br>    对整个堆进行整理，包括Young、Tenured和Perm。Major Collection因为需要对整个对进行回收，所以比Minor Collection要慢，因此应该尽可能减少Full GC的次数。在对JVM调优的过程中，很大一部分工作就是对于Major Collection的调节。有如下原因可能导致Full GC：</p>
<ol>
<li>年老代（Tenured）被写满;</li>
<li>持久代（Perm）被写满;</li>
<li>System.gc()被显示调用;</li>
<li>上一次GC之后Heap的各域分配策略动态变化;</li>
</ol>
<h2 id="Garbage_Collector">Garbage Collector</h2><p>经过发展，Java已有如下的垃圾回收器：</p>
<ol>
<li><p>Serial收集器/SerialOld收集器<br>Serial收集器/Serial Old收集器，是单线程的，使用“复制”算法。当它工作时，必须暂停其它所有工作线程。特点：简单而高效。对于运行在Client模式下的虚拟机来说是一个很好的选择。<br>串行收集器并不是只能使用一个CPU进行收集，而是当JVM需要进行垃圾回收的时候，需要中断所有的用户线程，知道它回收结束为止，因此又号称“Stop The World” 的垃圾回收器。<br>Serial收集器默认新旧生代的回收器搭配为Serial+ SerialOld</p>
</li>
<li><p>ParNew收集器<br>ParNew收集器其实就是多线程版本的Serial收集器，同样有<br>Stop The World的问题，他是多CPU模式下的首选回收器（该回收器在单CPU的环境下回收效率远远低于Serial收集器，所以一定要注意场景哦），也是Server模式下默认的新生代收集器。除了Serial收集器外，目前只有它能与CMS收集器配合工作。</p>
</li>
<li><p>ParallelScavenge/ParallelOld收集器<br>ParallelScavenge又被称为是吞吐量优先的收集器，也是使用“复制”算法的、并行的多线程收集器。这些都和ParNew收集器一样。但它关注的是吞吐量（CPU用于运行用户代码的时间与CPU总消耗时间的比值），而其它收集器（Serial/Serial Old、ParNew、CMS）关注的是垃圾收集时用户线程的停顿时间。<br>Parallel Old收集器是Parallel Scavenge收集器的老年代版本。</p>
</li>
<li><p>CMS<br>CMS(Concurrent Mark Sweep）)又称响应时间优先(最短回收停顿)的回收器，使用并发模式回收垃圾，使用”标记-清除“算法，CMS对CPU是非常敏感的，它的回收线程数=（CPU+3）/4，因此当CPU是2核的实惠，回收线程将占用的CPU资源的50%，而当CPU核心数为4时仅占用25%。<br>CMS收集器分4个步骤进行垃圾收集工作：<br>a. 初始标记(CMS initial mark)<br>b. 并发标记(CMS concurrent mark)<br>c. 重新标记(CMS remark)<br>d. 并发清除(CMS concurrent sweep)</p>
</li>
<li><p>GarbageFirst(G1)<br>G1（Garbage First）收集器，基于“标记-整理”算法，可以非常精确地控制停顿。其实这是一个新的垃圾回收器，既可以回收新生代也可以回收旧生代，SunHotSpot 1.6u14以上EarlyAccess版本加入了这个回收器</p>
</li>
</ol>
<h2 id="监控命令">监控命令</h2><p>JDK自带的性能调优监控工具，包括：VisualVM、jConsole、jps、jstack、jmap、jhat、jstat、hprof等。<br>jstack 主要用于查看Java进程内的线程堆栈信息。<br>例子：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># jps</span></span><br><span class="line"><span class="number">6923</span> HelloWorld</span><br><span class="line"><span class="number">14009</span> Jps</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># jstack -m 6923</span></span><br></pre></td></tr></table></figure></p>
<p>jmap 主要用于查看堆内存的使用情况。<br>例子：<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"># jmap -heap 6923 </span><br><span class="line">Attaching to process ID 6923, please wait...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.</span><br><span class="line">JVM version is 24.65-b04</span><br><span class="line"></span><br><span class="line">using parallel threads in the new generation.</span><br><span class="line">using thread-local object allocation.</span><br><span class="line">Concurrent Mark-Sweep GC</span><br><span class="line"></span><br><span class="line">Heap Configuration:</span><br><span class="line">   MinHeapFreeRatio = 40</span><br><span class="line">   MaxHeapFreeRatio = 70</span><br><span class="line">   MaxHeapSize      = <span class="number">25769803776</span> (<span class="number">24576.0</span>MB)</span><br><span class="line">   NewSize          = <span class="number">2147483648</span> (2048.0MB)</span><br><span class="line">   MaxNewSize       = <span class="number">2147483648</span> (2048.0MB)</span><br><span class="line">   OldSize          = <span class="number">5439488</span> (5.1875MB)</span><br><span class="line">   NewRatio         = 2</span><br><span class="line">   SurvivorRatio    = 8</span><br><span class="line">   PermSize         = <span class="number">21757952</span> (20.75MB)</span><br><span class="line">   MaxPermSize      = <span class="number">85983232</span> (82.0MB)</span><br><span class="line">   G1HeapRegionSize = 0 (0.0MB)</span><br><span class="line"></span><br><span class="line">Heap Usage:</span><br><span class="line">New Generation (Eden + 1 Survivor Space):</span><br><span class="line">   capacity = <span class="number">1932787712</span> (1843.25MB)</span><br><span class="line">   used     = <span class="number">1216988784</span> (<span class="number">1160.61094</span><span class="number">66552734</span>MB)</span><br><span class="line">   free     = <span class="number">715798928</span> (<span class="number">682.63905</span><span class="number">33447266</span>MB)</span><br><span class="line">   <span class="number">62.96546570</span>759655% used</span><br><span class="line">Eden Space:</span><br><span class="line">   capacity = <span class="number">1718091776</span> (1638.5MB)</span><br><span class="line">   used     = <span class="number">1210401768</span> (<span class="number">1154.32907</span><span class="number">86743164</span>MB)</span><br><span class="line">   free     = <span class="number">507690008</span> (<span class="number">484.1709213</span>256836MB)</span><br><span class="line">   <span class="number">70.45035573</span>233545% used</span><br><span class="line">From Space:</span><br><span class="line">   capacity = <span class="number">214695936</span> (204.75MB)</span><br><span class="line">   used     = <span class="number">6587016</span> (<span class="number">6.28186798</span><span class="number">0957031</span>MB)</span><br><span class="line">   free     = <span class="number">208108920</span> (<span class="number">198.4681320190</span>4297MB)</span><br><span class="line">   <span class="number">3.06806738</span><span class="number">9966804</span>% used</span><br><span class="line">To Space:</span><br><span class="line">   capacity = <span class="number">214695936</span> (204.75MB)</span><br><span class="line">   used     = 0 (0.0MB)</span><br><span class="line">   free     = <span class="number">214695936</span> (204.75MB)</span><br><span class="line">   0.0% used</span><br><span class="line">concurrent mark-sweep generation:</span><br><span class="line">   capacity = <span class="number">23622320128</span> (<span class="number">22528.0</span>MB)</span><br><span class="line">   used     = <span class="number">9615124536</span> (<span class="number">9169.69731</span><span class="number">9030762</span>MB)</span><br><span class="line">   free     = <span class="number">14007195592</span> (<span class="number">13358.30268</span><span class="number">0969238</span>MB)</span><br><span class="line">   <span class="number">40.70355699</span><span class="number">1436265</span>% used</span><br><span class="line">Perm Generation:</span><br><span class="line">   capacity = <span class="number">64733184</span> (<span class="number">61.734375</span>MB)</span><br><span class="line">   used     = <span class="number">39187912</span> (<span class="number">37.372505187</span>98828MB)</span><br><span class="line">   free     = <span class="number">25545272</span> (<span class="number">24.36186981</span>201172MB)</span><br><span class="line">   <span class="number">60.53759382</span><span class="number">5139204</span>% used</span><br><span class="line"></span><br><span class="line">11258 interned Strings occupying <span class="number">1021928</span> bytes.</span><br></pre></td></tr></table></figure></p>
<p>jstat 主要用于查看Java进程的统计信息。</p>
<p>例子1：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># <span class="tag">jstat</span> <span class="tag">-gc</span> 6923 2000 200000</span><br><span class="line"> <span class="tag">S0C</span>    <span class="tag">S1C</span>    <span class="tag">S0U</span>    <span class="tag">S1U</span>      <span class="tag">EC</span>       <span class="tag">EU</span>        <span class="tag">OC</span>         <span class="tag">OU</span>       <span class="tag">PC</span>     <span class="tag">PU</span>    <span class="tag">YGC</span>     <span class="tag">YGCT</span>    <span class="tag">FGC</span>    <span class="tag">FGCT</span>     <span class="tag">GCT</span>   </span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span> 83598<span class="class">.2</span> 39179<span class="class">.8</span> 1677824<span class="class">.0</span> 1677824<span class="class">.0</span> 23068672<span class="class">.0</span> 10786771<span class="class">.2</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21541 11322<span class="class">.520</span>   6     35<span class="class">.315</span> 11357<span class="class">.835</span></span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span>  0<span class="class">.0</span>   48833<span class="class">.8</span> 1677824<span class="class">.0</span> 502293<span class="class">.9</span> 23068672<span class="class">.0</span> 10787056<span class="class">.8</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21541 11325<span class="class">.066</span>   6     35<span class="class">.315</span> 11360<span class="class">.380</span></span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span>  0<span class="class">.0</span>   48833<span class="class">.8</span> 1677824<span class="class">.0</span> 1013386<span class="class">.7</span> 23068672<span class="class">.0</span> 10787056<span class="class">.8</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21541 11325<span class="class">.066</span>   6     35<span class="class">.315</span> 11360<span class="class">.380</span></span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span>  0<span class="class">.0</span>   48833<span class="class">.8</span> 1677824<span class="class">.0</span> 1515312<span class="class">.1</span> 23068672<span class="class">.0</span> 10787056<span class="class">.8</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21541 11325<span class="class">.066</span>   6     35<span class="class">.315</span> 11360<span class="class">.380</span></span><br><span class="line">209664<span class="class">.0</span> 209664<span class="class">.0</span> 79664<span class="class">.9</span>  0<span class="class">.0</span>   1677824<span class="class">.0</span> 363216<span class="class">.4</span> 23068672<span class="class">.0</span> 10787062<span class="class">.6</span> 64496<span class="class">.0</span> 38947<span class="class">.4</span>  21542 11325<span class="class">.180</span>   6     35<span class="class">.315</span> 11360<span class="class">.494</span></span><br></pre></td></tr></table></figure></p>
<p>显示结果标题栏字段含义：<br>S0C：Survivor 0区容量(Capacity)。<br>S1C：Survivor 1区容量(Capacity)。<br>S0U：Survivor 0区使用量(Used)。<br>S1U：Survivor 1区使用量(Used)。<br>EC：  Eden区容量。<br>EU：  Eden区使用量。<br>OC：  Old区容量。<br>OU：  Old区使用量。<br>PC：  Perm区容量。<br>PU：  Perm区使用量。<br>YGC： Young GC次数。<br>YGCT：Young GC耗时。<br>FGC： Full GC次数。<br>FGCT：Full GC耗时。<br>GCT： GC总耗时。</p>
<p>例子2：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># <span class="tag">jstat</span> <span class="tag">-gcutil</span> 6923 2000 1000</span><br><span class="line">  <span class="tag">S0</span>     <span class="tag">S1</span>     <span class="tag">E</span>      <span class="tag">O</span>      <span class="tag">P</span>     <span class="tag">YGC</span>     <span class="tag">YGCT</span>    <span class="tag">FGC</span>    <span class="tag">FGCT</span>     <span class="tag">GCT</span>   </span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.22</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.36</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.36</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.36</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.39</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.42</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.80</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  70<span class="class">.80</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  71<span class="class">.04</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br><span class="line">  1<span class="class">.73</span>   0<span class="class">.00</span>  71<span class="class">.17</span>  40<span class="class">.62</span>  99<span class="class">.85</span>   2764  125<span class="class">.732</span>     0    0<span class="class">.000</span>  125<span class="class">.732</span></span><br></pre></td></tr></table></figure></p>
<h2 id="GC_日志输出参数配置">GC 日志输出参数配置</h2><ul>
<li>打开 -verbose:gc 开关可显示GC的操作内容，包括最忙和最空闲收集行为发生的时间、收集前后的内存大小、收集需要的时间等。</li>
<li>打开 -xx:+printGCdetails 开关，可以详细了解GC中的变化。</li>
<li>打开 -XX:+PrintGCTimeStamps 开关，可以了解这些垃圾收集发生的时间，自JVM启动以后以秒计量。</li>
<li>打开 -xx:+PrintHeapAtGC 开关了解堆的更详细的信息。</li>
<li>打开 -XX:+PrintTenuringDistribution 开关了解获得使用期的对象权。</li>
<li>打开 -Xloggc:/var/log/gclogs/gc.log gc日志产生的路径</li>
<li>打开 -XX:+PrintGCApplicationStoppedTime 输出GC造成应用暂停的时间</li>
<li>打开 -XX:+PrintGCDateStamps GC发生的时间信息</li>
</ul>
<h2 id="GC_日志输出样例">GC 日志输出样例</h2><p>Minor GC日志：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2015<span class="tag">-05-08T15</span><span class="pseudo">:50</span><span class="pseudo">:10</span><span class="class">.113</span>+0800: 5<span class="class">.930</span>: <span class="attr_selector">[GC2015-05-08T15:50:10.113+0800: 5.930: [ParNew: 174706K-&gt;16932K(184320K), 0.0309770 secs]</span> 201085<span class="tag">K-</span>&gt;43310<span class="tag">K</span>(1028096<span class="tag">K</span>), 0<span class="class">.0311100</span> <span class="tag">secs</span>] <span class="attr_selector">[Times: user=0.14 sys=0.00, real=0.03 secs]</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>表示发生一次Minor GC，ParNew是新生代的gc算法，174706K表示Eden区的存活对象的内存总和，16932K表示回收后的存活对象的内存总和，184320K是整个eden区的内存总和。0.0309770 secs表示minor gc花费的时间。</li>
<li>201085K-&gt;43310K(1028096K) 表明这个JVM Heap从 201085K 降低到了 43310K。</li>
<li>[Times: user=0.14 sys=0.00, real=0.03 secs]表明这次GC的user time是0.14，而real time是0.03秒；( user/sys/real 的解释参见：<a href="http://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1" target="_blank" rel="external">http://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1</a> )</li>
</ul>
<p>Full GC日志：<br><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">08</span>T17:<span class="number">31</span>:<span class="number">19.816</span>+<span class="number">0800</span>: <span class="number">8317.639</span>: [<span class="keyword">Full</span> GC2015-<span class="number">04</span>-<span class="number">08</span>T17:<span class="number">31</span>:<span class="number">19.816</span>+<span class="number">0800</span>: <span class="number">8317.639</span>: [CMS: <span class="number">603725</span><span class="keyword">K</span>-&gt;<span class="number">464743</span><span class="keyword">K</span>(<span class="number">843776</span><span class="keyword">K</span>), <span class="number">0.6577700</span> secs] <span class="number">788045</span><span class="keyword">K</span>-&gt;<span class="number">464743</span><span class="keyword">K</span>(<span class="number">1028096</span><span class="keyword">K</span>), [CMS Perm : <span class="number">40447</span><span class="keyword">K</span>-&gt;<span class="number">40446</span><span class="keyword">K</span>(<span class="number">67100</span><span class="keyword">K</span>)], <span class="number">0.6579650</span> secs] [<span class="keyword">Times</span>: user=<span class="number">0.54</span> sys=<span class="number">0.00</span>, real=<span class="number">0.65</span> secs]</span><br></pre></td></tr></table></figure></p>
<ul>
<li>最前面的数字 8317.639 代表GC发生的时间，是从Java虚拟机启动以来经过的秒数；</li>
<li>表示发生了一次Full GC，有Full说明发生了Stop-The-World，，如果是调用system.gc()方法所触发的收集，将显示(System)；</li>
<li>整个JVM都停顿了 0.6577700 秒。</li>
<li>CMS Perm表示GC发生的区域，名称是由收集器决定的。</li>
</ul>
<h2 id="参考：">参考：</h2><p><a href="http://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/generations.html" target="_blank" rel="external">http://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/generations.html</a><br><a href="http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html" target="_blank" rel="external">http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html</a><br><a href="http://www.oracle.com/technetwork/java/gc-tuning-5-138395.html" target="_blank" rel="external">http://www.oracle.com/technetwork/java/gc-tuning-5-138395.html</a><br><a href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" target="_blank" rel="external">http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html</a><br><a href="http://www.infoq.com/cn/articles/Java-PERMGEN-Removed" target="_blank" rel="external">http://www.infoq.com/cn/articles/Java-PERMGEN-Removed</a><br><a href="http://docs.oracle.com/cd/E21764_01/web.1111/e13814/jvm_tuning.htm#PERFM169" target="_blank" rel="external">http://docs.oracle.com/cd/E21764_01/web.1111/e13814/jvm_tuning.htm#PERFM169</a><br><a href="http://blog.csdn.net/ning109314/article/details/10411495" target="_blank" rel="external">http://blog.csdn.net/ning109314/article/details/10411495</a><br><a href="http://jbutton.iteye.com/blog/1569746" target="_blank" rel="external">http://jbutton.iteye.com/blog/1569746</a><br><a href="http://buddie.iteye.com/blog/1824937" target="_blank" rel="external">http://buddie.iteye.com/blog/1824937</a><br><a href="http://www.cnblogs.com/ggjucheng/p/3977384.html" target="_blank" rel="external">http://www.cnblogs.com/ggjucheng/p/3977384.html</a><br><a href="http://blog.csdn.net/historyasamirror/article/details/6233007" target="_blank" rel="external">http://blog.csdn.net/historyasamirror/article/details/6233007</a><br><a href="http://sargeraswang.com/blog/2014/02/03/la-ji-shou-ji-qi-yu-nei-cun-fen-pei-ce-lue/" target="_blank" rel="external">http://sargeraswang.com/blog/2014/02/03/la-ji-shou-ji-qi-yu-nei-cun-fen-pei-ce-lue/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="题外话">题外话</h2><p>本文当前的范围是Java 8之前的虚拟机，因为Java 8之后虚拟机的架构有比较大的调整，存储类的元数据信息的永久代被删除了，而是放在虚拟机的元空间。<br>JDK 7及之前的版本中永久代有如下的特点：</p>
<ol>
<li>永]]>
    </summary>
    
      <category term="JVM" scheme="http://navigating.github.io/tags/JVM/"/>
    
      <category term="Java" scheme="http://navigating.github.io/tags/Java/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201509]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201509/"/>
    <id>http://navigating.github.io/2015/大数据动态之201509/</id>
    <published>2015-10-16T02:41:31.000Z</published>
    <updated>2015-10-16T02:43:52.651Z</updated>
    <content type="html"><![CDATA[<p>Apache Kylin<br>Apache Kylin v1.0 发布，分布式分析引擎<br><a href="http://www.oschina.net/news/65938/apache-kylin-1-0-released" target="_blank" rel="external">http://www.oschina.net/news/65938/apache-kylin-1-0-released</a> </p>
<p>Apache Calcite<br>Apache Calcite：Hadoop中新型大数据查询引擎<br><a href="http://www.infoq.com/cn/articles/new-big-data-hadoop-query-engine-apache-calcite" target="_blank" rel="external">http://www.infoq.com/cn/articles/new-big-data-hadoop-query-engine-apache-calcite</a> </p>
<p>Apache Flink<br><a href="http://flink.apache.org/" target="_blank" rel="external">http://flink.apache.org/</a> </p>
<p>Cloudera<br>新的快数据存储Hadoop组件，Kudu:<br><a href="http://blog.cloudera.com/blog/2015/09/kudu-new-apache-hadoop-storage-for-fast-analytics-on-fast-data/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/kudu-new-apache-hadoop-storage-for-fast-analytics-on-fast-data/</a><br>Hadoop细粒度的安全增强组件，RecordService：<br><a href="http://blog.cloudera.com/blog/2015/09/recordservice-for-fine-grained-security-enforcement-across-the-hadoop-ecosystem/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/recordservice-for-fine-grained-security-enforcement-across-the-hadoop-ecosystem/</a><br>HDFS Erasure Coding特性<br><a href="http://blog.cloudera.com/blog/2015/09/introduction-to-hdfs-erasure-coding-in-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/introduction-to-hdfs-erasure-coding-in-apache-hadoop/</a><br>Spark测试基础库<br><a href="http://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/</a><br>如何使用Impala对非结构化数据进行分析：<br><a href="http://blog.cloudera.com/blog/2015/09/how-to-prepare-unstructured-data-in-impala-for-analysis/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/how-to-prepare-unstructured-data-in-impala-for-analysis/</a><br>BI场景下Impala测试结果<br><a href="http://blog.cloudera.com/blog/2015/09/how-impala-scales-for-business-intelligence-new-test-results/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/how-impala-scales-for-business-intelligence-new-test-results/</a><br>揭秘Apache Hadoop YARN:<br><a href="http://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/</a><br><a href="http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-users/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-users/</a><br><a href="http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-operators/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-operators/</a><br>Impala支持shell执行的动态进度报告(Impala’s debug webpages (http:::25000))：<br><a href="http://blog.cloudera.com/blog/2015/09/dynamic-progress-reports-in-the-impala-shell/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/09/dynamic-progress-reports-in-the-impala-shell/</a><br>Cloudera One Platform:<br><a href="http://vision.cloudera.com/one-platform/" target="_blank" rel="external">http://vision.cloudera.com/one-platform/</a> </p>
<p>Hortonworks<br>Microsoft Azure HDInsight对Ubuntu Linux支持，可以支持到Hadoop 2.6：<br><a href="http://hortonworks.com/blog/microsoft-azure-hdinsight-on-linux-choice/" target="_blank" rel="external">http://hortonworks.com/blog/microsoft-azure-hdinsight-on-linux-choice/</a><br>HDP 2.3 Sandbox在Microsoft Azure Gallery上线：<br><a href="http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/</a><br>Impala与Hive性能对比<br><a href="http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/" target="_blank" rel="external">http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/</a><br>HDP迁移案例：<br><a href="http://hortonworks.com/blog/migration-to-hdp-as-easy-as-1-2-3-without-downtime-or-disruption/" target="_blank" rel="external">http://hortonworks.com/blog/migration-to-hdp-as-easy-as-1-2-3-without-downtime-or-disruption/</a> </p>
<p>MapR<br>Spark on YARN资源分配配置<br><a href="https://www.mapr.com/blog/resource-allocation-configuration-spark-yarn#.VfoRrSWqqko" target="_blank" rel="external">https://www.mapr.com/blog/resource-allocation-configuration-spark-yarn#.VfoRrSWqqko</a><br><a href="https://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="external">https://spark.apache.org/docs/latest/running-on-yarn.html</a><br><a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="external">https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation</a><br>SAP HANA与Mapr DP混合架构<br><a href="https://www.mapr.com/blog/sap-hana-vora-and-mapr-data-platform#.VfoRsCWqqko" target="_blank" rel="external">https://www.mapr.com/blog/sap-hana-vora-and-mapr-data-platform#.VfoRsCWqqko</a><br>Spark Streaming with HBase<br><a href="https://www.mapr.com/blog/spark-streaming-hbase#.VfoR0SWqqko" target="_blank" rel="external">https://www.mapr.com/blog/spark-streaming-hbase#.VfoR0SWqqko</a><br>MapR对Docker的支持<br><a href="https://www.mapr.com/blog/how-create-instant-mapr-clusters-docker#.VfoR2CWqqko" target="_blank" rel="external">https://www.mapr.com/blog/how-create-instant-mapr-clusters-docker#.VfoR2CWqqko</a><br><a href="https://www.mapr.com/blog/my-experience-running-docker-containers-on-mesos#.Vfoc_yWqqkp" target="_blank" rel="external">https://www.mapr.com/blog/my-experience-running-docker-containers-on-mesos#.Vfoc_yWqqkp</a> </p>
<p>Databricks<br>Spark Survey 2015调查结果：<br><a href="https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/24/spark-survey-results-2015-are-now-available.html</a><br>Spark代码调试：实时进度条和Spark UI<br><a href="https://databricks.com/blog/2015/09/23/easier-spark-code-debugging-real-time-progress-bar-and-spark-ui-integration-in-databricks.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/23/easier-spark-code-debugging-real-time-progress-bar-and-spark-ui-integration-in-databricks.html</a><br>新版本Spark 1.5上LDA算法的性能提升：<br><a href="https://databricks.com/blog/2015/09/22/large-scale-topic-modeling-improvements-to-lda-on-spark.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/22/large-scale-topic-modeling-improvements-to-lda-on-spark.html</a><br>Spark 1.5 DataFrame API:<br><a href="https://databricks.com/blog/2015/09/16/spark-1-5-dataframe-api-highlights-datetimestring-handling-time-intervals-and-udafs.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/16/spark-1-5-dataframe-api-highlights-datetimestring-handling-time-intervals-and-udafs.html</a><br>Spark 1.5发布，在性能、可用性、运维、Data Science API等方面有重大改进：<br><a href="https://databricks.com/blog/2015/09/09/announcing-spark-1-5.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/09/announcing-spark-1-5.html</a><br><a href="http://www.csdn.net/article/2015-09-29/2825825" target="_blank" rel="external">http://www.csdn.net/article/2015-09-29/2825825</a><br><a href="http://www.csdn.net/article/2015-09-10/2825669" target="_blank" rel="external">http://www.csdn.net/article/2015-09-10/2825669</a> </p>
<p>MongoDB<br>MongoDB性能优化五个简单步骤：<br><a href="http://www.csdn.net/article/2015-09-30/2825833" target="_blank" rel="external">http://www.csdn.net/article/2015-09-30/2825833</a><br>MongoDB开发版本3.1.8发布<br><a href="http://www.csdn.net/article/2015-09-17/2825734" target="_blank" rel="external">http://www.csdn.net/article/2015-09-17/2825734</a><br>分布式文档数据库MongoDB开发版本3.1.7发布<br><a href="http://www.csdn.net/article/2015-09-01/2825599-mongodb-317-is-released" target="_blank" rel="external">http://www.csdn.net/article/2015-09-01/2825599-mongodb-317-is-released</a> </p>
<p>参考<br>逆水行舟，看前行中的Spark<br><a href="http://www.csdn.net/article/2015-09-21/2825754" target="_blank" rel="external">http://www.csdn.net/article/2015-09-21/2825754</a><br>微店的大数据平台建设实践与探讨<br><a href="http://www.csdn.net/article/2015-09-21/2825756" target="_blank" rel="external">http://www.csdn.net/article/2015-09-21/2825756</a><br>打造数据驱动的组织：第二年<br><a href="http://zhuanlan.zhihu.com/donglaoshi/20205116" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20205116</a><br>揭开 Growth Hacking 的神秘面纱（上篇）<br><a href="http://zhuanlan.zhihu.com/qinchao/20190015" target="_blank" rel="external">http://zhuanlan.zhihu.com/qinchao/20190015</a><br>京东大数据基础架构和实践—王彦明<br><a href="http://share.csdn.net/slides/9138" target="_blank" rel="external">http://share.csdn.net/slides/9138</a><br>京东数据仓库海量数据交换工具—张侃<br><a href="http://share.csdn.net/slides/9137" target="_blank" rel="external">http://share.csdn.net/slides/9137</a><br>京东大数据分析与创新应用<br><a href="http://share.csdn.net/slides/9139" target="_blank" rel="external">http://share.csdn.net/slides/9139</a><br>LinkedIn架构这十年<br><a href="http://engineering.linkedin.com/architecture/brief-history-scaling-linkedin" target="_blank" rel="external">http://engineering.linkedin.com/architecture/brief-history-scaling-linkedin</a><br><a href="http://colobu.com/2015/07/24/brief-history-scaling-linkedin/" target="_blank" rel="external">http://colobu.com/2015/07/24/brief-history-scaling-linkedin/</a><br>LinkedIn是如何优化Kafka的<br><a href="http://www.infoq.com/cn/articles/linkedIn-improving-kafka" target="_blank" rel="external">http://www.infoq.com/cn/articles/linkedIn-improving-kafka</a><br><a href="http://engineering.linkedin.com/apache-kafka/how-we%E2%80%99re-improving-and-advancing-kafka-linkedin" target="_blank" rel="external">http://engineering.linkedin.com/apache-kafka/how-we%E2%80%99re-improving-and-advancing-kafka-linkedin</a><br>阿里CDN从自建到服务<br><a href="http://share.csdn.net/slides/8319" target="_blank" rel="external">http://share.csdn.net/slides/8319</a><br>系统架构设计-负载均衡和高可用<br><a href="http://share.csdn.net/slides/12338" target="_blank" rel="external">http://share.csdn.net/slides/12338</a><br>OSTC2015-朱照远（叔度）阿里开源经验分享<br><a href="http://share.csdn.net/slides/13730" target="_blank" rel="external">http://share.csdn.net/slides/13730</a><br>Voidbox<br><a href="http://dongxicheng.org/mapreduce-nextgen/voidbox-docker-on-hadoop-hulu/" target="_blank" rel="external">http://dongxicheng.org/mapreduce-nextgen/voidbox-docker-on-hadoop-hulu/</a><br>深入理解Spark Streaming执行模型<br><a href="http://www.csdn.net/article/2015-09-13/2825689" target="_blank" rel="external">http://www.csdn.net/article/2015-09-13/2825689</a><br>Apache Spark 1.5新特性介绍<br><a href="http://www.csdn.net/article/2015-09-10/2825669" target="_blank" rel="external">http://www.csdn.net/article/2015-09-10/2825669</a><br>盘点大数据生态圈，那些繁花似锦的开源项目<br><a href="http://www.csdn.net/article/2015-09-11/2825674" target="_blank" rel="external">http://www.csdn.net/article/2015-09-11/2825674</a><br>Redis整合Spring项目搭建实例<br><a href="http://www.csdn.net/article/2015-09-01/2825600" target="_blank" rel="external">http://www.csdn.net/article/2015-09-01/2825600</a><br>MongoDB开发版本3.1.8发布<br><a href="http://www.csdn.net/article/2015-09-17/2825734" target="_blank" rel="external">http://www.csdn.net/article/2015-09-17/2825734</a><br>分布式并行数据库将在 OLTP 领域促进去“Oracle”<br><a href="http://www.csdn.net/article/2015-09-11/2825678" target="_blank" rel="external">http://www.csdn.net/article/2015-09-11/2825678</a><br>Gartner 2015新兴技术发展周期简评：大数据实用化、机器学习崛起<br><a href="http://www.csdn.net/article/2015-09-06/2825620" target="_blank" rel="external">http://www.csdn.net/article/2015-09-06/2825620</a><br>Hortonworks收购Onyara，启动数据流自动化<br><a href="http://www.csdn.net/article/2015-09-02/2825612" target="_blank" rel="external">http://www.csdn.net/article/2015-09-02/2825612</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Apache Kylin<br>Apache Kylin v1.0 发布，分布式分析引擎<br><a href="http://www.oschina.net/news/65938/apache-kylin-1-0-released" target="_blank" rel]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《Impala vs. Hive Performance Benchmark》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8AImpala-vs-Hive-Performance-Benchmark%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《Impala-vs-Hive-Performance-Benchmark》/</id>
    <published>2015-09-24T04:56:08.000Z</published>
    <updated>2015-09-24T04:59:01.109Z</updated>
    <content type="html"><![CDATA[<p>原文：<a href="http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/" target="_blank" rel="external">http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/</a><br>学习如下：</p>
<p>本文是Yahoo! JAPAN针对自己的场景需求进行设计书选型，对Impala和Hive(Tez on YARN)所做的评测。<br>场景数据和要求：</p>
<ol>
<li>数据格式为 Text 或者 gz ;</li>
<li>每天新增数据文件为10G，数据记录为13亿行；</li>
<li>数据留存(retention)周期为13个月，共有数据6000G，共有4500亿行；</li>
<li>每个小时需要生成 15000 个报表(reporting)；</li>
<li>查询条件包含少量的的 grouping ，grouping的条件主要是地区(region)或者性别(gender)；</li>
<li>没有过滤条件查询；</li>
<li>绝大部分基于时间的报表(report)生成都是周期性的，除了小时报表，还有天报表和周报表；</li>
</ol>
<p>技术选型：</p>
<ol>
<li>Cloudera Impala，没提到所评估的版本，估计为最近的版本。当前Impala的最新版本为。</li>
<li>Hortonworks HDP 2.2, Apache Hive-0.14, Apache Tez。</li>
</ol>
<p>考虑Impala</p>
<ol>
<li>Imapa查询耗时比较少，一般在几秒到几十秒之间；</li>
<li>当一次查询的响应时间为15秒，每小时能够执行240次查询；</li>
<li>针对单位时间内处理量的不断增加，需要考虑通过增加单位时间内的并行查询数量来提升查询的数量；使用Impala遇到的问题是，随着查询并行度的增加，Impala查询的响应时间线性增加；因此，在每天数据更新之后Impala无法自动处理完成所有的批量查询。</li>
</ol>
<p>考虑Hive和Tez on YARN<br>Hadoop-2.x, YARN有更好粒度的并行执行控制，Tez引擎能大幅度的降低MapReduce的延时。<br>YARN和Tez大幅度的增加处理能力，每小时需要处理超过15000个任务。之前的Hadoop 1.0集群，每天已经能够处理100000个任务。</p>
<p>测试验证</p>
<p>测试条件</p>
<ol>
<li>计划模拟真实业务场景测试执行近2000个SQL；</li>
<li>绝大部分查询返回的结果少于 1000 行数据；</li>
<li>少数查询返回的结果超过100000行数据;</li>
<li>执行近2000个SQL的并发请求为32；</li>
</ol>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/HiveImpala_001.png" alt="这是一张图片"></p>
<p>测试结果</p>
<ol>
<li>Hive请求的大部分请求在20秒以内返回，随着返回结果集数据的增加查询时间也随之增加，最长返回时间为70秒；</li>
<li>Impala请求的大部分请求返回在30秒～60秒之间，最长返回时间为10分钟，随着返回结果集数据的增加查询时间大幅度显著的增加；</li>
<li>在一些低负载(low load)条件的查询中，Impala能够达到毫秒(milliseconds)级返回；如果没有SQL并行化的处理需求，Impala是有效的选择。</li>
<li>对于要求批处理，并且SQL并行化是必须的场景中，Hive on Tez是更好的选择。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/HiveImpala_002.png" alt="这是一张图片"></p>
<p>Hive的并发性<br>对于单个的HiveServer2实例，我们测试验证多少个并行查询可以被执行，多少并行度的处理是最有效的。<br>我们以16作为SQL并发执行的提升倍数，衡量的指标是SQL执行的处理时间。<br>在并发达到64之前查询的吞吐量快速增加，在这点之后开始下降，因此在当前环境下64是最好的并发数。</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/HiveImpala_003.png" alt="这是一张图片"></p>
<p>结论<br>对于Hive on Tez，单个SQL执行时间一般为15秒，会随着并行度的提升而增加。(并行度的限制主要依赖于集群的大小和性能。)<br>在低负载状态下Impala有非常快的响应时间，但并不适合于SQL并行度非常高的场景。<br>最后的结论是测试者最后选择了Hive on Tez，因为测试者的场景是每小时至少处理15000个SQL请求。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>原文：<a href="http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/" target="_blank" rel="external">http://hortonworks.com/blog]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Hive" scheme="http://navigating.github.io/tags/Hive/"/>
    
      <category term="Impala" scheme="http://navigating.github.io/tags/Impala/"/>
    
      <category term="Tez" scheme="http://navigating.github.io/tags/Tez/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hortonworks HDP 2.3.0]]></title>
    <link href="http://navigating.github.io/2015/Hortonworks-HDP-2-3-0/"/>
    <id>http://navigating.github.io/2015/Hortonworks-HDP-2-3-0/</id>
    <published>2015-09-22T02:23:39.000Z</published>
    <updated>2015-09-22T02:26:21.433Z</updated>
    <content type="html"><![CDATA[<p>HDP 2.3 相对于 HDP 2.2.6</p>
<ol>
<li>HBase版本变化比较大，HBase 1.1.1；</li>
<li>新增加了一些组件；</li>
<li>Ambari在配置、监控等方面有较大的升级；</li>
</ol>
<p>新增加的组件：</p>
<pre><code>1. <span class="tag">Apache</span> <span class="tag">Atlas</span> 0<span class="class">.5</span><span class="class">.0</span>
2. <span class="tag">Apache</span> <span class="tag">Calcite</span> 1<span class="class">.2</span><span class="class">.0</span>
3. <span class="tag">Apache</span> <span class="tag">Solr</span> 5<span class="class">.2</span><span class="class">.1</span>
4. <span class="tag">Cascading</span> 3<span class="class">.0</span><span class="class">.1</span>
5. <span class="tag">Cloudbreak</span> 1<span class="class">.0</span>
6. <span class="tag">SmartSense</span>
</code></pre><p>版本升级的组件：</p>
<pre><code>1. <span class="tag">Apache</span> <span class="tag">Ambari</span> 2<span class="class">.1</span>
2. <span class="tag">Apache</span> <span class="tag">Hadoop</span> 2<span class="class">.7</span><span class="class">.1</span>
3. <span class="tag">Apache</span> <span class="tag">HBase</span> 1<span class="class">.1</span><span class="class">.1</span>
4. <span class="tag">Apache</span> <span class="tag">Spark</span> 1<span class="class">.3</span><span class="class">.1</span>
5. <span class="tag">Apache</span> <span class="tag">Hive</span> 1<span class="class">.2</span><span class="class">.1</span>
6. <span class="tag">Apache</span> <span class="tag">Kafka</span> 0<span class="class">.8</span><span class="class">.2</span>
7. <span class="tag">Apache</span> <span class="tag">Phoenix</span> 4<span class="class">.4</span><span class="class">.0</span>
8. <span class="tag">Apache</span> <span class="tag">Pig</span> 0<span class="class">.15</span><span class="class">.0</span>
9. <span class="tag">Apache</span> <span class="tag">Sqoop</span> 1<span class="class">.4</span><span class="class">.6</span>
10. <span class="tag">Apache</span> <span class="tag">Oozie</span> 4<span class="class">.2</span><span class="class">.0</span>
11. <span class="tag">Apache</span> <span class="tag">Knox</span> 0<span class="class">.6</span><span class="class">.0</span>
12. <span class="tag">Apache</span> <span class="tag">Ranger</span> 0<span class="class">.5</span><span class="class">.0</span>
13. <span class="tag">Apache</span> <span class="tag">Falcon</span> 0<span class="class">.6</span><span class="class">.1</span>
14. <span class="tag">Slider</span> 0<span class="class">.80</span><span class="class">.0</span>
15. <span class="tag">Tez</span> 0<span class="class">.7</span><span class="class">.0</span>
16. <span class="tag">Storm</span> 0<span class="class">.10</span><span class="class">.0</span>
</code></pre><p>Ambari 2.1新特性：</p>
<pre><code><span class="bullet">1. </span>通过Ambari参数配置UI优化；
<span class="bullet">2. </span>各项服务Metrics可以进行监控指标添加，通过添加Widgets就可以；
<span class="bullet">3. </span>支持Hive, Pig, Files, Capacity Scheduler的User Views界面，可以通过UI执行Hive、Pig语句；
<span class="bullet">4. </span>配置 Capacity Scheduler 支持图形化了，更加方便；
<span class="bullet">5. </span>支持 SQL User View；
<span class="bullet">6. </span>支持机架感应(Rack Awareness)配置：通过Ambari来配置；
<span class="bullet">7. </span>支持 Cloudbreak；
<span class="bullet">8. </span>支持 SmartSense；
</code></pre><p>系统要求：</p>
<pre><code>1. <span class="tag">Red</span> <span class="tag">Hat</span> <span class="tag">Enterprise</span> <span class="tag">Linux</span> (<span class="tag">RHEL</span>) <span class="tag">v6</span><span class="class">.x</span> 或者 <span class="tag">Red</span> <span class="tag">Hat</span> <span class="tag">Enterprise</span> <span class="tag">Linux</span> (<span class="tag">RHEL</span>) <span class="tag">v7</span><span class="class">.x</span>
2. <span class="tag">Oracle</span> <span class="tag">JDK</span> 1<span class="class">.8</span> 64<span class="tag">-bit</span> (<span class="tag">minimum</span> <span class="tag">JDK</span> 1<span class="class">.8_40</span>) (<span class="tag">default</span>) 或者 <span class="tag">Oracle</span> <span class="tag">JDK</span> 1<span class="class">.7</span> 64<span class="tag">-bit</span> (<span class="tag">minimum</span> <span class="tag">JDK</span> 1<span class="class">.7_67</span>)
</code></pre><p>参考：<br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_RelNotes/content/ch_relnotes_v230.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_RelNotes/content/ch_relnotes_v230.html</a><br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>HDP 2.3 相对于 HDP 2.2.6</p>
<ol>
<li>HBase版本变化比较大，HBase 1.1.1；</li>
<li>新增加了一些组件；</li>
<li>Ambari在配置、监控等方面有较大的升级；</li>
</ol>
<p>新增加的组件：</p>]]>
    </summary>
    
      <category term="Ambari" scheme="http://navigating.github.io/tags/Ambari/"/>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Oozie：入门概述]]></title>
    <link href="http://navigating.github.io/2015/Oozie%EF%BC%9A%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0/"/>
    <id>http://navigating.github.io/2015/Oozie：入门概述/</id>
    <published>2015-09-18T15:24:54.000Z</published>
    <updated>2015-09-18T15:40:18.791Z</updated>
    <content type="html"><![CDATA[<h3 id="Oozie能做什么(What_Oozie_Does)">Oozie能做什么(What Oozie Does)</h3><p>Oozie是一个Java Web应用，用于Apache Hadoop的任务(jobs)调度。Oozie顺序的合并多个任务(jobs)成为一个可工作的逻辑单元。其主要是集成了Hadoop技术栈，包括YARN等，支持Apache MapReduce, Apache Pig, Apache Hive, Apache Sqoop等。Oozie使得用户能够通过Java应用或者Shell脚本的方式调度任务。<br>Oozie任务有两种基本类型</p>
<pre><code>* Oozie Workflow jobs：这种任务是一个有向无环图(DAG, <span class="keyword">Direct</span> Acyclical Graph)，并按着规则顺序的执行，即上一个<span class="keyword">Action</span>运行完成后才能运行下一个<span class="keyword">Action</span>。所以其经常不得不等待。
* Oozie Coordinator jobs：这种任务是重复性的工作流，一般被时间或者数据达到可用会被触发。
</code></pre><p>Oozie Bundle提供一个复合的方式，将多个Workflow jobs和Coordinator jobs打包合并在一起并能对它们的生命周期进行管理。</p>
<h3 id="Oozie如何工作(How_Oozie_works)">Oozie如何工作(How Oozie works)</h3><p>一个Oozie Workflow是一系列编排成有向无环图（DAG）的Action集合。控制节点定义job时间,设置开始和结束workflow的规则(rules)。这样，Oozie通过decision，fork和join节点控制工作流的执行路径。Action节点触发任务的执行。<br>Oozie触发工作流的action操作，实际上由Hadoop MapReduce去执行。Oozie利用Hadoop技术栈来均衡负载和处理失败。<br>Oozie通过回调(callback)和轮询(polling)来检测任务的是否完成。当Oozie开始一个任务(task)，它的提供了一个唯一的可以回调的HTTP URL，当这个任务完成的时候就通知这个URL。如果任务失败就调用回调URL，Oozie可以设置任务完成。<br>经常有这种需求，在规则的时间间隔内运行Oozie workflow，处理那些无法预期的有效数据或者时间。在这些情况下，Oozie Coordinator允许你根据、时间或者事件的条件对工作流触发的时机进行建模。在这些条件得到满足之后，工作流任务就就开始启动。<br>Oozie Coordinator也可以管理多个工作流，是依赖于子工作流的输出结果。子工作流程的输出将会成为下一个工作流的输入。这条链被称为“数据应用管道”(data application pipeline)。</p>
<h3 id="工作流定义">工作流定义</h3><p>定义一个Oozie工作流，两个配置文件是必须的，job.properties和workflow.xml。<br>job.properties的环境变量如下：<br>nameNode                        hdfs://mycluster:8020                HDFS地址<br>jobTracker                        localhost:8034                        jobTracker地址<br>queueName                        default                                Oozie队列<br>examplesRoot                    examples                            全局目录<br>oozie.usr.system.libpath        true                                是否加载用户的lib库<br>oozie.libpath                    share/lib/user                        用户lib库<br>oozie.wf.application.path        ${nameNode}/user/${user.name}/        Oozie流程所在的HDFS地址</p>
<p>workflow.xml示例如下：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span><br><span class="line">  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">  distributed with this work for additional information</span><br><span class="line">  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">  to you under the Apache License, Version 2.0 (the</span><br><span class="line">  "License"); you may not use this file except in compliance</span><br><span class="line">  with the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">       http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an "AS IS" BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License.</span><br><span class="line">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">workflow-app</span> <span class="attribute">xmlns</span>=<span class="value">"uri:oozie:workflow:0.2"</span> <span class="attribute">name</span>=<span class="value">"map-reduce-wf"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">start</span> <span class="attribute">to</span>=<span class="value">"mr-node"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">action</span> <span class="attribute">name</span>=<span class="value">"mr-node"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">map-reduce</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="title">job-tracker</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="title">name-node</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">prepare</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">delete</span> <span class="attribute">path</span>=<span class="value">"$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">prepare</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.mapper.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.oozie.example.SampleMapper<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.reducer.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.oozie.example.SampleReducer<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.map.tasks<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.input.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/text<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.output.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">map-reduce</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">ok</span> <span class="attribute">to</span>=<span class="value">"end"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">error</span> <span class="attribute">to</span>=<span class="value">"fail"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">action</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">kill</span> <span class="attribute">name</span>=<span class="value">"fail"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">message</span>&gt;</span>Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="title">message</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">kill</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">end</span> <span class="attribute">name</span>=<span class="value">"end"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="实战命令">实战命令</h3><p>上传example目录到hdfs用户oozie根目录(/user/oozie)下：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su - oozie</span><br><span class="line">cd <span class="regexp">/usr/</span>hdp<span class="regexp">/current/</span>oozie-server/doc</span><br><span class="line">hdfs dfs -put example example</span><br></pre></td></tr></table></figure></p>
<p>启动任务命令：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie <span class="string">http:</span><span class="comment">//localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run</span></span><br></pre></td></tr></table></figure></p>
<p>停止任务命令：<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie http://localhost:11000/oozie -kill <span class="number">0000002-150</span><span class="number">914143759473</span>-oozie-oozi-W</span><br></pre></td></tr></table></figure></p>
<h3 id="Oozie_Web_UI效果图：">Oozie Web UI效果图：</h3><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_001.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_002.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_003.JPG" alt="这是一张图片"></p>
<p>参考：<br><a href="http://hortonworks.com/hadoop/oozie/" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/</a><br><a href="http://oozie.apache.org/" target="_blank" rel="external">http://oozie.apache.org/</a><br><a href="http://hortonworks.com/hadoop/oozie/#blog" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/#blog</a><br><a href="http://hortonworks.com/hadoop/oozie/#forums" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/#forums</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/</a><br><a href="https://github.com/yahoo/oozie" target="_blank" rel="external">https://github.com/yahoo/oozie</a><br>书籍：<br>《Apache Oozie: The Workflow Scheduler for Hadoop》<br><a href="http://book.douban.com/subject/26348732/" target="_blank" rel="external">http://book.douban.com/subject/26348732/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="Oozie能做什么(What_Oozie_Does)">Oozie能做什么(What Oozie Does)</h3><p>Oozie是一个Java Web应用，用于Apache Hadoop的任务(jobs)调度。Oozie顺序的合并多个任务(jobs)成为一个]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Oozie" scheme="http://navigating.github.io/tags/Oozie/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据技术百度指数201508]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%99%BE%E5%BA%A6%E6%8C%87%E6%95%B0201508/"/>
    <id>http://navigating.github.io/2015/大数据技术百度指数201508/</id>
    <published>2015-09-16T06:17:00.000Z</published>
    <updated>2015-09-16T09:35:47.738Z</updated>
    <content type="html"><![CDATA[<p>关于大数据技术点的搜索指数，这里只关注一下百度指数的结果。</p>
<ol>
<li>当前最热的依次是：Hadoop、Redis、MongoDB、Spark、Storm。可以看到国内Redis、MongoDB的用户很多，有时候比HBase都热，可见热度之高。</li>
<li>从趋势来看，Spark是最强劲的，Hadoop、Redis表现都很不错。</li>
<li>从搜索热词看，当前主要表现在入门介绍、安装、教程的需求量非常大。对于使用中的问题、优化议题还不多，对于监控更少。</li>
<li>当前搜索热词来源Top5的城市：北京、上海、深圳、广州、南京。北京、珠三角、长三角，同时整体上中部IT发展的相对不错。</li>
<li>用户人群的年龄主要是30～39之间，几乎达到50%，其次是20～29，几乎40%，其他年龄段的人很少。</li>
</ol>
<p>首先是意外的收获，就是发现Redis和MongoDB在国内这么火，热词竟然超过了Hadoop了，也许是两个比较简单易用，不像Hadoop如今已经发展成为了一个大家族了。<br>第二个意外是，发现Cloudera、Hortonworks、CHD、HDP尽然不是指数热词，看来一般印象的大数据技术等于Hadoop真的是偏见啊。<br>第三个意外，发现中部城市IT整体发展的不错，除了上海、南京，还包括：成都、重庆、武汉、长沙、西安、郑州。</p>
<p>趋势研究<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_01.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_02.JPG" alt="这是一张图片"></p>
<p>需求图谱<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_03.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_04.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_05.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_06.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_07.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_08.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_09.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_10.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_11.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_12.JPG" alt="这是一张图片"></p>
<p>人群画像<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_13.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_14.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_15.JPG" alt="这是一张图片"></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>关于大数据技术点的搜索指数，这里只关注一下百度指数的结果。</p>
<ol>
<li>当前最热的依次是：Hadoop、Redis、MongoDB、Spark、Storm。可以看到国内Redis、MongoDB的用户很多，有时候比HBase都热，可见热度之高。</li>
<l]]>
    </summary>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Redis" scheme="http://navigating.github.io/tags/Redis/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="Storm" scheme="http://navigating.github.io/tags/Storm/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201508]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201508/"/>
    <id>http://navigating.github.io/2015/大数据动态之201508/</id>
    <published>2015-09-07T08:13:04.000Z</published>
    <updated>2015-10-16T02:44:00.973Z</updated>
    <content type="html"><![CDATA[<p>Cloudera：<br>Cloudera Navigator路线图<br><a href="http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-governance-cloudera-navigator-roadmap/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-governance-cloudera-navigator-roadmap/</a><br>NoSQL性能测试开放标准套件YCSB加入Cloudera实验室项目中<br><a href="http://blog.cloudera.com/blog/2015/08/ycsb-the-open-standard-for-nosql-benchmarking-joins-cloudera-labs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/ycsb-the-open-standard-for-nosql-benchmarking-joins-cloudera-labs/</a><br>Spark在TripAdvisor的机器学习应用案例<br><a href="http://blog.cloudera.com/blog/2015/08/using-apache-spark-for-massively-parallel-nlp-at-tripadvisor/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/using-apache-spark-for-massively-parallel-nlp-at-tripadvisor/</a><br>CDH支持Mesos<br><a href="http://blog.cloudera.com/blog/2015/08/how-to-run-apache-mesos-on-cdh/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/how-to-run-apache-mesos-on-cdh/</a><br>HBase开始支持HBase-Spark模块<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br>Navigator Encrypt开始支持YARN Container安全<br><a href="http://blog.cloudera.com/blog/2015/08/how-to-secure-yarn-containers-with-cloudera-navigator-encrypt/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/how-to-secure-yarn-containers-with-cloudera-navigator-encrypt/</a><br>基于Kafka和HBase的近实时集成架构案例: Santanders<br><a href="http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/</a> </p>
<p>Hortonworks:<br>Microsoft Azure Gallery开始支持HDP 2.3<br><a href="http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/</a><br>Microsoft Azure支持Spark<br><a href="http://hortonworks.com/blog/microsoft-and-hortonworks-do-spark-in-the-cloud/" target="_blank" rel="external">http://hortonworks.com/blog/microsoft-and-hortonworks-do-spark-in-the-cloud/</a><br>Storm的容错Nimbus架构<br><a href="http://hortonworks.com/blog/fault-tolerant-nimbus-in-apache-storm/" target="_blank" rel="external">http://hortonworks.com/blog/fault-tolerant-nimbus-in-apache-storm/</a> </p>
<p>MapR<br>Spark Streaming with HBase<br><a href="https://www.mapr.com/blog/spark-streaming-hbase" target="_blank" rel="external">https://www.mapr.com/blog/spark-streaming-hbase</a><br>Apache Drill Architecture: The Ultimate Guide<br><a href="https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide" target="_blank" rel="external">https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide</a><br>HBase架构深度剖析<br><a href="https://www.mapr.com/blog/in-depth-look-hbase-architecture" target="_blank" rel="external">https://www.mapr.com/blog/in-depth-look-hbase-architecture</a><br>HBase Schema设计指导<br><a href="https://www.mapr.com/blog/guidelines-hbase-schema-design" target="_blank" rel="external">https://www.mapr.com/blog/guidelines-hbase-schema-design</a><br>如何利用Spark进行机器学习的并行与交互处理<br><a href="https://www.mapr.com/blog/parallel-and-iterative-processing-machine-learning-recommendations-spark" target="_blank" rel="external">https://www.mapr.com/blog/parallel-and-iterative-processing-machine-learning-recommendations-spark</a></p>
<p>Databricks<br>Spark 1.5发布，包含Tungsten，其利用代码生成技术和Cache感知算法，大幅度提升运行时的性能：<br><a href="https://databricks.com/blog/2015/08/18/spark-1-5-preview-now-available-in-databricks.html" target="_blank" rel="external">https://databricks.com/blog/2015/08/18/spark-1-5-preview-now-available-in-databricks.html</a><br><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" target="_blank" rel="external">https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html</a></p>
<p>mongoDB<br>mongoDB 2.x版本发布了2个，3.x发布了3个：<br><a href="http://blog.mongodb.org/post/128063809158/mongodb-306-rc2-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/128063809158/mongodb-306-rc2-is-released</a><br><a href="http://blog.mongodb.org/post/127802855483/mongodb-317-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/127802855483/mongodb-317-is-released</a><br><a href="http://blog.mongodb.org/post/126436298628/mongodb-2611-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/126436298628/mongodb-2611-is-released</a><br><a href="http://blog.mongodb.org/post/126436227873/mongodb-306-rc0-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/126436227873/mongodb-306-rc0-is-released</a><br><a href="http://blog.mongodb.org/post/125850939688/mongodb-2611-rc0-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/125850939688/mongodb-2611-rc0-is-released</a> </p>
<p>Redis</p>
<p>参考：<br>NoSQL大数据分类<br><a href="http://www.nosql-database.org/" target="_blank" rel="external">http://www.nosql-database.org/</a><br>Autodesk基于Mesos的通用事件系统架构<br><a href="http://www.csdn.net/article/2015-08-27/2825550" target="_blank" rel="external">http://www.csdn.net/article/2015-08-27/2825550</a><br>QingCloud推出Spark即服务<br><a href="http://mt.sohu.com/20150826/n419752360.shtml" target="_blank" rel="external">http://mt.sohu.com/20150826/n419752360.shtml</a><br>Spark大数据分析框架的核心部件<br><a href="http://my.oschina.net/u/2306127/blog/489024?p=1" target="_blank" rel="external">http://my.oschina.net/u/2306127/blog/489024?p=1</a><br>Hadoop和大数据：60款顶级开源工具<br><a href="http://os.51cto.com/art/201508/487936.htm" target="_blank" rel="external">http://os.51cto.com/art/201508/487936.htm</a><br>【微信分享】QingCloud周小四：Spark学习简谈<br><a href="http://www.csdn.net/article/2015-08-07/2825404" target="_blank" rel="external">http://www.csdn.net/article/2015-08-07/2825404</a><br>【微信分享】李滔：搜狐基于Spark的新闻和广告推荐实战<br><a href="http://www.csdn.net/article/2015-07-31/2825353" target="_blank" rel="external">http://www.csdn.net/article/2015-07-31/2825353</a><br>【微信分享】王团结：七牛是如何搞定每天500亿条日志的<br><a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br>对七牛云存储日志处理的思考<br><a href="http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/" target="_blank" rel="external">http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/</a><br>STORM在线业务实践-集群空闲CPU飙高问题排查<br><a href="http://daiwa.ninja/index.php/2015/07/18/storm-cpu-overload/" target="_blank" rel="external">http://daiwa.ninja/index.php/2015/07/18/storm-cpu-overload/</a><br>Spark与Flink：对比与分析<br><a href="http://www.csdn.net/article/2015-07-16/2825232" target="_blank" rel="external">http://www.csdn.net/article/2015-07-16/2825232</a><br>一共81个，开源大数据处理工具汇总（上）<br><a href="http://www.36dsj.com/archives/24852" target="_blank" rel="external">http://www.36dsj.com/archives/24852</a><br>一共81个，开源大数据处理工具汇总（下）<br><a href="http://home.hylanda.com/show_26_11558.html" target="_blank" rel="external">http://home.hylanda.com/show_26_11558.html</a></p>
<p>总结：</p>
<pre><code><span class="bullet">1. </span>Cloudera和Hortonworks都开始注重数据管理和数据治理，Cloudera是通过增强Cloudera Navigator来实现，Hortonworks通过引入Informatic组件Fabric来实现。
<span class="bullet">2. </span>Spark 1.5发布；
<span class="bullet">3. </span>HBase、Cassandra是Column Families/Wide Column Store；
<span class="bullet">4. </span>MongoDB是Document Store；
<span class="bullet">5. </span>Redis是Key Value/Tuple Store；
<span class="bullet">6. </span>Neo4J是Graph Databases；
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>Cloudera：<br>Cloudera Navigator路线图<br><a href="http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-gov]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[SparkOnHBase(Cloudera)]]></title>
    <link href="http://navigating.github.io/2015/SparkOnHBase-Cloudera/"/>
    <id>http://navigating.github.io/2015/SparkOnHBase-Cloudera/</id>
    <published>2015-08-18T05:35:51.000Z</published>
    <updated>2015-08-18T05:46:25.533Z</updated>
    <content type="html"><![CDATA[<p>2014年2月4日，Cloudera宣布CDH支持Spark，在CDH 4.4中引入Spark 0.9。<br><a href="http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/" target="_blank" rel="external">http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/</a><br>在引入的时候强调了三点：</p>
<pre><code><span class="bullet">1. </span>Machine Learning
<span class="bullet">2. </span>Spark Streaming
<span class="bullet">3. </span>Faster Batch
</code></pre><p>2014年7月，在github上创建了Apache HBase与Spark的集成项目SparkOnHBase<br><a href="http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/" target="_blank" rel="external">http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/</a><br><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="external">https://github.com/cloudera-labs/SparkOnHBase</a><br>当前SparkOnHBase主要集中在这几个方面的功能改进：</p>
<pre><code>1. 在MR的map或者reduce阶段对HBase的全量访问(Full Access)；
2. 支持bulk <span class="operator"><span class="keyword">load</span>；
<span class="number">3.</span> 支持<span class="keyword">get</span>, put, <span class="keyword">delete</span>等bulk操作(bulk operation)；
<span class="number">4.</span> 支持成为<span class="keyword">SQL</span> <span class="keyword">engines</span>。</span>
</code></pre><p>2015年8月SparkOnHBase项目有了里程碑似的进展，被提交到HBase的主干(trunk)上，模块名为HBase-Spark Module，HBASE-13992 。<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br><a href="https://issues.apache.org/jira/browse/HBASE-13992" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-13992</a><br>HBase-Spark module相比于SparkOnHBase在架构上没有什么变化：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Cloudera_Spark_2015_01.png" alt="这是一张图片"><br>在具体实现上当前有三点改进：</p>
<pre><code><span class="bullet">1. </span>使用了全新的HBase 1.0+的API；
<span class="bullet">2. </span>从RDD和DStream functions操作HBase的直接支持；
<span class="bullet">3. </span>简化 foreach 和 map functions；
</code></pre><p>计划工作有两项：</p>
<pre><code><span class="bullet">1. </span>Spark-HBase Module支持bulkload；
<span class="bullet">2. </span>Spark-HBase Module支持Spark DataFrame DataSource；
</code></pre><p><a href="https://issues.apache.org/jira/browse/HBASE-14150" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-14150</a><br><a href="https://issues.apache.org/jira/browse/HBASE-14181" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-14181</a> </p>
<p>实际上集成Spark作为计算引擎的项目还有Hive和Pig：<br><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/spark.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/spark.html</a><br><a href="http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/</a><br><a href="http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/" target="_blank" rel="external">http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/</a> </p>
<p>参考：<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="external">https://github.com/cloudera-labs/SparkOnHBase</a><br><a href="http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>2014年2月4日，Cloudera宣布CDH支持Spark，在CDH 4.4中引入Spark 0.9。<br><a href="http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/" targ]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Cloudera" scheme="http://navigating.github.io/tags/Cloudera/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《Hadoop生态技术在阿里全网商品搜索实战》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8AHadoop%E7%94%9F%E6%80%81%E6%8A%80%E6%9C%AF%E5%9C%A8%E9%98%BF%E9%87%8C%E5%85%A8%E7%BD%91%E5%95%86%E5%93%81%E6%90%9C%E7%B4%A2%E5%AE%9E%E6%88%98%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《Hadoop生态技术在阿里全网商品搜索实战》/</id>
    <published>2015-08-17T04:41:00.000Z</published>
    <updated>2015-08-18T08:55:01.529Z</updated>
    <content type="html"><![CDATA[<p>资料参见文档：<a href="http://wenku.it168.com/d_001428550.shtml" target="_blank" rel="external">http://wenku.it168.com/d_001428550.shtml</a><br>版本:</p>
<pre><code><span class="bullet">1. </span>Hadoop: 基于 Hadoop 2.2 的阿里定制版
<span class="bullet">2. </span>HBase: 基于 HBase 0.94 的阿里定制版
</code></pre><p>部署方式：</p>
<pre><code><span class="bullet">1. </span>服务总数近1000台，分2个集群；
<span class="bullet">2. </span>Hadoop/HBase共同部署；
</code></pre><p>分析：服务器数量可能是2014年初的数据；HBase部署方式可能是RS和DN部署在同一个节点上。<br>服务器配置：</p>
<pre><code><span class="bullet">1. </span>CPU：24/32 Cores
<span class="bullet">2. </span>Memory：48G/96G
<span class="bullet">3. </span>Disk：12 <span class="bullet">* 1T SATA Disk 或者 12 *</span> 2T SATA Disk
</code></pre><p>分析：服务器配置计算能力比较强，内存和磁盘配置都不是很高。<br>大数据组件：</p>
<pre><code><span class="bullet">1. </span>HDFS + YARN
<span class="bullet">2. </span>HBase
<span class="bullet">3. </span>MR
<span class="bullet">4. </span>iStream
<span class="bullet">5. </span>Spark
<span class="bullet">6. </span>HQueue
<span class="bullet">7. </span>Phoenix
<span class="bullet">8. </span>OpenTSDB
<span class="bullet">9. </span>Zookeeper
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_01.JPG" alt="这是一张图片"><br>分析：</p>
<pre><code>* 对于基于HBase的HQueue是一个创新，当前没有看到更多的资料，无法和Kafka对比。(在性能和TPC上可能Kafka更强大，但通过对HBase的复用做出Queue，很赞。)
* iStream是一个Steaming <span class="function_start"><span class="keyword">on</span></span> YARN的产品，从架构上看很类似storm的设计理念。
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_02.JPG" alt="这是一张图片"><br>HBase<br>HBase应用</p>
<pre><code><span class="number">1</span>. <span class="function"><span class="title">Phoenix</span><span class="params">(SQL on HBase)</span></span>
<span class="number">2</span>. <span class="function"><span class="title">OpenTSDB</span><span class="params">(Metrics on HBase)</span></span>
<span class="number">3</span>. <span class="function"><span class="title">HQueue</span><span class="params">(Queue on HBase)</span></span>
</code></pre><p>分析：</p>
<pre><code><span class="bullet">* </span>在HBase集群上运行了Phoenix、OpenTSDB、HQueue三种应用，因此HBase具有作为一种数据存储的基础设施的能力。
</code></pre><p>HBase网页库存储方案</p>
<pre><code>1. 版本从0<span class="class">.25</span>、0<span class="class">.26</span>、0<span class="class">.90</span>、0<span class="class">.92</span>、0<span class="class">.94</span>、0<span class="class">.98</span>逐步升级的。
2. <span class="tag">HBase</span>集群规模从30多台持续升级到300多台。
3. <span class="tag">HBase</span> <span class="tag">Region</span>个数从 1<span class="tag">K</span> 增长到 20<span class="tag">K</span>。
4. 网页数量从 十亿 增长到 百亿。
</code></pre><p>存储业务数据的CF如下：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_05.JPG" alt="这是一张图片"><br>在HBase/Hadoop的I/O上的优化如下：</p>
<pre><code><span class="bullet">1. </span>Compression：Snappy/Gzip
<span class="bullet">2. </span>Block Encoding：Diff
<span class="bullet">3. </span>Block Size：64KB - 1MB
<span class="bullet">4. </span>Block Cache：InMemory
<span class="bullet">5. </span>Bloom Filter：ROW
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_06.JPG" alt="这是一张图片"><br>HBase Coprocessor应用<br>在网页库中使用了三种Coprocessor：</p>
<pre><code><span class="bullet">1. </span>Trace Coprocessor
<span class="bullet">2. </span>Clone Coprocessor
<span class="bullet">3. </span>Incremental Coprocessor
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_07.JPG" alt="这是一张图片"><br>分析：</p>
<pre><code><span class="keyword">*</span> 如果HBase集群就是两个集群中的一个，那么裸存储容量最大为：12 <span class="keyword">*</span> 2T <span class="keyword">*</span> 300 = 7200T = 7.2P，如果考虑到压缩、复制因子、数据冗余、容量冗余，可以存储有效数据约为：8P 数据。 
<span class="keyword">*</span> 平均每台服务器运行的Region个数：20K/300 = 67 个，这个数字比较符合HBase官方推荐的值。
<span class="keyword">*</span> Compression方法用了snappy和gzip两种。CF访问频繁，使用snappy，速度快；Raw CF访问较少，使用gzip，压缩比高。
<span class="keyword">*</span> Block Encoding使用Diff，0.98后改用PrefixTree；
<span class="keyword">*</span> Block Size的大小为 64KB - 1MB 
</code></pre><p>实时处理架构<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_08.JPG" alt="这是一张图片"><br>分析</p>
<pre><code><span class="subst">*</span> Metrics实时采集的流程大约是：HBase <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iStream <span class="subst">-&gt; </span>OpenTSDB <span class="keyword">on</span> HBase
<span class="subst">*</span> 流处理的全流程：HBase <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iStream <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iSearch/iStream
<span class="subst">*</span> 参见前文分析，猜测iStream是一个类似Storm的YARN框架。
</code></pre><p>关于阿里搜索自研的iStream的架构与文档参加如下：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_09.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_10.JPG" alt="这是一张图片"></p>
<p><a href="http://www.infoq.com/cn/news/2014/09/hadoop-alibaba-yarn" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/09/hadoop-alibaba-yarn</a><br><a href="http://club.alibabatech.org/resource_detail.htm?topicId=140" target="_blank" rel="external">http://club.alibabatech.org/resource_detail.htm?topicId=140</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>资料参见文档：<a href="http://wenku.it168.com/d_001428550.shtml" target="_blank" rel="external">http://wenku.it168.com/d_001428550.shtml</a><br>]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="HBase" scheme="http://navigating.github.io/tags/HBase/"/>
    
      <category term="HQueue" scheme="http://navigating.github.io/tags/HQueue/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="iStream" scheme="http://navigating.github.io/tags/iStream/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop发行版(2015第二季)]]></title>
    <link href="http://navigating.github.io/2015/Hadoop%E5%8F%91%E8%A1%8C%E7%89%88(2015%E7%AC%AC%E4%BA%8C%E5%AD%A3)/"/>
    <id>http://navigating.github.io/2015/Hadoop发行版(2015第二季)/</id>
    <published>2015-08-11T14:40:02.000Z</published>
    <updated>2015-08-11T14:56:06.872Z</updated>
    <content type="html"><![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" target="_blank" rel="external">http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support</a><br>其中在最有名为人所知的三家：<br>1.Cloudera<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_1.JPG" alt="这是一张图片"><br>2.Hortonwork<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_2.JPG" alt="这是一张图片"><br>3.MapR<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_3.JPG" alt="这是一张图片"><br>这三个厂商之中，MapR最为封闭；Hortonworks最为开放，产品线全开源，在线文档比较丰富。国内使用Cloudera CDH和Hortonworks的应该是最多的。<br>国内市场当前有两家也非常有竞争力，一家是Huawei，一家是星环科技。<br>4.Huawei FusionInsight<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_7.JPG" alt="这是一张图片"><br>5.星环科技TDH，TDH对Spark的支持据说非常不错的，有良好的性能表现。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_6.JPG" alt="这是一张图片"><br>准实时计算框架/即席查询<br>1.CDH的框架有：Impala + Spark；<br>2.HDP的框架有：Tez + Spark；<br>3.MapR的框架有：Drill + Tez + Spark。<br>关于Spark：<br>2014年大数据最热门的技术路线就是算是Spark了，而且得力于Spark不遗余力的推广和快速成长。Cloudera是最早支持Spark，也是最激进的。下图即是Spark在Cloudera产品线中的定位：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_4.JPG" alt="这是一张图片"><br>实际上基于Hadoop的快速计算框架的发展才刚刚开始，社区中已经有如下几种：<br>1.Spark/Shark<br>2.Hortonworks Tez/Stinger<br>3.Cloudera Impala<br>4.Apache Drill<br>5.Apache Flink<br>6.Apache Nifi<br>7.Facebook Presto</p>
<p>SQL on Hadoop<br>SQL on Hadoop的发展主要是传统的SQL过于强大，人才库非常庞大，从Hadoop出现的第一天就在SQL发力。当前技术路线上更是百花齐放，这里从开源和商业产品来说。<br>Open Source</p>
<pre><code><span class="bullet">1. </span>Apache Hive(Hive on MR)
<span class="bullet">2. </span>Hortonworks Tez/Stinger(Hive on Tez)
<span class="bullet">3. </span>Cloudera Impala
<span class="bullet">4. </span>Shark
<span class="bullet">5. </span>Spark SQL
<span class="bullet">6. </span>Apache Drill - MapR
<span class="bullet">7. </span>Facebook Presto
<span class="bullet">8. </span>Apache Phoenix(on HBase) - Saleforce
<span class="bullet">9. </span>Apache Kylin
<span class="bullet">10. </span>Apache Tajo - (Database Lab, Korea University)
<span class="bullet">11. </span>Cascading Lingual - (Cascading, Optiq)
<span class="bullet">12. </span>Dato (GraphLab) - Dato
</code></pre><p>Commercial</p>
<pre><code><span class="bullet">1. </span>EMC HAWQ
<span class="bullet">2. </span>IBM BigSQL
<span class="bullet">3. </span>TERADATA SQL-H
<span class="bullet">4. </span>Hadapt/HadoopDB
<span class="bullet">5. </span>Transwarp Inceptor
</code></pre><p>在开源领域里面，当前比受追捧的主要是：Hive、Impala、Spark、Phoenix。</p>
<p>参考：<br>SQL on Hadoop开源项目总结<br><a href="http://segmentfault.com/a/1190000002799235" target="_blank" rel="external">http://segmentfault.com/a/1190000002799235</a><br>如何选择满足需求的SQL on Hadoop系统<br><a href="http://www.searchbi.com.cn/showcontent_89816.htm" target="_blank" rel="external">http://www.searchbi.com.cn/showcontent_89816.htm</a><br>2015Hadoop技术峰会演讲速记3： 基于Transwarp Stream和Discover的实时大数据人流密度估计<br><a href="http://www.transwarp.cn/news/detail?id=70" target="_blank" rel="external">http://www.transwarp.cn/news/detail?id=70</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="SQL on Hadoop" scheme="http://navigating.github.io/tags/SQL-on-Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《七牛是如何搞定每天500亿条日志的》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8A%E4%B8%83%E7%89%9B%E6%98%AF%E5%A6%82%E4%BD%95%E6%90%9E%E5%AE%9A%E6%AF%8F%E5%A4%A9500%E4%BA%BF%E6%9D%A1%E6%97%A5%E5%BF%97%E7%9A%84%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《七牛是如何搞定每天500亿条日志的》/</id>
    <published>2015-08-10T06:48:00.000Z</published>
    <updated>2015-08-10T14:12:54.949Z</updated>
    <content type="html"><![CDATA[<p>七牛是如何搞定每天500亿条日志的 <a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/qiniu_01.jpg" alt=""><br>日志处理的大致分为三步：</p>
<pre><code><span class="bullet">1. </span>日志采集，主要是通过Agent和Flume；
<span class="bullet">2. </span>日志流转，主要是通过Kafka；
<span class="bullet">3. </span>日志计算，主要是通过Spark Streaming作为计算引擎；
</code></pre><p>大致的处理流程：</p>
<pre><code><span class="number">1.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>HDFS <span class="subst">-&gt; </span>mongoDB
<span class="number">2.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>Spark <span class="subst">-&gt; </span>mongoDB
<span class="number">3.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>Spark <span class="subst">-&gt; </span>opentsdb 
</code></pre><p>流程3只是见于图上，文字上没有任何提到。<br>在日志采集中，通过Agent将业务应用和日志采集进行了分离，采取了Agent主动来拉的模式。专门强调了Agent 的设计需求：<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">每台机器上会有一个Agent去同步这些日志，这是个典型的队列模型，业务进程在不断的push，Agent在不停的pop。Agent需要有记忆功能，用来保存同步的位置(<span class="command">offset</span>)，这样才尽可能保证数据准确性，但不可能做到完全准确。由于发送数据和保存<span class="command">offset</span>是两个动作，不具有事务性，不可避免的会出现数据不一致性情况，通常是发送成功后保存<span class="command">offset</span>，那么在Agent异常退出或机器断电时可能会造成多余的数据。</span><br><span class="line">在这里，Agent需要足够轻，这主要体现在运维和逻辑两个方面。Agent在每台机器上都会部署，运维成本、接入成本是需要考虑的。Agent不应该有解析日志、过滤、统计等动作，这些逻辑应该给数据消费者。倘若Agent有较多的逻辑，那它是不可完成的，不可避免的经常会有升级变更动作。</span><br></pre></td></tr></table></figure></p>
<p>为什么Agent没有直接将日志发送给Kafka，而是通过Flume来做：<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">具体架构上，Agent并没把数据直接发送到Kafka，在Kafka前面有层由Flume构成的<span class="keyword">forward</span>。这样做有两个原因：</span><br><span class="line"><span class="number">1</span>. Kafka的API对非JVM系的语言支持很不友好，<span class="keyword">forward</span>对外提供更加通用的http接口。</span><br><span class="line"><span class="number">2</span>. <span class="keyword">forward</span>层可以做路由、Kafka topic和Kafka partition key等逻辑，进一步减少Agent端的逻辑。</span><br></pre></td></tr></table></figure></p>
<p>Kafka使用建议<br>1.Topic划分。尽量通过划分Topic分离不同类型的数据；<br>2.Kafka partition数目直接关系整体的吞吐量。3个Partition能够跑满一块磁盘的IO。<br>3.Partition key设计。partition key选择不当，可能会造成数据倾斜。在对数据有顺序性要求才需使用partition key。Kafka的producer sdk在没指定partition key时，在一定时间内只会往一个partition写数据，这种情况下当producer数少于partition数也会造成数据倾斜，可以提高producer数目来解决这个问题。<br>实时计算Spark Streaming<br>1.当前Spark只用作统计，没有进行迭代计算(DAG)。场景比较简单。<br>2.Spark Streaming从Kafka中读数据，统计完结果如mongoDB。可以理解是Spark Streaming + mongoDB的应用。<br>3.Spark Streaming对存储计算结果的数据库tps要求较高。比如有10万个域名需要统计流量，batch interval为10s，每个域名有4个相关统计项，算下来平均是4万 tps，考虑到峰值可能更高，固态硬盘上的mongo也只能抗1万tps，后续我们会考虑用redis来抗这么高的tps。难道Redis能够支持很高的TPS？<br>4.有状态的Task的挑战：有外部状态的task逻辑上不可重入的，当开启speculation参数时候，可能会造成计算的结果不准确。说个简单的例子。这个任务，如果被重做了，会造成落入mongo的结果比实际多。有状态的对象生命周期不好管理，这种对象不可能做到每个task都去new一个。我们的策略是一个JVM内一个对象，同时在代码层面做好并发控制。<br>七牛数据平台规模<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">线上的规模：Flume ＋ Kafka ＋ Spark8台高配机器，日均500亿条数据，峰值80万tps。</span><br></pre></td></tr></table></figure></p>
<p>因此，<br>1.如果是Flume/Kafka/Spark共享同一个物理集群，硬件压力如何？<br>2.如果每条日志 0.1K，那么每天总数据量 50G <em> 0.1K = 5T，每个节点每秒 5T/24</em>3600/8 = 7.23M。 </p>
<p>参考：<br>【微信分享】王团结：七牛是如何搞定每天500亿条日志的<br><a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br>对七牛云存储日志处理的思考<br><a href="http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/" target="_blank" rel="external">http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>七牛是如何搞定每天500亿条日志的 <a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/201]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Flume" scheme="http://navigating.github.io/tags/Flume/"/>
    
      <category term="Kafka" scheme="http://navigating.github.io/tags/Kafka/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[学习《腾讯在Spark上的应用与实践优化》]]></title>
    <link href="http://navigating.github.io/2015/%E5%AD%A6%E4%B9%A0%E3%80%8A%E8%85%BE%E8%AE%AF%E5%9C%A8Spark%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5%E4%BC%98%E5%8C%96%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/学习《腾讯在Spark上的应用与实践优化》/</id>
    <published>2015-08-07T08:28:42.000Z</published>
    <updated>2015-08-07T08:37:53.504Z</updated>
    <content type="html"><![CDATA[<p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.net/detail/happytofly/8637461</a></p>
<p>TDW: Tencent Distributed Data Warehouse，腾讯分布式数据仓库；<br>GAIA：腾讯自研的基于YARN定制化和优化的资源管理系统；<br>Lhoste：腾讯自研的作业的工作流调度系统，类似于Oozie；<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_1.JPG" alt=""></p>
<p>TDW集群规模：</p>
<pre><code><span class="bullet">1. </span>Gaia集群节点数：8000+；
<span class="bullet">2. </span>HDFS的存储空间：150PB+；
<span class="bullet">3. </span>每天新增数据：1PB+；
<span class="bullet">4. </span>每天任务数：1M+；
<span class="bullet">5. </span>每天计算量：10PB+；
</code></pre><p>Spark集群：</p>
<pre><code><span class="bullet">1. </span>Spark部署在Gaia之上，即是Spark on YARN模式，每个节点是 24 cores 和 60G 内存；
<span class="bullet">2. </span>底层存储包括：HDFS、HBase、Hive、MySQL；
<span class="bullet">3. </span>作业类型，包括：ETL、SparkSQL、Machine Learning、Graph Compute、Streaming；
<span class="bullet">4. </span>每天任务数，10K+；
<span class="bullet">5. </span>腾讯从2013年开始引入Spark 0.6，已经使用2年了；
</code></pre><p>Spark的典型应用：</p>
<pre><code><span class="bullet">1. </span>预测用户的广告点击概率；
<span class="bullet">2. </span>计算两个好友间的共同好友数；
<span class="bullet">3. </span>用于ETL的SparkSQL和DAG任务；
</code></pre><p>Case 1: 预测用户的广告点击概率<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_4.JPG" alt=""></p>
<pre><code><span class="number">1</span>. 数据是通过<span class="function"><span class="title">DCT</span><span class="params">(Data Collect Tool)</span></span>推送到HDFS上，然后Spark直接将HDFS数据导入到 RDD&amp;Cache；
<span class="number">2</span>. <span class="number">60</span>次迭代计算的时间为<span class="number">10</span>～<span class="number">15</span>分钟，即每次迭代<span class="number">10</span>～<span class="number">15</span>秒；
</code></pre><p>Case 2: 计算两个好友间的共同好友数</p>
<pre><code>1. 根据shuffle数量来确定partition数量；
2. 尽量使用sort-based shuffle，减少reduce的内存使用；
3. 当连接超时后选择重试来减少executor丢失的概率；
4. 避免executor被YARN给<span class="operator"><span class="keyword">kill</span>掉，设置 spark.yarn.executor.memoryoverhead
<span class="number">5.</span> 执行语句 <span class="keyword">INSERT</span> <span class="keyword">TABLE</span> test_result <span class="keyword">SELECT</span> t3.d, <span class="keyword">COUNT</span>(*) FROＭ( <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> a, b <span class="keyword">FROM</span> join_1 ) t1 <span class="keyword">JOIN</span> （<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> b, c <span class="keyword">FROM</span> join_2 ) t2 <span class="keyword">ON</span> (t1.a = t2.c) <span class="keyword">JOIN</span> (<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c, d <span class="keyword">FROM</span> c, d <span class="keyword">FROM</span> join_3 ) t3 <span class="keyword">ON</span> (t2.b = t3.d) <span class="keyword">GROUP</span> <span class="keyword">BY</span> t3.d 使用Hive需要<span class="number">30</span>分钟，使用SparkSQL需要<span class="number">5</span>分钟；
<span class="number">6.</span> 当有小表时使用broadcase <span class="keyword">join</span>代替Common <span class="keyword">join</span>；
<span class="number">7.</span> 尽量使用ReduceByKey代替GroupByKey；
<span class="number">8.</span> 设置spark.serializer = org.apache.spark.serializer.KryoSerializer；
<span class="number">9.</span> 使用YARN时，设置spark.shuffle.service.enabled = <span class="literal">true</span>；
<span class="number">10.</span> 在早期版本中Spark通过启动参数固定executor的数量，当前支持动态资源扩缩容特性

    * spark.dynamicAllocation.enabled = <span class="literal">true</span>
    * spark.dynamicAllocation.executorIdleTimeout = <span class="number">120</span>
    * spark.dynamicAllocation.schedulerBacklogTimeout = <span class="number">10</span>
    * spark.dynamicAllocation.minExecutors/maxExecutors

<span class="number">11.</span> 当申请固定的executors时且task数大于executor数时，存在着资源的空闲状态。</span>
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_5.JPG" alt=""><br>&lt;完&gt;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[13~14年收集的大数据的一些技术架构图]]></title>
    <link href="http://navigating.github.io/2015/13-14%E5%B9%B4%E6%94%B6%E9%9B%86%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E5%9B%BE/"/>
    <id>http://navigating.github.io/2015/13-14年收集的大数据的一些技术架构图/</id>
    <published>2015-08-05T05:16:53.000Z</published>
    <updated>2015-08-05T05:38:56.350Z</updated>
    <content type="html"><![CDATA[<p>1 Big Data Solution</p>
<p>1.1 HP</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_01.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_02.png" alt=""></p>
<p>1.2 Oracle</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_03.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_04.JPG" alt=""></p>
<p>1.3 IBM</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_05.jpg" alt=""></p>
<p>1.4 Microsoft</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_06.png" alt=""></p>
<p>1.5 Huawei</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_07.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_08.jpg" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_09.jpg" alt=""></p>
<p>2 Big Data on Cloud</p>
<p>2.1 Amazon AWS</p>
<p>2.1.1 Netflix BigData on AWS<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_10.png" alt=""></p>
<p>2.2 Microsoft Azure</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_11.png" alt=""></p>
<p>2.3 Facebook</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_12.png" alt=""></p>
<p>2.4 Linkedin</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_13.png" alt=""></p>
<p>2.5 Twitter</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_14.png" alt=""></p>
<p>2.6 Alibaba/Taobao</p>
<p>2.6.1 淘宝数据魔方<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_15.png" alt=""></p>
<p>2.6.2 阿里大数据应用平台<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_16.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_17.png" alt=""></p>
<p>2.6.3 阿里搜索实时流计算<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_18.png" alt=""></p>
<p>2.7 Tencent</p>
<p>2.7.1 腾讯大规模Hadoop集群TDW<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_19.png" alt=""></p>
<p>2.7.2 腾讯实时计算平台 广点通<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_20.png" alt=""></p>
<p>2.8 JD(京东)</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_21.png" alt=""></p>
<p>2.9 CMCC(中国移动)</p>
<p>2.9.1 大云PaaS 2.5<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_22.png" alt=""></p>
<p>3 Hadoop Distribution</p>
<p>3.1 Apache Hadoop</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_23.png" alt=""></p>
<p>3.2 Cloudera</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_24.png" alt=""></p>
<p>3.3 Hortonworks</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_25.png" alt=""></p>
<p>3.4 MapR</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_26.png" alt=""></p>
<p>3.5 Intel</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_27.png" alt=""></p>
<p>3.6 EMC Pivotal HD</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_28.jpg" alt=""></p>
<p>3.7 IBM</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_29.jpg" alt=""></p>
<p>3.8 Huawei</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_30.jpg" alt=""></p>
<p>4 Landscape</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_31.jpg" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_32.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_33.png" alt=""></p>
<p>5 参考</p>
<ul>
<li><a href="http://www-01.ibm.com/software/data/bigdata/platform/resources.html" target="_blank" rel="external">IBM Report</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1E7OTT7&amp;ct=130225&amp;st=sb" target="_blank" rel="external">Gartner - Hadoop Is Not a Data Integration Solution</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1DBWMQY&amp;ct=121220&amp;st=sb" target="_blank" rel="external">Gartner - Magic Quadrant for Data Masking Technology 2012</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1IMDMZ5&amp;ct=130819&amp;st=sb" target="_blank" rel="external"> Magic Quadrant for Cloud Infrastructure as a Service 2013</a></li>
<li><a href="http://wenku.it168.com/d_000048434.shtml" target="_blank" rel="external">Facebook Hadoop</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>1 Big Data Solution</p>
<p>1.1 HP</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_0]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MapR" scheme="http://navigating.github.io/tags/MapR/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[读《微软研发制胜策略》]]></title>
    <link href="http://navigating.github.io/2015/%E8%AF%BB%E3%80%8A%E5%BE%AE%E8%BD%AF%E7%A0%94%E5%8F%91%E5%88%B6%E8%83%9C%E7%AD%96%E7%95%A5%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/读《微软研发制胜策略》/</id>
    <published>2015-08-04T14:30:45.000Z</published>
    <updated>2015-08-04T14:31:31.555Z</updated>
    <content type="html"><![CDATA[<p>软件开发的核心就是：达成项目目标，提高生产率，提高软件的质量。除此之外，都不要重要。<br>管理上、复用上，一切的核心就是人的问题，提高人的能力是第一生产力。</p>
<p>1.项目中一个现象就是紧紧的去控制进度，调整进度，进度的跟踪只是一种日常的事务工作。<br>2.观念的改变是第一位的，什么是观念改变的原则：规则不是法律，是可以触碰的。什么是我们要改变的规则，就是要有主动、计划、灵活。<br>3.紧密的进度计划，是一般的管理人员的通常做法，他的好处就是看到不断的工作，会有不断的压力；如果运用不当，就可能让人觉得厌烦和沮丧。<br>4.为了日程进度，牺牲质量往往是不值得的，除非你要一笑而过的做法。再不管这个项目的后续开发和维护了。对于产品或者项目的期限，要谨慎，要反思可能为了进度而牺牲质量。这叫着草率的期限。<br>5.一个好的日程表会兼顾公司和员工的利益的。<br>6.没有期限的目标不过是梦想而已。<br>7.把一个大项目，切分成n个小项目来做，每一个项目的周期大约是2个月。叫着阶段式的日程控制法。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>软件开发的核心就是：达成项目目标，提高生产率，提高软件的质量。除此之外，都不要重要。<br>管理上、复用上，一切的核心就是人的问题，提高人的能力是第一生产力。</p>
<p>1.项目中一个现象就是紧紧的去控制进度，调整进度，进度的跟踪只是一种日常的事务工作。<br>2.观念]]>
    </summary>
    
      <category term="读书" scheme="http://navigating.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201507]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201507/"/>
    <id>http://navigating.github.io/2015/大数据动态之201507/</id>
    <published>2015-07-31T08:22:01.000Z</published>
    <updated>2015-10-16T02:45:14.668Z</updated>
    <content type="html"><![CDATA[<p>Hortonworks<br>HDP 2.3发布：<br>HDP 2.3新增加组件Apache Atlas、Apache Calcite<br><a href="http://hortonworks.com/blog/available-now-hdp-2-3/" target="_blank" rel="external">http://hortonworks.com/blog/available-now-hdp-2-3/</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-2/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-2/</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/</a><br>Spark 1.2开始支持ORC(Columnar Formats)<br><a href="http://hortonworks.com/blog/bringing-orc-support-into-apache-spark/" target="_blank" rel="external">http://hortonworks.com/blog/bringing-orc-support-into-apache-spark/</a><br>Spark in HDInsight新特性一览<br><a href="http://hortonworks.com/blog/spark-in-hdinsight/" target="_blank" rel="external">http://hortonworks.com/blog/spark-in-hdinsight/</a> </p>
<p>Cloudera<br>HBase 1.0 开始支持Thrift客户端鉴权<br><a href="http://blog.cloudera.com/blog/2015/07/thrift-client-authentication-support-in-apache-hbase-1-0/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/thrift-client-authentication-support-in-apache-hbase-1-0/</a><br>Pig on MR优化<br><a href="http://blog.cloudera.com/blog/2015/07/how-to-tune-mapreduce-parallelism-in-apache-pig-jobs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/how-to-tune-mapreduce-parallelism-in-apache-pig-jobs/</a><br>Apache Zeppelin on CDH<br><a href="http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/</a><br>大数据欺诈检测架构<br><a href="http://blog.cloudera.com/blog/2015/07/designing-fraud-detection-architecture-that-works-like-your-brain-does/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/designing-fraud-detection-architecture-that-works-like-your-brain-does/</a> </p>
<p>MapR<br>YARN资源管理实践<br><a href="https://www.mapr.com/blog/best-practices-yarn-resource-management" target="_blank" rel="external">https://www.mapr.com/blog/best-practices-yarn-resource-management</a><br>Hive 1.0对Transaction的支持<br><a href="https://www.mapr.com/blog/hive-transaction-feature-hive-10" target="_blank" rel="external">https://www.mapr.com/blog/hive-transaction-feature-hive-10</a> </p>
<p>Databricks<br>Spark Streaming执行模型<br><a href="https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html</a><br>Spark 1.4 MLP新特性<br><a href="https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html</a><br>从Spark 1.2开始支持ORC<br><a href="https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/16/joint-blog-post-bringing-orc-support-into-apache-spark.html</a><br>从Spark 1.4开始支持窗口函数<br><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br>从Spark 1.4开始新的Web UI<br><a href="https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html" target="_blank" rel="external">https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html</a> </p>
<p>Phoenix对join的支持，TPC in Apache Phoenix<br><a href="https://blogs.apache.org/phoenix/entry/tpc_in_apache_phoenix" target="_blank" rel="external">https://blogs.apache.org/phoenix/entry/tpc_in_apache_phoenix</a> </p>
<p>Cassandra<br><a href="http://cassandra.apache.org/" target="_blank" rel="external">http://cassandra.apache.org/</a> </p>
<p>mongoDB<br><a href="https://www.mongodb.org/" target="_blank" rel="external">https://www.mongodb.org/</a> </p>
<p>Confluent<br>基于Kafka的实时流处理<br><a href="http://www.confluent.io/" target="_blank" rel="external">http://www.confluent.io/</a><br>大数据生态系统之Kafka价值<br><a href="http://www.confluent.io/blog/the-value-of-apache-kafka-in-big-data-ecosystem/" target="_blank" rel="external">http://www.confluent.io/blog/the-value-of-apache-kafka-in-big-data-ecosystem/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hortonworks<br>HDP 2.3发布：<br>HDP 2.3新增加组件Apache Atlas、Apache Calcite<br><a href="http://hortonworks.com/blog/available-now-hdp-2-3/" targ]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="Cassandra" scheme="http://navigating.github.io/tags/Cassandra/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="MongoDB" scheme="http://navigating.github.io/tags/MongoDB/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用Hexo搭建Github静态博客]]></title>
    <link href="http://navigating.github.io/2015/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BAGithub%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2/"/>
    <id>http://navigating.github.io/2015/使用Hexo搭建Github静态博客/</id>
    <published>2015-07-28T09:20:22.000Z</published>
    <updated>2015-09-18T15:20:42.160Z</updated>
    <content type="html"><![CDATA[<p>环境：</p>
<pre><code><span class="bullet">1. </span>Windows XP
<span class="bullet">2. </span>Git
</code></pre><p>步骤：</p>
<pre><code><span class="bullet">1. </span>安装Node.js
<span class="bullet">2. </span>安装Hexo
<span class="bullet">3. </span>创建博客(初始化Hexo)
<span class="bullet">4. </span>创建文章本地调试
<span class="bullet">5. </span>配置Github
<span class="bullet">6. </span>远程发布
<span class="bullet">7. </span>支持sitemap和feed
<span class="bullet">8. </span>支持百度统计
<span class="bullet">9. </span>支持图片
<span class="bullet">10. </span>支持Swiftype站内搜索
<span class="bullet">11. </span>参考资源
</code></pre><h2 id="安装Node-js">安装Node.js</h2><p>下载并安装，<a href="https://nodejs.org/" target="_blank" rel="external">https://nodejs.org/</a></p>
<h2 id="安装Hexo">安装Hexo</h2><p>通过命令 npm install -g hexo 安装</p>
<pre><code><span class="attribute">D</span>:\git\hexo&gt;npm install -g hexo

npm WARN optional dep failed, continuing fsevents<span class="variable">@0</span>.<span class="number">3.6</span>
npm WARN optional dep failed, continuing fsevents<span class="variable">@0</span>.<span class="number">3.6</span>
-


&gt; dtrace-provider<span class="variable">@0</span>.<span class="number">5.0</span> install <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo\node_modules\bunyan\node_modules\dtrace-provider
&gt; node scripts/install.js

<span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\hexo -&gt; <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo\bin\hexo
hexo<span class="variable">@3</span>.<span class="number">1.1</span> <span class="attribute">C</span>:\Users\stevenxu\AppData\Roaming\npm\node_modules\hexo
├── pretty-hrtime<span class="variable">@1</span>.<span class="number">0.0</span>
├── hexo-front-matter<span class="variable">@0</span>.<span class="number">2.2</span>
├── abbrev<span class="variable">@1</span>.<span class="number">0.7</span>
├── titlecase<span class="variable">@1</span>.<span class="number">0.2</span>
├── archy<span class="variable">@1</span>.<span class="number">0.0</span>
├── text-table<span class="variable">@0</span>.<span class="number">2.0</span>
├── tildify<span class="variable">@1</span>.<span class="number">1.0</span> (os-homedir<span class="variable">@1</span>.<span class="number">0.1</span>)
├── strip-indent<span class="variable">@1</span>.<span class="number">0.1</span> (get-stdin<span class="variable">@4</span>.<span class="number">0.1</span>)
├── hexo-i18n<span class="variable">@0</span>.<span class="number">2.1</span> (sprintf-js<span class="variable">@1</span>.<span class="number">0.3</span>)
├── chalk<span class="variable">@1</span>.<span class="number">1.0</span> (escape-string-regexp<span class="variable">@1</span>.<span class="number">0.3</span>, supports-color<span class="variable">@2</span>.<span class="number">0.0</span>, ansi-styles<span class="variable">@2</span>.<span class="number">1.0</span>, strip-ansi<span class="variable">@3</span>.<span class="number">0.0</span>, has-ansi<span class="variable">@2</span>.<span class="number">0.0</span>)
├── bluebird<span class="variable">@2</span>.<span class="number">9.34</span>
├── minimatch<span class="variable">@2</span>.<span class="number">0.10</span> (brace-expansion<span class="variable">@1</span>.<span class="number">1.0</span>)
├── through2<span class="variable">@1</span>.<span class="number">1.1</span> (xtend<span class="variable">@4</span>.<span class="number">0.0</span>, readable-stream<span class="variable">@1</span>.<span class="number">1.13</span>)
├── swig-extras<span class="variable">@0</span>.<span class="number">0.1</span> (markdown<span class="variable">@0</span>.<span class="number">5.0</span>)
├── hexo-fs<span class="variable">@0</span>.<span class="number">1.3</span> (escape-string-regexp<span class="variable">@1</span>.<span class="number">0.3</span>, graceful-fs<span class="variable">@3</span>.<span class="number">0.8</span>, chokidar<span class="variable">@0</span>.<span class="number">12.6</span>)
├── js-yaml<span class="variable">@3</span>.<span class="number">3.1</span> (esprima<span class="variable">@2</span>.<span class="number">2.0</span>, argparse<span class="variable">@1</span>.<span class="number">0.2</span>)
├── nunjucks<span class="variable">@1</span>.<span class="number">3.4</span> (optimist<span class="variable">@0</span>.<span class="number">6.1</span>, chokidar<span class="variable">@0</span>.<span class="number">12.6</span>)
├── warehouse<span class="variable">@1</span>.<span class="number">0.2</span> (graceful-fs<span class="variable">@3</span>.<span class="number">0.8</span>, cuid<span class="variable">@1</span>.<span class="number">2.5</span>, JSONStream<span class="variable">@0</span>.<span class="number">10.0</span>)
├── cheerio<span class="variable">@0</span>.<span class="number">19.0</span> (entities<span class="variable">@1</span>.<span class="number">1.1</span>, dom-serializer<span class="variable">@0</span>.<span class="number">1.0</span>, css-select<span class="variable">@1</span>.<span class="number">0.0</span>, htmlparser2<span class="variable">@3</span>.<span class="number">8.3</span>)
├── bunyan<span class="variable">@1</span>.<span class="number">4.0</span> (safe-json-stringify<span class="variable">@1</span>.<span class="number">0.3</span>, dtrace-provider<span class="variable">@0</span>.<span class="number">5.0</span>, mv<span class="variable">@2</span>.<span class="number">1.1</span>)

├── hexo-cli<span class="variable">@0</span>.<span class="number">1.7</span> (minimist<span class="variable">@1</span>.<span class="number">1.2</span>)
├── moment-timezone<span class="variable">@0</span>.<span class="number">3.1</span>
├── moment<span class="variable">@2</span>.<span class="number">10.3</span>
├── hexo-util<span class="variable">@0</span>.<span class="number">1.7</span> (ent<span class="variable">@2</span>.<span class="number">2.0</span>, highlight.js<span class="variable">@8</span>.<span class="number">6.0</span>)
├── swig<span class="variable">@1</span>.<span class="number">4.2</span> (optimist<span class="variable">@0</span>.<span class="number">6.1</span>, uglify-js<span class="variable">@2</span>.<span class="number">4.24</span>)
└── lodash<span class="variable">@3</span>.<span class="number">10.0</span>

<span class="attribute">D</span>:\git\hexo&gt;
</code></pre><h2 id="创建博客(初始化hexo)">创建博客(初始化hexo)</h2><p>创建博客站点的本地目录，然后在文件夹下执行命令：</p>
<pre><code><span class="variable">$ </span>hexo init
</code></pre><p>[info] Copying data<br>[info] You are almost done! Don’t forget to run <code>npm install</code> before you start b<br>logging with Hexo!</p>
<p>Hexo会自动在目标文件夹下建立网站所需要的文件。然后按照提示，安装node_modules，执行如下命令：</p>
<pre><code>$ hexo <span class="keyword">install</span>
</code></pre><h2 id="创建文章本地调试">创建文章本地调试</h2><p>预览本地调试模式，执行如下命令：</p>
<pre><code>$ hexo <span class="keyword">server</span>
</code></pre><p>[info] Hexo is running at <a href="http://localhost:4000/" target="_blank" rel="external">http://localhost:4000/</a>. Press Ctrl+C to stop.</p>
<p>关键命令简介：</p>
<pre><code><span class="title">hexo</span> n     <span class="comment">#创建新的文章</span>
hexo g     <span class="comment">#重新生成站点</span>
hexo s     <span class="comment">#启动本地服务</span>
hexo d     <span class="comment">#发布到github</span>
</code></pre><p>创建文章</p>
<pre><code>$ hexo <span class="keyword">new</span> <span class="string">"使用Hexo搭建Github静态博客"</span> 
</code></pre><p>在Hexo工作文件夹下source_posts发现新创建的md文件 使用Hexo搭建Github静态博客.md 。</p>
<h2 id="配置Github">配置Github</h2><p>部署到Github需要修改配置文件_config.yml文件，在Hexo工作目录之下：</p>
<pre><code># Deployment
## <span class="string">Docs:</span> <span class="string">http:</span><span class="comment">//hexo.io/docs/deployment.html</span>
<span class="label">
deploy:</span>
<span class="label">    type:</span> git
<span class="label">    repository:</span> git<span class="annotation">@github</span>.<span class="string">com:</span>&lt;Your Github Username&gt;/&lt;Your github.io url&gt;
<span class="label">    branch:</span> master
</code></pre><p>注意，当前type为git，而不是github</p>
<p>测试Github是否好用    </p>
<pre><code><span class="title">ssh</span> -T git<span class="variable">@github</span>.com
</code></pre><h2 id="远程发布">远程发布</h2><p>远程部署到Github，通过执行如下命令：    </p>
<pre><code><span class="variable">$ </span>hexi deploy
</code></pre><p>Troubleshooting<br>出现错误：Error: spawn git ENOENT<br>解决方案：<br><a href="http://blog.csdn.net/rainloving/article/details/46595559" target="_blank" rel="external">http://blog.csdn.net/rainloving/article/details/46595559</a> </p>
<p>使用github出现：fatal: unable to access: Failed connect to github.com:8080: No error<br>解决方案：<br><a href="http://www.zhihu.com/question/26954892" target="_blank" rel="external">http://www.zhihu.com/question/26954892</a> </p>
<p>使用github出现：ssh:connect to host github.com port 22: Bad file number<br>解决方案：<br><a href="http://www.xnbing.org/?p=759" target="_blank" rel="external">http://www.xnbing.org/?p=759</a><br><a href="http://blog.csdn.net/temotemo/article/details/7641883" target="_blank" rel="external">http://blog.csdn.net/temotemo/article/details/7641883</a> </p>
<h2 id="支持sitemap和feed">支持sitemap和feed</h2><p>首先安装sitemap和feed插件</p>
<pre><code>$ npm <span class="keyword">install</span> hexo-generator-sitemap
$ npm <span class="keyword">install</span> hexo-generator-feed
</code></pre><p>修改配置，在文件 _config.yml 增加以下内容</p>
<pre><code><span class="preprocessor"># Extensions</span>
<span class="label">Plugins:</span>
- hexo-generator-feed
- hexo-generator-sitemap

<span class="preprocessor">#Feed Atom</span>
<span class="label">feed:</span>
    type: atom
    path: atom.xml
    limit: <span class="number">20</span>

<span class="preprocessor">#sitemap</span>
<span class="label">sitemap:</span>
    path: sitemap.xml
</code></pre><p>在 themes\landscape_config.yml 中添加：</p>
<pre><code><span class="attribute">menu</span>:
    <span class="attribute">Home</span>: /
    <span class="attribute">Archives</span>: /archives
    <span class="attribute">Sitemap</span>: /sitemap.xml
<span class="attribute">rss</span>: /atom.xml
</code></pre><h2 id="支持百度统计">支持百度统计</h2><p>在 <a href="http://tongji.baidu.com" target="_blank" rel="external">http://tongji.baidu.com</a> 注册帐号，添加网站，生成统计功能的 JS 代码。</p>
<p>在 themes\landscape_config.yml 中新添加一行：</p>
<pre><code><span class="keyword">baidu_t</span>ongji: <span class="keyword">true</span>
</code></pre><p>在 themes\landscape\layout_partial\head.ejs 中head的结束标签  之前新添加一行代码</p>
<pre><code>&lt;<span class="preprocessor">%</span>- partial<span class="comment">('baidu_tongji')</span> <span class="preprocessor">%</span>&gt;
</code></pre><p>在 themes\landscape\layout_partial 中新创建一个文件 baidu_tongji.ejs 并添加如下内容：</p>
<pre><code><span class="xml"></span>&lt;%<span class="ruby"> <span class="keyword">if</span> (theme.baidu_tongji){ </span>%&gt;<span class="xml">
<span class="tag">&lt;<span class="title">script</span> <span class="attribute">type</span>=<span class="value">"text/javascript"</span>&gt;</span><span class="apache">
    <span class="tag">&lt;百度统计的 JS 代码&gt;</span>
</span><span class="tag">&lt;/<span class="title">script</span>&gt;</span>
</span>&lt;%<span class="ruby"> } </span>%&gt;<span class="xml"></span>
</code></pre><p>添加统计，参考：<br><a href="http://ibruce.info/2013/11/22/hexo-your-blog/" target="_blank" rel="external">http://ibruce.info/2013/11/22/hexo-your-blog/</a><br><a href="http://www.cnblogs.com/zhcncn/p/4097881.html" target="_blank" rel="external">http://www.cnblogs.com/zhcncn/p/4097881.html</a> </p>
<h2 id="支持图片">支持图片</h2><p>在source目录下创建images目录，然后将图片放在其中。<br>在文章中引用本地图片的语法例如：</p>
<pre><code>![<span class="link_label">这是一张图片</span>](<span class="link_url">/images/2005_TuoZhanXunLian.jpg</span>)
</code></pre><h2 id="支持swiftype站内搜索">支持swiftype站内搜索</h2><p>在 <a href="http://siftype.com" target="_blank" rel="external">http://siftype.com</a> 注册一个帐号，按着网站引导流程就可以了。<br>安装 install code 代码到hexo，添加到 themes\landscape\layout_partial\after-footer.ejs，类似添加百度统计的代码。<br>然后回到swiftype网站对 install code 进行确认。通过会在下方弹出一条消息：</p>
<pre><code><span class="title">Installation</span> successfully activated
</code></pre><h2 id="添加robots-txt">添加robots.txt</h2><p><a href="http://blog.lmintlcx.com/post/blog-with-hexo.html" target="_blank" rel="external">http://blog.lmintlcx.com/post/blog-with-hexo.html</a> </p>
<h2 id="支持多说评论">支持多说评论</h2><p>首先，在多说官网 <a href="http://www.duoshuo.com" target="_blank" rel="external">http://www.duoshuo.com</a> 激活账户，并添加网站配置；<br>其次，参考多说官网的Hexo配置指南进行配置，<a href="http://dev.duoshuo.com/threads/541d3b2b40b5abcd2e4df0e9" target="_blank" rel="external">http://dev.duoshuo.com/threads/541d3b2b40b5abcd2e4df0e9</a><br>针对landscap theme就是两步：<br>第一步在Hexo根目录下的配置文件 _config.yml 中添加多说配置项</p>
<pre><code><span class="attribute">duoshuo_shortname</span>: <span class="string">你站点的short_name</span>
</code></pre><p>第二步修改themes\landscape\layout_partial\article.ejs模板，用多说推荐的代码替换之前的Disqus代码。<br>将<br><figure class="highlight erb"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"></span>&lt;%<span class="ruby"> <span class="keyword">if</span> (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; </span>%&gt;<span class="xml"></span><br><span class="line"><span class="tag">&lt;<span class="title">section</span> <span class="attribute">id</span>=<span class="value">"comments"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">div</span> <span class="attribute">id</span>=<span class="value">"disqus_thread"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">noscript</span>&gt;</span>Please enable JavaScript to view the <span class="tag">&lt;<span class="title">a</span> <span class="attribute">href</span>=<span class="value">"//disqus.com/?ref_noscript"</span>&gt;</span>comments powered by Disqus.<span class="tag">&lt;/<span class="title">a</span>&gt;</span><span class="tag">&lt;/<span class="title">noscript</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">section</span>&gt;</span></span><br><span class="line"></span>&lt;%<span class="ruby"> &#125; </span>%&gt;<span class="xml"></span></span><br></pre></td></tr></table></figure></p>
<p>改为<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">%</span> <span class="attribute">if</span> (!<span class="attribute">index</span> &amp;&amp; <span class="attribute">post.comments</span> &amp;&amp; <span class="attribute">config.duoshuo_shortname</span>)&#123; %&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">section</span> <span class="attribute">id</span>=<span class="value">"comments"</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 多说评论框 start --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">div</span> <span class="attribute">class</span>=<span class="value">"ds-thread"</span> <span class="attribute">data-thread-key</span>=<span class="value">"&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;"</span> <span class="attribute">data-title</span>=<span class="value">"&lt;%= post.title %&gt;"</span> <span class="attribute">data-url</span>=<span class="value">"&lt;%= page.permalink %&gt;"</span>&gt;</span><span class="tag">&lt;/<span class="title">div</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 多说评论框 end --&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 多说公共JS代码 start (一个网页只需插入一次) --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">script</span> <span class="attribute">type</span>=<span class="value">"text/javascript"</span>&gt;</span><span class="javascript"></span><br><span class="line">  <span class="keyword">var</span> duoshuoQuery = &#123;short_name:<span class="string">'&lt;%= config.duoshuo_shortname %&gt;'</span>&#125;;</span><br><span class="line">    (<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> ds = <span class="built_in">document</span>.createElement(<span class="string">'script'</span>);</span><br><span class="line">      ds.type = <span class="string">'text/javascript'</span>;ds.async = <span class="literal">true</span>;</span><br><span class="line">      ds.src = (<span class="built_in">document</span>.location.protocol == <span class="string">'https:'</span> ? <span class="string">'https:'</span> : <span class="string">'http:'</span>) + <span class="string">'//static.duoshuo.com/embed.js'</span>;</span><br><span class="line">      ds.charset = <span class="string">'UTF-8'</span>;</span><br><span class="line">      (<span class="built_in">document</span>.getElementsByTagName(<span class="string">'head'</span>)[<span class="number">0</span>] </span><br><span class="line">       || <span class="built_in">document</span>.getElementsByTagName(<span class="string">'body'</span>)[<span class="number">0</span>]).appendChild(ds);</span><br><span class="line">    &#125;)();</span><br><span class="line">    </span><span class="tag">&lt;/<span class="title">script</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 多说公共JS代码 end --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">section</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">%</span> &#125; %&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="参考资源">参考资源</h2><p><a href="http://blog.lmintlcx.com/post/blog-with-hexo.html" target="_blank" rel="external">http://blog.lmintlcx.com/post/blog-with-hexo.html</a><br><a href="https://github.com/bruce-sha" target="_blank" rel="external">https://github.com/bruce-sha</a><br><a href="http://zipperary.com/2013/05/28/hexo-guide-2/" target="_blank" rel="external">http://zipperary.com/2013/05/28/hexo-guide-2/</a><br><a href="http://zipperary.com/2013/05/29/hexo-guide-3/" target="_blank" rel="external">http://zipperary.com/2013/05/29/hexo-guide-3/</a><br><a href="http://zipperary.com/2013/05/30/hexo-guide-4/" target="_blank" rel="external">http://zipperary.com/2013/05/30/hexo-guide-4/</a><br><a href="http://cnfeat.com/2014/05/10/2014-05-11-how-to-build-a-blog/" target="_blank" rel="external">http://cnfeat.com/2014/05/10/2014-05-11-how-to-build-a-blog/</a><br><a href="http://www.cnblogs.com/zhcncn/p/4097881.html" target="_blank" rel="external">http://www.cnblogs.com/zhcncn/p/4097881.html</a><br><a href="http://blog.moyizhou.cn/web/search-engine-for-static-pages/" target="_blank" rel="external">http://blog.moyizhou.cn/web/search-engine-for-static-pages/</a><br><a href="http://www.jerryfu.net/post/search-engine-for-hexo-with-swiftype.html" target="_blank" rel="external">http://www.jerryfu.net/post/search-engine-for-hexo-with-swiftype.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>环境：</p>
<pre><code><span class="bullet">1. </span>Windows XP
<span class="bullet">2. </span>Git
</code></pre><p>步骤：</p>
<pre><code><span ]]>
    </summary>
    
      <category term="blog" scheme="http://navigating.github.io/tags/blog/"/>
    
      <category term="github" scheme="http://navigating.github.io/tags/github/"/>
    
      <category term="hexo" scheme="http://navigating.github.io/tags/hexo/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://navigating.github.io/2015/hello-world/"/>
    <id>http://navigating.github.io/2015/hello-world/</id>
    <published>2015-07-27T09:20:22.000Z</published>
    <updated>2015-07-28T09:21:58.301Z</updated>
    <content type="html"><![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io]]>
    </summary>
    
      <category term="hexo" scheme="http://navigating.github.io/tags/hexo/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop 2.7.1 发布]]></title>
    <link href="http://navigating.github.io/2015/Hadoop-2-7-1-%E5%8F%91%E5%B8%83/"/>
    <id>http://navigating.github.io/2015/Hadoop-2-7-1-发布/</id>
    <published>2015-07-09T13:49:30.000Z</published>
    <updated>2015-07-30T13:50:50.764Z</updated>
    <content type="html"><![CDATA[<p>2015年7月6日，Apache Hadoop的稳定版本 2.7.1 正式发布。<br><a href="http://hadoop.apache.org/releases.html#Release+Notes" target="_blank" rel="external">http://hadoop.apache.org/releases.html#Release+Notes</a> </p>
<p>Hadoop 2.7的一个小版本发布了，本版本属于稳定版本。<br>修复了2.7.0中存在的131个bug。<br>这是2.7.x第一个稳定版本，增强的功能列表请通过2.7.0版本部分查看。<br>按着计划，下一个2.7.x的小版本是2.7.2.</p>
<p>原文：<br>06 July, 2015: Release 2.7.1 (stable) availableA point release for the 2.7 line. This release is now considered stable.<br>Please see the Hadoop 2.7.1 Release Notes for the list of 131 bug fixes and patches since the previous release 2.7.0. Please look at the 2.7.0 section below for the list of enhancements enabled by this first stable release of 2.7.x.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>2015年7月6日，Apache Hadoop的稳定版本 2.7.1 正式发布。<br><a href="http://hadoop.apache.org/releases.html#Release+Notes" target="_blank" rel="external"]]>
    </summary>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[读《Deploying Apache Kafka: A Practical FAQ》]]></title>
    <link href="http://navigating.github.io/2015/%E8%AF%BB%E3%80%8ADeploying-Apache-Kafka-A-Practical-FAQ%E3%80%8B/"/>
    <id>http://navigating.github.io/2015/读《Deploying-Apache-Kafka-A-Practical-FAQ》/</id>
    <published>2015-07-02T14:57:45.000Z</published>
    <updated>2015-07-30T15:01:55.553Z</updated>
    <content type="html"><![CDATA[<p>Cloudera发布了Kafka的好文，《Deploying Apache Kafka: A Practical FAQ》，参见：<a href="http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-practical-faq" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-practical-faq</a></p>
<p>是否应当为Kafka Broker使用 固态硬盘 (SSD)<br>实际上使用SSD盘并不能显著地改善 Kafka 的性能，主要有两个原因：</p>
<pre><code>* Kafka写磁盘是异步的，不是同步的。就是说，除了启动、停止之外，Kafka的任何操作都不会去等待磁盘同步（sync）完成；而磁盘同步(disk syncs)总是在后台完成的。这就是为什么Kafka消息至少复制到三个副本是至关重要的，因为一旦单个副本崩溃，这个副本就会丢失数据无法同步写到磁盘。
* 每一个Kafka <span class="keyword">Partition</span>被存储为一个串行的WAL（<span class="keyword">Write</span> Ahead <span class="keyword">Log</span>）日志文件。因此，除了极少数的数据查询，Kafka中的磁盘读写都是串行的。现代的操作系统已经对串行读写做了大量的优化工作。
</code></pre><p>如何对Kafka Broker上持久化的数据进行加密<br>目前，Kafka不提供任何机制对Broker上持久化的数据进行加密。用户可以自己对写入到Kafka的数据进行加密，即是，生产者(Producers)在写Kafka之前加密数据，消费者(Consumers)能解密收到的消息。这就要求生产者(Producers)把加密协议(protocols)和密钥(keys)分享给消费者(Consumers)。<br>另外一种选择，就是使用软件提供的文件系统级别的加密，例如Cloudera Navigator Encrypt。Cloudera Navigator Encrypt是Cloudera企业版(Cloudera Enterprise)的一部分，在应用程序和文件系统之间提供了一个透明的加密层。<br>Apache Zookeeper正成为Kafka集群的一个痛点(pain point)，真的吗？<br>Kafka高级消费者(high-level consumer)的早期版本(0.8.1或更早)使用Zookeeper来维护读的偏移量(offsets，主要是Topic的每个Partition的读偏移量)。如果有大量生产者(consumers)同时从Kafka中读数据，对Kafka的读写负载可能就会超出它的容量，Zookeeper就变成一个瓶颈(bottleneck)。当然，这仅仅出现在一些很极端的案例中(extreme cases)，即有成百上千个消费者(consumers)在使用同一个Zookeeper集群来管理偏移量(offset)。<br>不过，这个问题已经在Kafka当前的版本(0.8.2)中解决。从版本0.8.2开始，高级消费者(high-level consumer)能够使用Kafka自己来管理偏移量(offsets)。本质上讲，它使用一个单独的Kafka Topic来管理最近的读偏移量(read offsets)，因此偏移量管理(offset management)不再要求Zookeeper必须存在。然后，用户将不得不面临选择是用Kafka还是Zookeeper来管理偏移量(offsets)，由消费者(consumer)配置参数 offsets.storage 决定。<br>Cloudera强烈推荐使用Kafka来存储偏移量。当然，为了保证向后兼容性，你可以继续选择使用Zookeeper存储偏移量。(例如，你可能有一个监控平台需要从Zookeeper中读取偏移量信息。) 假如你不得不使用Zookeeper进行偏移量(offset)管理，我们推荐你为Kafka集群使用一个专用的Zookeeper集群。假如一个专用的Zookeeper集群仍然有性能瓶颈，你依然可以通过在Zookeeper节点上使用固态硬盘(SSD)来解决问题。<br>Kafka是否支持跨数据中心的可用性<br>Kafka跨数据中心可用性的推荐解决方案是使用MirrorMaker(<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330" target="_blank" rel="external">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330</a> ) 。在你的每一个数据中心都搭建一个Kafka集群，在Kafka集群之间使用MirrorMaker来完成近实时的数据复制。<br>使用MirrorMaker的架构模式是为每一个”逻辑”的topic在每一个数据中心创建一个topic：例如，在逻辑上你有一个”clicks”的topic，那么你实际上有”DC1.clicks”和“DC2.clicks”两个topic(DC1和DC2指得是你的数据中心)。DC1向DC1.clicks中写数据，DC2向DC2.clicks中写数据。MirrorMaker将复制所有的DC1 topics到DC2，并且复制所有的DC2 topics到DC1。现在每个DC上的应用程序都能够访问写入到两个DC的事件。这个应用程序能够合并信息和处理相应的冲突。<br>另一种更复杂的模式是在每一个DC都搭建本地和聚合Kafka集群。这个模式已经被Linkedin使用，Linkedin Kafka运维团队已经在这篇Blog(<a href="https://engineering.linkedin.com/kafka/running-kafka-scale" target="_blank" rel="external">https://engineering.linkedin.com/kafka/running-kafka-scale</a> )中有详细的描述(参见“Tiers and Aggregation”)。<br>Kafka支持哪些类型的数据转换(data transformation)<br>数据流过的Kafka的时候，Kafka并不能进行数据转换。为了处理数据转换，我们推荐如下方法：</p>
<pre><code>* 对于简单事件处理，使用<span class="constant">Flume Kafka </span>integration(<span class="symbol">http:</span>/<span class="regexp">/blog.cloudera.com/blog</span><span class="regexp">/2014/</span><span class="number">11</span>/flafka-apache-flume-meets-apache-kafka-<span class="keyword">for</span>-event-processing )，并且写一个简单的<span class="constant">Apache Flume Interceptor。</span>
* 对于复杂(事件)处理，使用<span class="constant">Apache Spark Streaming从Kafka中</span>读数据和处理数据。
</code></pre><p>在这两种情况下，被转换或者处理的数据可被写会到新的Kafka Topic中，或者直接传送到数据的最终消费者(Consumer)那里。<br>对于实时事件处理模式更全面的描述，看看这篇文章(<a href="http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/</a> )。<br>如何通过Kafka发送大消息或者超大负荷量？<br>Cloudera的性能测试表明Kafka达到最大吞吐量的消息大小为10K左右。更大的消息将导致吞吐量下降。然后，在一些情况下，用户需要发送比10K大的多的消息。<br>如果消息负荷大小是每100s处理MB级别，我们推荐探索以下选择：</p>
<pre><code><span class="bullet">* </span>如果可以使用共享存储(HDFS、S3、NAS)，那么将超负载放在共享存储上，仅用Kafka发送负载数据位置的消息。
<span class="bullet">* </span>对于大消息，在写入Kafka之前将消息拆分成更小的部分，使用消息Key确保所有的拆分部分都写入到同一个partition中，以便于它们能被同一个消息着(Consumer)消费的到，在消费的时候将拆分部分重新组装成一个大消息。
</code></pre><p>在通过Kafka发送大消息时，请记住以下几点：<br>压缩配置</p>
<pre><code><span class="keyword">*</span> Kafka生产者(Producers)能够压缩消息。通过配置参数compression.codec确保压缩已经开启。有效的选项为<span class="string">"gzip"</span>和<span class="string">"snappy"</span>。
</code></pre><p>Broker配置</p>
<pre><code>* message.<span class="built_in">max</span>.<span class="keyword">bytes</span> (default: <span class="number">1000000</span>): Broker能够接受的最大消息。增加这个值以便于匹配你的最大消息。
* <span class="built_in">log</span>.<span class="keyword">segment</span>.<span class="keyword">bytes</span> (default: <span class="number">1</span>GB): Kafka数据文件的大小。确保它至少大于一条消息。默认情况下已经够用，一般最大的消息不会超过<span class="number">1</span>G大小。
* replica.fetch.<span class="built_in">max</span>.<span class="keyword">bytes</span> (default: <span class="number">1</span>MB): Broker间复制的最大的数据大小。这个值必须大于message.<span class="built_in">max</span>.<span class="keyword">bytes</span>，否则一个Broker接受到消息但是会复制失败，从而导致潜在的数据丢失。
</code></pre><p>Consumer配置</p>
<pre><code>* <span class="tag">fetch</span><span class="class">.message</span><span class="class">.max</span><span class="class">.bytes</span> (<span class="rule"><span class="attribute">default</span>:<span class="value"> <span class="number">1</span>MB): Consumer所读消息的最大大小。这个值应该大于或者等于Broker配置的message.max.bytes的值。</span></span>
</code></pre><p>其他方面的考虑：</p>
<pre><code>* <span class="tag">Broker</span>需要针对复制为每一个<span class="tag">partition</span>分配一个<span class="tag">replica</span><span class="class">.fetch</span><span class="class">.max</span><span class="class">.bytes</span>大小的缓存区。需要计算确认( <span class="tag">partition</span>的数量 * 最大消息的大小 )不会超过可用的内存，否则就会引发<span class="tag">OOMs</span>（内存溢出异常）。
* <span class="tag">Consumers</span>有同样的问题，因子参数为 <span class="tag">fetch</span><span class="class">.message</span><span class="class">.max</span><span class="class">.bytes</span> ：确认每一个<span class="tag">partition</span>的消费者针对最大的消息有足够可用的内存。
* 大消息可能引发更长时间的垃圾回收停顿(<span class="tag">garbage</span> <span class="tag">collection</span> <span class="tag">pauses</span>)(<span class="tag">brokers</span>需要申请更大块的内存)。注意观察<span class="tag">GC</span>日志和服务器日志。假如发现长时间的<span class="tag">GC</span>停顿导致<span class="tag">Kafka</span>丢失了<span class="tag">Zookeeper</span> <span class="tag">session</span>，你可能需要为<span class="tag">zookeeper</span><span class="class">.session</span><span class="class">.timeout</span><span class="class">.ms</span>配置更长的<span class="tag">timeout</span>值。
</code></pre><p>Kafka是否支持MQTT或JMS协议<br>目前，Kafka针对上述协议不提供直接支持。但是，用户可以自己编写Adaptors从MQTT或者JMS中读取数据，然后写入到Kafka中。</p>
<p>更多关于在CDH中使用Kafka的信息，下载Deployment Guide(<a href="http://www.cloudera.com/content/cloudera/en/resources/library/datasheet/kafka-reference-architecture.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/resources/library/datasheet/kafka-reference-architecture.html</a> ) 或者 观看webinar “Bringing Real-Time Data to Hadoop”(<a href="http://www.cloudera.com/content/cloudera/en/resources/library/recordedwebinar/kafka-webinar-recording.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/resources/library/recordedwebinar/kafka-webinar-recording.html</a> )。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Cloudera发布了Kafka的好文，《Deploying Apache Kafka: A Practical FAQ》，参见：<a href="http://blog.cloudera.com/blog/2015/07/deploying-apache-kafka-a-]]>
    </summary>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="Kafka" scheme="http://navigating.github.io/tags/Kafka/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[大数据动态之201506]]></title>
    <link href="http://navigating.github.io/2015/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8A%A8%E6%80%81%E4%B9%8B201506/"/>
    <id>http://navigating.github.io/2015/大数据动态之201506/</id>
    <published>2015-06-09T13:52:23.000Z</published>
    <updated>2015-08-10T06:25:16.483Z</updated>
    <content type="html"><![CDATA[<p>Pinot：LinkedIn的实时数据分析系统<br><a href="http://www.infoq.com/cn/news/2014/10/linkdln" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/10/linkdln</a><br><a href="https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot" target="_blank" rel="external">https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot</a></p>
<p>Twitter Heron：Twitter发布新的大数据实时分析系统Heron<br><a href="http://geek.csdn.net/news/detail/33750" target="_blank" rel="external">http://geek.csdn.net/news/detail/33750</a><br><a href="http://www.longda.us/?p=529" target="_blank" rel="external">http://www.longda.us/?p=529</a> </p>
<p>Cloudera<br>HBase对MOBs( Moderate Objects, 主要是大小100K到10M的对象存储 )的支持<br><a href="http://blog.cloudera.com/blog/2015/06/inside-apache-hbases-new-support-for-mobs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/inside-apache-hbases-new-support-for-mobs/</a><br>准实时计算架构模式<br><a href="http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/</a><br>(翻译：<a href="http://zhuanlan.zhihu.com/donglaoshi/20082628" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20082628</a> )<br>CDH 5.4 新功能：敏感数据处理(Sensitive Data Redaction)<br><a href="http://blog.cloudera.com/blog/2015/06/new-in-cdh-5-4-sensitive-data-redaction/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/06/new-in-cdh-5-4-sensitive-data-redaction/</a> </p>
<p>Hortonworks<br>YARN的CapacityScheduler对Resource-preemption的支持<br><a href="http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/" target="_blank" rel="external">http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/</a><br>Hadoop集群对Multihoming的支持<br><a href="http://hortonworks.com/blog/multihoming-on-hadoop-yarn-clusters/" target="_blank" rel="external">http://hortonworks.com/blog/multihoming-on-hadoop-yarn-clusters/</a><br>HDP 2.3企业级HDFS数据加密<br><a href="http://hortonworks.com/blog/new-in-hdp-2-3-enterprise-grade-hdfs-data-at-rest-encryption/" target="_blank" rel="external">http://hortonworks.com/blog/new-in-hdp-2-3-enterprise-grade-hdfs-data-at-rest-encryption/</a><br>Apache Slider 0.80.0版本发布<br><a href="http://hortonworks.com/blog/announcing-apache-slider-0-80-0/" target="_blank" rel="external">http://hortonworks.com/blog/announcing-apache-slider-0-80-0/</a><br>Apache Spark 1.3.1 on HDP 2.2<br><a href="http://hortonworks.com/blog/apache-spark-on-hdp-learn-try-and-do/" target="_blank" rel="external">http://hortonworks.com/blog/apache-spark-on-hdp-learn-try-and-do/</a><br><a href="http://hortonworks.com/hadoop-tutorial/using-apache-spark-technical-preview-with-hdp-2-2/" target="_blank" rel="external">http://hortonworks.com/hadoop-tutorial/using-apache-spark-technical-preview-with-hdp-2-2/</a><br>Ambari 2.0.1 和 HDP 2.2.6 发布<br><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.6/bk_HDP_RelNotes/content/ch_relnotes_v226.html</a><br><a href="http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_releasenotes_ambari_2.0.1.0/content/ch_relnotes-ambari-2.0.1.0.html" target="_blank" rel="external">http://docs.hortonworks.com/HDPDocuments/Ambari-2.0.1.0/bk_releasenotes_ambari_2.0.1.0/content/ch_relnotes-ambari-2.0.1.0.html</a></p>
<p>其他：<br>Graphite的百万Metrics实践之路<br><a href="http://calvin1978.blogcn.com/articles/graphite.html" target="_blank" rel="external">http://calvin1978.blogcn.com/articles/graphite.html</a><br>HBaseCon 2015 大会幻灯片 &amp; 视频<br><a href="http://hbasecon.com/archive.html" target="_blank" rel="external">http://hbasecon.com/archive.html</a><br>HBase在腾讯大数据的应用实践<br><a href="http://www.d1net.com/bigdata/news/353500.html" target="_blank" rel="external">http://www.d1net.com/bigdata/news/353500.html</a><br>从Spark到Hadoop的架构实践<br><a href="http://www.csdn.net/article/2015-06-08/2824889" target="_blank" rel="external">http://www.csdn.net/article/2015-06-08/2824889</a><br>56网大数据<br><a href="http://share.csdn.net/slides/10903" target="_blank" rel="external">http://share.csdn.net/slides/10903</a><br>七牛技术总监陈超：记Spark Summit China 2015<br><a href="http://www.csdn.net/article/2015-04-30/2824594-spark-summit-china-2015" target="_blank" rel="external">http://www.csdn.net/article/2015-04-30/2824594-spark-summit-china-2015</a><br>唯品会美研中心郭安琪：2015 Hadoop Summit见闻<br><a href="http://zhuanlan.zhihu.com/donglaoshi/20072576" target="_blank" rel="external">http://zhuanlan.zhihu.com/donglaoshi/20072576</a><br>华为叶琪：论Spark Streaming的数据可靠性和一致性<br><a href="http://www.csdn.net/article/2015-06-12/2824938" target="_blank" rel="external">http://www.csdn.net/article/2015-06-12/2824938</a><br>Hadoop Summit 2015<br><a href="http://2015.hadoopsummit.org/san-jose/agenda/" target="_blank" rel="external">http://2015.hadoopsummit.org/san-jose/agenda/</a><br>Spark Summit 2015<br><a href="https://spark-summit.org/2015/" target="_blank" rel="external">https://spark-summit.org/2015/</a> </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Pinot：LinkedIn的实时数据分析系统<br><a href="http://www.infoq.com/cn/news/2014/10/linkdln" target="_blank" rel="external">http://www.infoq.com/cn/]]>
    </summary>
    
      <category term="BigData" scheme="http://navigating.github.io/tags/BigData/"/>
    
      <category term="CDH" scheme="http://navigating.github.io/tags/CDH/"/>
    
      <category term="HDP" scheme="http://navigating.github.io/tags/HDP/"/>
    
      <category term="Hadoop" scheme="http://navigating.github.io/tags/Hadoop/"/>
    
      <category term="Spark" scheme="http://navigating.github.io/tags/Spark/"/>
    
      <category term="技术" scheme="http://navigating.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
</feed>