<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>On The Open Way</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="On The Open Way">
<meta property="og:url" content="http://navigating.github.io/index.html">
<meta property="og:site_name" content="On The Open Way">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On The Open Way">
<meta name="twitter:description">
<meta name="twitter:creator" content="@xujinghui">
  
    <link rel="alternative" href="/atom.xml" title="On The Open Way" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  
<script type="text/javascript">
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?5554b1765137f9e84f6b5d1958c4bdfd";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  

  <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','W7_-_9WqoJq6hWaJJ8zZ','2.0.0');
</script>
</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">On The Open Way</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">自信人生二百年，会当水击三千里！</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/sitemap.xml">Sitemap</a>
        
      </nav>
      <nav id="sub-nav">	    
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>		
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://navigating.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-2015/Oozie：入门概述" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/Oozie：入门概述/" class="article-date">
  <time datetime="2015-09-18T15:24:54.000Z" itemprop="datePublished">2015-09-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/Oozie：入门概述/">Oozie：入门概述</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Oozie能做什么(What Oozie Does)<br>Oozie是一个Java Web应用，用于Apache Hadoop的任务(jobs)调度。Oozie顺序的合并多个任务(jobs)成为一个可工作的逻辑单元。其主要是集成了Hadoop技术栈，包括YARN等，支持Apache MapReduce, Apache Pig, Apache Hive, Apache Sqoop等。Oozie使得用户能够通过Java应用或者Shell脚本的方式调度任务。<br>Oozie任务有两种基本类型</p>
<pre><code>* Oozie Workflow jobs：这种任务是一个有向无环图(DAG, <span class="keyword">Direct</span> Acyclical Graph)，并按着规则顺序的执行，即上一个<span class="keyword">Action</span>运行完成后才能运行下一个<span class="keyword">Action</span>。所以其经常不得不等待。
* Oozie Coordinator jobs：这种任务是重复性的工作流，一般被时间或者数据达到可用会被触发。
</code></pre><p>Oozie Bundle提供一个复合的方式，将多个Workflow jobs和Coordinator jobs打包合并在一起并能对它们的生命周期进行管理。<br>Oozie如何工作(How Oozie works)<br>一个Oozie Workflow是一系列编排成有向无环图（DAG）的Action集合。控制节点定义job时间,设置开始和结束workflow的规则(rules)。这样，Oozie通过decision，fork和join节点控制工作流的执行路径。Action节点触发任务的执行。<br>Oozie触发工作流的action操作，实际上由Hadoop MapReduce去执行。Oozie利用Hadoop技术栈来均衡负载和处理失败。<br>Oozie通过回调(callback)和轮询(polling)来检测任务的是否完成。当Oozie开始一个任务(task)，它的提供了一个唯一的可以回调的HTTP URL，当这个任务完成的时候就通知这个URL。如果任务失败就调用回调URL，Oozie可以设置任务完成。<br>经常有这种需求，在规则的时间间隔内运行Oozie workflow，处理那些无法预期的有效数据或者时间。在这些情况下，Oozie Coordinator允许你根据、时间或者事件的条件对工作流触发的时机进行建模。在这些条件得到满足之后，工作流任务就就开始启动。<br>Oozie Coordinator也可以管理多个工作流，是依赖于子工作流的输出结果。子工作流程的输出将会成为下一个工作流的输入。这条链被称为“数据应用管道”(data application pipeline)。<br>工作流定义<br>定义一个Oozie工作流，两个配置文件是必须的，job.properties和workflow.xml。<br>job.properties的环境变量如下：<br>nameNodehdfs://mycluster:8020HDFS地址jobTrackerlocalhost:8034jobTracker地址queueNamedefaultOozie队列examplesRootexamples全局目录oozie.usr.system.libpathtrue是否加载用户的lib库oozie.libpathshare/lib/user用户lib库oozie.wf.application.path${nameNode}/user/${user.name}/Oozie流程所在的HDFS地址</p>
<p>workflow.xml示例如下：<br>&lt;!—<br>  Licensed to the Apache Software Foundation (ASF) under one<br>  or more contributor license agreements.  See the NOTICE file<br>  distributed with this work for additional information<br>  regarding copyright ownership.  The ASF licenses this file<br>  to you under the Apache License, Version 2.0 (the<br>  “License”); you may not use this file except in compliance<br>  with the License.  You may obtain a copy of the License at</p>
<pre><code><span class="label">http:</span>//www.apache<span class="preprocessor">.org</span>/licenses/LICENSE-<span class="number">2.0</span>
</code></pre><p>  Unless required by applicable law or agreed to in writing, software<br>  distributed under the License is distributed on an “AS IS” BASIS,<br>  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>  See the License for the specific language governing permissions and<br>  limitations under the License.<br>—&gt;</p>
<p><workflow-app xmlns="uri:oozie:workflow:0.2" name="map-reduce-wf"><br>    <start to="mr-node"><br>    <action name="mr-node"><br>        <map-reduce><br>            <job-tracker>${jobTracker}</job-tracker><br>            <name-node>${nameNode}</name-node><br>            <prepare><br>                <delete path="${nameNode}/user/${wf:user()}/${examplesRoot}/output-data/${outputDir}"><br>            </delete></prepare><br>            <configuration><br>                <property><br>                    <name>mapred.job.queue.name</name><br>                    <value>${queueName}</value><br>                </property><br>                <property><br>                    <name>mapred.mapper.class</name><br>                    <value>org.apache.oozie.example.SampleMapper</value><br>                </property><br>                <property><br>                    <name>mapred.reducer.class</name><br>                    <value>org.apache.oozie.example.SampleReducer</value><br>                </property><br>                <property><br>                    <name>mapred.map.tasks</name><br>                    <value>1</value><br>                </property><br>                <property><br>                    <name>mapred.input.dir</name><br>                    <value>/user/${wf:user()}/${examplesRoot}/input-data/text</value><br>                </property><br>                <property><br>                    <name>mapred.output.dir</name><br>                    <value>/user/${wf:user()}/${examplesRoot}/output-data/${outputDir}</value><br>                </property><br>            </configuration><br>        </map-reduce><br>        <ok to="end"><br>        <error to="fail"><br>    </error></ok></action><br>    <kill name="fail"><br>        <message>Map/Reduce failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message><br>    </kill><br>    <end name="end"><br></end></start></workflow-app><br>执行命令<br>上传example目录到hdfs用户oozie根目录(/user/oozie)下：<br>su - oozie<br>cd /usr/hdp/current/oozie-server/doc<br>hdfs dfs -put example example</p>
<p>启动任务命令：<br>oozie job -oozie <a href="http://localhost:11000/oozie" target="_blank" rel="external">http://localhost:11000/oozie</a> -config examples/apps/map-reduce/job.properties -run</p>
<p>停止任务命令：<br>oozie job -oozie <a href="http://localhost:11000/oozie" target="_blank" rel="external">http://localhost:11000/oozie</a> -kill 0000002-150914143759473-oozie-oozi-W</p>
<p>Oozie Web UI效果图：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_001.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_002.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Oozie_003.JPG" alt="这是一张图片"></p>
<p>参考：<br><a href="http://hortonworks.com/hadoop/oozie/" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/</a><br><a href="http://oozie.apache.org/" target="_blank" rel="external">http://oozie.apache.org/</a><br><a href="http://hortonworks.com/hadoop/oozie/#blog" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/#blog</a><br><a href="http://hortonworks.com/hadoop/oozie/#forums" target="_blank" rel="external">http://hortonworks.com/hadoop/oozie/#forums</a><br><a href="http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/" target="_blank" rel="external">http://hortonworks.com/blog/introducing-availability-of-hdp-2-3-part-3/</a><br><a href="https://github.com/yahoo/oozie" target="_blank" rel="external">https://github.com/yahoo/oozie</a><br>书籍：<br>《Apache Oozie: The Workflow Scheduler for Hadoop》<br><a href="http://book.douban.com/subject/26348732/" target="_blank" rel="external">http://book.douban.com/subject/26348732/</a> </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/Oozie：入门概述/" data-id="ciepsypui002wh4lmqa2s8pdp" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BigData/">BigData</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Oozie/">Oozie</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/大数据技术百度指数201508" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/大数据技术百度指数201508/" class="article-date">
  <time datetime="2015-09-16T06:17:00.000Z" itemprop="datePublished">2015-09-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/大数据技术百度指数201508/">大数据技术百度指数201508</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>关于大数据技术点的搜索指数，这里只关注一下百度指数的结果。</p>
<ol>
<li>当前最热的依次是：Hadoop、Redis、MongoDB、Spark、Storm。可以看到国内Redis、MongoDB的用户很多，有时候比HBase都热，可见热度之高。</li>
<li>从趋势来看，Spark是最强劲的，Hadoop、Redis表现都很不错。</li>
<li>从搜索热词看，当前主要表现在入门介绍、安装、教程的需求量非常大。对于使用中的问题、优化议题还不多，对于监控更少。</li>
<li>当前搜索热词来源Top5的城市：北京、上海、深圳、广州、南京。北京、珠三角、长三角，同时整体上中部IT发展的相对不错。</li>
<li>用户人群的年龄主要是30～39之间，几乎达到50%，其次是20～29，几乎40%，其他年龄段的人很少。</li>
</ol>
<p>首先是意外的收获，就是发现Redis和MongoDB在国内这么火，热词竟然超过了Hadoop了，也许是两个比较简单易用，不像Hadoop如今已经发展成为了一个大家族了。<br>第二个意外是，发现Cloudera、Hortonworks、CHD、HDP尽然不是指数热词，看来一般印象的大数据技术等于Hadoop真的是偏见啊。<br>第三个意外，发现中部城市IT整体发展的不错，除了上海、南京，还包括：成都、重庆、武汉、长沙、西安、郑州。</p>
<p>趋势研究<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_01.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_02.JPG" alt="这是一张图片"></p>
<p>需求图谱<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_03.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_04.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_05.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_06.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_07.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_08.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_09.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_10.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_11.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_12.JPG" alt="这是一张图片"></p>
<p>人群画像<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_13.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_14.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/bigdata_zhishu_baidu_15.JPG" alt="这是一张图片"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/大数据技术百度指数201508/" data-id="ciepsypqz0011h4lmcfy7s6wh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MongoDB/">MongoDB</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Redis/">Redis</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Storm/">Storm</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/大数据动态之201508" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/大数据动态之201508/" class="article-date">
  <time datetime="2015-09-07T08:13:04.000Z" itemprop="datePublished">2015-09-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/大数据动态之201508/">大数据动态之201508</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Cloudera：<br>Cloudera Navigator路线图<br><a href="http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-governance-cloudera-navigator-roadmap/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/whats-next-for-apache-hadoop-data-management-and-governance-cloudera-navigator-roadmap/</a><br>NoSQL性能测试开放标准套件YCSB加入Cloudera实验室项目中<br><a href="http://blog.cloudera.com/blog/2015/08/ycsb-the-open-standard-for-nosql-benchmarking-joins-cloudera-labs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/ycsb-the-open-standard-for-nosql-benchmarking-joins-cloudera-labs/</a><br>Spark在TripAdvisor的机器学习应用案例<br><a href="http://blog.cloudera.com/blog/2015/08/using-apache-spark-for-massively-parallel-nlp-at-tripadvisor/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/using-apache-spark-for-massively-parallel-nlp-at-tripadvisor/</a><br>CDH支持Mesos<br><a href="http://blog.cloudera.com/blog/2015/08/how-to-run-apache-mesos-on-cdh/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/how-to-run-apache-mesos-on-cdh/</a><br>HBase开始支持HBase-Spark模块<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br>Navigator Encrypt开始支持YARN Container安全<br><a href="http://blog.cloudera.com/blog/2015/08/how-to-secure-yarn-containers-with-cloudera-navigator-encrypt/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/how-to-secure-yarn-containers-with-cloudera-navigator-encrypt/</a><br>基于Kafka和HBase的近实时集成架构案例: Santanders<br><a href="http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/inside-santanders-near-real-time-data-ingest-architecture/</a> </p>
<p>Hortonworks:<br>Microsoft Azure Gallery开始支持HDP 2.3<br><a href="http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/" target="_blank" rel="external">http://hortonworks.com/blog/hortonworks-sandbox-with-hdp-2-3-is-now-available-on-microsoft-azure-gallery/</a><br>Microsoft Azure支持Spark<br><a href="http://hortonworks.com/blog/microsoft-and-hortonworks-do-spark-in-the-cloud/" target="_blank" rel="external">http://hortonworks.com/blog/microsoft-and-hortonworks-do-spark-in-the-cloud/</a><br>Storm的容错Nimbus架构<br><a href="http://hortonworks.com/blog/fault-tolerant-nimbus-in-apache-storm/" target="_blank" rel="external">http://hortonworks.com/blog/fault-tolerant-nimbus-in-apache-storm/</a> </p>
<p>MapR<br>Spark Streaming with HBase<br><a href="https://www.mapr.com/blog/spark-streaming-hbase" target="_blank" rel="external">https://www.mapr.com/blog/spark-streaming-hbase</a><br>Apache Drill Architecture: The Ultimate Guide<br><a href="https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide" target="_blank" rel="external">https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide</a><br>HBase架构深度剖析<br><a href="https://www.mapr.com/blog/in-depth-look-hbase-architecture" target="_blank" rel="external">https://www.mapr.com/blog/in-depth-look-hbase-architecture</a><br>HBase Schema设计指导<br><a href="https://www.mapr.com/blog/guidelines-hbase-schema-design" target="_blank" rel="external">https://www.mapr.com/blog/guidelines-hbase-schema-design</a><br>如何利用Spark进行机器学习的并行与交互处理<br><a href="https://www.mapr.com/blog/parallel-and-iterative-processing-machine-learning-recommendations-spark" target="_blank" rel="external">https://www.mapr.com/blog/parallel-and-iterative-processing-machine-learning-recommendations-spark</a></p>
<p>Databricks<br>Spark 1.5发布，包含Tungsten，其利用代码生成技术和Cache感知算法，大幅度提升运行时的性能：<br><a href="https://databricks.com/blog/2015/08/18/spark-1-5-preview-now-available-in-databricks.html" target="_blank" rel="external">https://databricks.com/blog/2015/08/18/spark-1-5-preview-now-available-in-databricks.html</a><br><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" target="_blank" rel="external">https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html</a></p>
<p>mongoDB<br>mongoDB 2.x版本发布了2个，3.x发布了3个：<br><a href="http://blog.mongodb.org/post/128063809158/mongodb-306-rc2-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/128063809158/mongodb-306-rc2-is-released</a><br><a href="http://blog.mongodb.org/post/127802855483/mongodb-317-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/127802855483/mongodb-317-is-released</a><br><a href="http://blog.mongodb.org/post/126436298628/mongodb-2611-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/126436298628/mongodb-2611-is-released</a><br><a href="http://blog.mongodb.org/post/126436227873/mongodb-306-rc0-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/126436227873/mongodb-306-rc0-is-released</a><br><a href="http://blog.mongodb.org/post/125850939688/mongodb-2611-rc0-is-released" target="_blank" rel="external">http://blog.mongodb.org/post/125850939688/mongodb-2611-rc0-is-released</a> </p>
<p>Redis</p>
<p>参考：<br>NoSQL大数据分类<br><a href="http://www.nosql-database.org/" target="_blank" rel="external">http://www.nosql-database.org/</a><br>Autodesk基于Mesos的通用事件系统架构<br><a href="http://www.csdn.net/article/2015-08-27/2825550" target="_blank" rel="external">http://www.csdn.net/article/2015-08-27/2825550</a><br>QingCloud推出Spark即服务<br><a href="http://mt.sohu.com/20150826/n419752360.shtml" target="_blank" rel="external">http://mt.sohu.com/20150826/n419752360.shtml</a><br>Spark大数据分析框架的核心部件<br><a href="http://my.oschina.net/u/2306127/blog/489024?p=1" target="_blank" rel="external">http://my.oschina.net/u/2306127/blog/489024?p=1</a><br>Hadoop和大数据：60款顶级开源工具<br><a href="http://os.51cto.com/art/201508/487936.htm" target="_blank" rel="external">http://os.51cto.com/art/201508/487936.htm</a><br>【微信分享】QingCloud周小四：Spark学习简谈<br><a href="http://www.csdn.net/article/2015-08-07/2825404" target="_blank" rel="external">http://www.csdn.net/article/2015-08-07/2825404</a><br>【微信分享】李滔：搜狐基于Spark的新闻和广告推荐实战<br><a href="http://www.csdn.net/article/2015-07-31/2825353" target="_blank" rel="external">http://www.csdn.net/article/2015-07-31/2825353</a><br>【微信分享】王团结：七牛是如何搞定每天500亿条日志的<br><a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br>对七牛云存储日志处理的思考<br><a href="http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/" target="_blank" rel="external">http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/</a><br>STORM在线业务实践-集群空闲CPU飙高问题排查<br><a href="http://daiwa.ninja/index.php/2015/07/18/storm-cpu-overload/" target="_blank" rel="external">http://daiwa.ninja/index.php/2015/07/18/storm-cpu-overload/</a><br>Spark与Flink：对比与分析<br><a href="http://www.csdn.net/article/2015-07-16/2825232" target="_blank" rel="external">http://www.csdn.net/article/2015-07-16/2825232</a><br>一共81个，开源大数据处理工具汇总（上）<br><a href="http://www.36dsj.com/archives/24852" target="_blank" rel="external">http://www.36dsj.com/archives/24852</a><br>一共81个，开源大数据处理工具汇总（下）<br><a href="http://home.hylanda.com/show_26_11558.html" target="_blank" rel="external">http://home.hylanda.com/show_26_11558.html</a></p>
<p>总结：</p>
<pre><code><span class="bullet">1. </span>Cloudera和Hortonworks都开始注重数据管理和数据治理，Cloudera是通过增强Cloudera Navigator来实现，Hortonworks通过引入Informatic组件Fabric来实现。
<span class="bullet">2. </span>Spark 1.5发布；
<span class="bullet">3. </span>HBase、Cassandra是Column Families/Wide Column Store；
<span class="bullet">4. </span>MongoDB是Document Store；
<span class="bullet">5. </span>Redis是Key Value/Tuple Store；
<span class="bullet">6. </span>Neo4J是Graph Databases；
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/大数据动态之201508/" data-id="ciepsyprp001bh4lmafo1m1cq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BigData/">BigData</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/">HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mongoDB/">mongoDB</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/SparkOnHBase-Cloudera" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/SparkOnHBase-Cloudera/" class="article-date">
  <time datetime="2015-08-18T05:35:51.000Z" itemprop="datePublished">2015-08-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/SparkOnHBase-Cloudera/">SparkOnHBase(Cloudera)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>2014年2月4日，Cloudera宣布CDH支持Spark，在CDH 4.4中引入Spark 0.9。<br><a href="http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/" target="_blank" rel="external">http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/</a><br>在引入的时候强调了三点：</p>
<pre><code><span class="bullet">1. </span>Machine Learning
<span class="bullet">2. </span>Spark Streaming
<span class="bullet">3. </span>Faster Batch
</code></pre><p>2014年7月，在github上创建了Apache HBase与Spark的集成项目SparkOnHBase<br><a href="http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/" target="_blank" rel="external">http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/</a><br><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="external">https://github.com/cloudera-labs/SparkOnHBase</a><br>当前SparkOnHBase主要集中在这几个方面的功能改进：</p>
<pre><code>1. 在MR的map或者reduce阶段对HBase的全量访问(Full Access)；
2. 支持bulk <span class="operator"><span class="keyword">load</span>；
<span class="number">3.</span> 支持<span class="keyword">get</span>, put, <span class="keyword">delete</span>等bulk操作(bulk operation)；
<span class="number">4.</span> 支持成为<span class="keyword">SQL</span> <span class="keyword">engines</span>。</span>
</code></pre><p>2015年8月SparkOnHBase项目有了里程碑似的进展，被提交到HBase的主干(trunk)上，模块名为HBase-Spark Module，HBASE-13992 。<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br><a href="https://issues.apache.org/jira/browse/HBASE-13992" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-13992</a><br>HBase-Spark module相比于SparkOnHBase在架构上没有什么变化：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Cloudera_Spark_2015_01.png" alt="这是一张图片"><br>在具体实现上当前有三点改进：</p>
<pre><code><span class="bullet">1. </span>使用了全新的HBase 1.0+的API；
<span class="bullet">2. </span>从RDD和DStream functions操作HBase的直接支持；
<span class="bullet">3. </span>简化 foreach 和 map functions；
</code></pre><p>计划工作有两项：</p>
<pre><code><span class="bullet">1. </span>Spark-HBase Module支持bulkload；
<span class="bullet">2. </span>Spark-HBase Module支持Spark DataFrame DataSource；
</code></pre><p><a href="https://issues.apache.org/jira/browse/HBASE-14150" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-14150</a><br><a href="https://issues.apache.org/jira/browse/HBASE-14181" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-14181</a> </p>
<p>实际上集成Spark作为计算引擎的项目还有Hive和Pig：<br><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/spark.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/spark.html</a><br><a href="http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/02/download-the-hive-on-spark-beta/</a><br><a href="http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/" target="_blank" rel="external">http://blog.cloudera.com/blog/2014/09/pig-is-flying-apache-pig-on-apache-spark/</a> </p>
<p>参考：<br><a href="http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/</a><br><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="external">https://github.com/cloudera-labs/SparkOnHBase</a><br><a href="http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/SparkOnHBase-Cloudera/" data-id="ciepsypu1002ph4lmplhynt4b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BigData/">BigData</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cloudera/">Cloudera</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/">HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/学习《Hadoop生态技术在阿里全网商品搜索实战》" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/学习《Hadoop生态技术在阿里全网商品搜索实战》/" class="article-date">
  <time datetime="2015-08-17T04:41:00.000Z" itemprop="datePublished">2015-08-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/学习《Hadoop生态技术在阿里全网商品搜索实战》/">学习《Hadoop生态技术在阿里全网商品搜索实战》</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>资料参见文档：<a href="http://wenku.it168.com/d_001428550.shtml" target="_blank" rel="external">http://wenku.it168.com/d_001428550.shtml</a><br>版本:</p>
<pre><code><span class="bullet">1. </span>Hadoop: 基于 Hadoop 2.2 的阿里定制版
<span class="bullet">2. </span>HBase: 基于 HBase 0.94 的阿里定制版
</code></pre><p>部署方式：</p>
<pre><code><span class="bullet">1. </span>服务总数近1000台，分2个集群；
<span class="bullet">2. </span>Hadoop/HBase共同部署；
</code></pre><p>分析：服务器数量可能是2014年初的数据；HBase部署方式可能是RS和DN部署在同一个节点上。<br>服务器配置：</p>
<pre><code><span class="bullet">1. </span>CPU：24/32 Cores
<span class="bullet">2. </span>Memory：48G/96G
<span class="bullet">3. </span>Disk：12 <span class="bullet">* 1T SATA Disk 或者 12 *</span> 2T SATA Disk
</code></pre><p>分析：服务器配置计算能力比较强，内存和磁盘配置都不是很高。<br>大数据组件：</p>
<pre><code><span class="bullet">1. </span>HDFS + YARN
<span class="bullet">2. </span>HBase
<span class="bullet">3. </span>MR
<span class="bullet">4. </span>iStream
<span class="bullet">5. </span>Spark
<span class="bullet">6. </span>HQueue
<span class="bullet">7. </span>Phoenix
<span class="bullet">8. </span>OpenTSDB
<span class="bullet">9. </span>Zookeeper
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_01.JPG" alt="这是一张图片"><br>分析：</p>
<pre><code>* 对于基于HBase的HQueue是一个创新，当前没有看到更多的资料，无法和Kafka对比。(在性能和TPC上可能Kafka更强大，但通过对HBase的复用做出Queue，很赞。)
* iStream是一个Steaming <span class="function_start"><span class="keyword">on</span></span> YARN的产品，从架构上看很类似storm的设计理念。
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_02.JPG" alt="这是一张图片"><br>HBase<br>HBase应用</p>
<pre><code><span class="number">1</span>. <span class="function"><span class="title">Phoenix</span><span class="params">(SQL on HBase)</span></span>
<span class="number">2</span>. <span class="function"><span class="title">OpenTSDB</span><span class="params">(Metrics on HBase)</span></span>
<span class="number">3</span>. <span class="function"><span class="title">HQueue</span><span class="params">(Queue on HBase)</span></span>
</code></pre><p>分析：</p>
<pre><code><span class="bullet">* </span>在HBase集群上运行了Phoenix、OpenTSDB、HQueue三种应用，因此HBase具有作为一种数据存储的基础设施的能力。
</code></pre><p>HBase网页库存储方案</p>
<pre><code>1. 版本从0<span class="class">.25</span>、0<span class="class">.26</span>、0<span class="class">.90</span>、0<span class="class">.92</span>、0<span class="class">.94</span>、0<span class="class">.98</span>逐步升级的。
2. <span class="tag">HBase</span>集群规模从30多台持续升级到300多台。
3. <span class="tag">HBase</span> <span class="tag">Region</span>个数从 1<span class="tag">K</span> 增长到 20<span class="tag">K</span>。
4. 网页数量从 十亿 增长到 百亿。
</code></pre><p>存储业务数据的CF如下：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_05.JPG" alt="这是一张图片"><br>在HBase/Hadoop的I/O上的优化如下：</p>
<pre><code><span class="bullet">1. </span>Compression：Snappy/Gzip
<span class="bullet">2. </span>Block Encoding：Diff
<span class="bullet">3. </span>Block Size：64KB - 1MB
<span class="bullet">4. </span>Block Cache：InMemory
<span class="bullet">5. </span>Bloom Filter：ROW
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_06.JPG" alt="这是一张图片"><br>HBase Coprocessor应用<br>在网页库中使用了三种Coprocessor：</p>
<pre><code><span class="bullet">1. </span>Trace Coprocessor
<span class="bullet">2. </span>Clone Coprocessor
<span class="bullet">3. </span>Incremental Coprocessor
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_07.JPG" alt="这是一张图片"><br>分析：</p>
<pre><code><span class="keyword">*</span> 如果HBase集群就是两个集群中的一个，那么裸存储容量最大为：12 <span class="keyword">*</span> 2T <span class="keyword">*</span> 300 = 7200T = 7.2P，如果考虑到压缩、复制因子、数据冗余、容量冗余，可以存储有效数据约为：8P 数据。 
<span class="keyword">*</span> 平均每台服务器运行的Region个数：20K/300 = 67 个，这个数字比较符合HBase官方推荐的值。
<span class="keyword">*</span> Compression方法用了snappy和gzip两种。CF访问频繁，使用snappy，速度快；Raw CF访问较少，使用gzip，压缩比高。
<span class="keyword">*</span> Block Encoding使用Diff，0.98后改用PrefixTree；
<span class="keyword">*</span> Block Size的大小为 64KB - 1MB 
</code></pre><p>实时处理架构<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_08.JPG" alt="这是一张图片"><br>分析</p>
<pre><code><span class="subst">*</span> Metrics实时采集的流程大约是：HBase <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iStream <span class="subst">-&gt; </span>OpenTSDB <span class="keyword">on</span> HBase
<span class="subst">*</span> 流处理的全流程：HBase <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iStream <span class="subst">-&gt; </span>HQueue <span class="subst">-&gt; </span>iSearch/iStream
<span class="subst">*</span> 参见前文分析，猜测iStream是一个类似Storm的YARN框架。
</code></pre><p>关于阿里搜索自研的iStream的架构与文档参加如下：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_09.JPG" alt="这是一张图片"><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Ali_Search_2015_10.JPG" alt="这是一张图片"></p>
<p><a href="http://www.infoq.com/cn/news/2014/09/hadoop-alibaba-yarn" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/09/hadoop-alibaba-yarn</a><br><a href="http://club.alibabatech.org/resource_detail.htm?topicId=140" target="_blank" rel="external">http://club.alibabatech.org/resource_detail.htm?topicId=140</a> </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/学习《Hadoop生态技术在阿里全网商品搜索实战》/" data-id="ciepsypqk000rh4lmkhzeekv6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BigData/">BigData</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/">HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HQueue/">HQueue</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/iStream/">iStream</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/Hadoop发行版(2015第二季)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/Hadoop发行版(2015第二季)/" class="article-date">
  <time datetime="2015-08-11T14:40:02.000Z" itemprop="datePublished">2015-08-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/Hadoop发行版(2015第二季)/">Hadoop发行版(2015第二季)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>自从Hadoop的出现，引领大数据的浪潮越来越热。大数据存储的主要技术路线有几种：<br>1.Hadoop<br>2.Cassandra<br>3.MongoDB<br>Hadoop是Apache的开源项目，同时有很多商业公司对Hadoop进行版本发行和商业支持,参见：<a href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" target="_blank" rel="external">http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support</a><br>其中在最有名为人所知的三家：<br>1.Cloudera<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_1.JPG" alt="这是一张图片"><br>2.Hortonwork<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_2.JPG" alt="这是一张图片"><br>3.MapR<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_3.JPG" alt="这是一张图片"><br>这三个厂商之中，MapR最为封闭；Hortonworks最为开放，产品线全开源，在线文档比较丰富。国内使用Cloudera CDH和Hortonworks的应该是最多的。<br>国内市场当前有两家也非常有竞争力，一家是Huawei，一家是星环科技。<br>4.Huawei FusionInsight<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_7.JPG" alt="这是一张图片"><br>5.星环科技TDH，TDH对Spark的支持据说非常不错的，有良好的性能表现。<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_6.JPG" alt="这是一张图片"><br>准实时计算框架/即席查询<br>1.CDH的框架有：Impala + Spark；<br>2.HDP的框架有：Tez + Spark；<br>3.MapR的框架有：Drill + Tez + Spark。<br>关于Spark：<br>2014年大数据最热门的技术路线就是算是Spark了，而且得力于Spark不遗余力的推广和快速成长。Cloudera是最早支持Spark，也是最激进的。下图即是Spark在Cloudera产品线中的定位：<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/Hadoop_2015_4.JPG" alt="这是一张图片"><br>实际上基于Hadoop的快速计算框架的发展才刚刚开始，社区中已经有如下几种：<br>1.Spark/Shark<br>2.Hortonworks Tez/Stinger<br>3.Cloudera Impala<br>4.Apache Drill<br>5.Apache Flink<br>6.Apache Nifi<br>7.Facebook Presto</p>
<p>SQL on Hadoop<br>SQL on Hadoop的发展主要是传统的SQL过于强大，人才库非常庞大，从Hadoop出现的第一天就在SQL发力。当前技术路线上更是百花齐放，这里从开源和商业产品来说。<br>Open Source</p>
<pre><code><span class="bullet">1. </span>Apache Hive(Hive on MR)
<span class="bullet">2. </span>Hortonworks Tez/Stinger(Hive on Tez)
<span class="bullet">3. </span>Cloudera Impala
<span class="bullet">4. </span>Shark
<span class="bullet">5. </span>Spark SQL
<span class="bullet">6. </span>Apache Drill - MapR
<span class="bullet">7. </span>Facebook Presto
<span class="bullet">8. </span>Apache Phoenix(on HBase) - Saleforce
<span class="bullet">9. </span>Apache Kylin
<span class="bullet">10. </span>Apache Tajo - (Database Lab, Korea University)
<span class="bullet">11. </span>Cascading Lingual - (Cascading, Optiq)
<span class="bullet">12. </span>Dato (GraphLab) - Dato
</code></pre><p>Commercial</p>
<pre><code><span class="bullet">1. </span>EMC HAWQ
<span class="bullet">2. </span>IBM BigSQL
<span class="bullet">3. </span>TERADATA SQL-H
<span class="bullet">4. </span>Hadapt/HadoopDB
<span class="bullet">5. </span>Transwarp Inceptor
</code></pre><p>在开源领域里面，当前比受追捧的主要是：Hive、Impala、Spark、Phoenix。</p>
<p>参考：<br>SQL on Hadoop开源项目总结<br><a href="http://segmentfault.com/a/1190000002799235" target="_blank" rel="external">http://segmentfault.com/a/1190000002799235</a><br>如何选择满足需求的SQL on Hadoop系统<br><a href="http://www.searchbi.com.cn/showcontent_89816.htm" target="_blank" rel="external">http://www.searchbi.com.cn/showcontent_89816.htm</a><br>2015Hadoop技术峰会演讲速记3： 基于Transwarp Stream和Discover的实时大数据人流密度估计<br><a href="http://www.transwarp.cn/news/detail?id=70" target="_blank" rel="external">http://www.transwarp.cn/news/detail?id=70</a> </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/Hadoop发行版(2015第二季)/" data-id="ciepsypuq0032h4lmcj9kn9n6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BigData/">BigData</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CDH/">CDH</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDP/">HDP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SQL-on-Hadoop/">SQL on Hadoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/学习《七牛是如何搞定每天500亿条日志的》" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/学习《七牛是如何搞定每天500亿条日志的》/" class="article-date">
  <time datetime="2015-08-10T06:48:00.000Z" itemprop="datePublished">2015-08-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/学习《七牛是如何搞定每天500亿条日志的》/">学习《七牛是如何搞定每天500亿条日志的》</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>七牛是如何搞定每天500亿条日志的 <a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/qiniu_01.jpg" alt=""><br>日志处理的大致分为三步：</p>
<pre><code><span class="bullet">1. </span>日志采集，主要是通过Agent和Flume；
<span class="bullet">2. </span>日志流转，主要是通过Kafka；
<span class="bullet">3. </span>日志计算，主要是通过Spark Streaming作为计算引擎；
</code></pre><p>大致的处理流程：</p>
<pre><code><span class="number">1.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>HDFS <span class="subst">-&gt; </span>mongoDB
<span class="number">2.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>Spark <span class="subst">-&gt; </span>mongoDB
<span class="number">3.</span> Agent/<span class="built_in">Local</span> Kafka <span class="subst">-&gt; </span>Flume <span class="subst">-&gt; </span>Kafka <span class="subst">-&gt; </span>Spark <span class="subst">-&gt; </span>opentsdb 
</code></pre><p>流程3只是见于图上，文字上没有任何提到。<br>在日志采集中，通过Agent将业务应用和日志采集进行了分离，采取了Agent主动来拉的模式。专门强调了Agent 的设计需求：<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">每台机器上会有一个Agent去同步这些日志，这是个典型的队列模型，业务进程在不断的push，Agent在不停的pop。Agent需要有记忆功能，用来保存同步的位置(<span class="command">offset</span>)，这样才尽可能保证数据准确性，但不可能做到完全准确。由于发送数据和保存<span class="command">offset</span>是两个动作，不具有事务性，不可避免的会出现数据不一致性情况，通常是发送成功后保存<span class="command">offset</span>，那么在Agent异常退出或机器断电时可能会造成多余的数据。</span><br><span class="line">在这里，Agent需要足够轻，这主要体现在运维和逻辑两个方面。Agent在每台机器上都会部署，运维成本、接入成本是需要考虑的。Agent不应该有解析日志、过滤、统计等动作，这些逻辑应该给数据消费者。倘若Agent有较多的逻辑，那它是不可完成的，不可避免的经常会有升级变更动作。</span><br></pre></td></tr></table></figure></p>
<p>为什么Agent没有直接将日志发送给Kafka，而是通过Flume来做：<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">具体架构上，Agent并没把数据直接发送到Kafka，在Kafka前面有层由Flume构成的<span class="keyword">forward</span>。这样做有两个原因：</span><br><span class="line"><span class="number">1</span>. Kafka的API对非JVM系的语言支持很不友好，<span class="keyword">forward</span>对外提供更加通用的http接口。</span><br><span class="line"><span class="number">2</span>. <span class="keyword">forward</span>层可以做路由、Kafka topic和Kafka partition key等逻辑，进一步减少Agent端的逻辑。</span><br></pre></td></tr></table></figure></p>
<p>Kafka使用建议<br>1.Topic划分。尽量通过划分Topic分离不同类型的数据；<br>2.Kafka partition数目直接关系整体的吞吐量。3个Partition能够跑满一块磁盘的IO。<br>3.Partition key设计。partition key选择不当，可能会造成数据倾斜。在对数据有顺序性要求才需使用partition key。Kafka的producer sdk在没指定partition key时，在一定时间内只会往一个partition写数据，这种情况下当producer数少于partition数也会造成数据倾斜，可以提高producer数目来解决这个问题。<br>实时计算Spark Streaming<br>1.当前Spark只用作统计，没有进行迭代计算(DAG)。场景比较简单。<br>2.Spark Streaming从Kafka中读数据，统计完结果如mongoDB。可以理解是Spark Streaming + mongoDB的应用。<br>3.Spark Streaming对存储计算结果的数据库tps要求较高。比如有10万个域名需要统计流量，batch interval为10s，每个域名有4个相关统计项，算下来平均是4万 tps，考虑到峰值可能更高，固态硬盘上的mongo也只能抗1万tps，后续我们会考虑用redis来抗这么高的tps。难道Redis能够支持很高的TPS？<br>4.有状态的Task的挑战：有外部状态的task逻辑上不可重入的，当开启speculation参数时候，可能会造成计算的结果不准确。说个简单的例子。这个任务，如果被重做了，会造成落入mongo的结果比实际多。有状态的对象生命周期不好管理，这种对象不可能做到每个task都去new一个。我们的策略是一个JVM内一个对象，同时在代码层面做好并发控制。<br>七牛数据平台规模<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">线上的规模：Flume ＋ Kafka ＋ Spark8台高配机器，日均500亿条数据，峰值80万tps。</span><br></pre></td></tr></table></figure></p>
<p>因此，<br>1.如果是Flume/Kafka/Spark共享同一个物理集群，硬件压力如何？<br>2.如果每条日志 0.1K，那么每天总数据量 50G <em> 0.1K = 5T，每个节点每秒 5T/24</em>3600/8 = 7.23M。 </p>
<p>参考：<br>【微信分享】王团结：七牛是如何搞定每天500亿条日志的<br><a href="http://www.csdn.net/article/2015-07-30/2825342" target="_blank" rel="external">http://www.csdn.net/article/2015-07-30/2825342</a><br>对七牛云存储日志处理的思考<br><a href="http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/" target="_blank" rel="external">http://hadoop1989.com/2015/08/02/Think-QiNiu-Cloud/</a> </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/学习《七牛是如何搞定每天500亿条日志的》/" data-id="ciepsypqb000kh4lmqdkmh6wx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BigData/">BigData</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flume/">Flume</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/学习《腾讯在Spark上的应用与实践优化》" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/学习《腾讯在Spark上的应用与实践优化》/" class="article-date">
  <time datetime="2015-08-07T08:28:42.000Z" itemprop="datePublished">2015-08-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/学习《腾讯在Spark上的应用与实践优化》/">学习《腾讯在Spark上的应用与实践优化》</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>《腾讯在Spark上的应用与实践优化》原文参见：<a href="http://download.csdn.net/detail/happytofly/8637461" target="_blank" rel="external">http://download.csdn.net/detail/happytofly/8637461</a></p>
<p>TDW: Tencent Distributed Data Warehouse，腾讯分布式数据仓库；<br>GAIA：腾讯自研的基于YARN定制化和优化的资源管理系统；<br>Lhoste：腾讯自研的作业的工作流调度系统，类似于Oozie；<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_1.JPG" alt=""></p>
<p>TDW集群规模：</p>
<pre><code><span class="bullet">1. </span>Gaia集群节点数：8000+；
<span class="bullet">2. </span>HDFS的存储空间：150PB+；
<span class="bullet">3. </span>每天新增数据：1PB+；
<span class="bullet">4. </span>每天任务数：1M+；
<span class="bullet">5. </span>每天计算量：10PB+；
</code></pre><p>Spark集群：</p>
<pre><code><span class="bullet">1. </span>Spark部署在Gaia之上，即是Spark on YARN模式，每个节点是 24 cores 和 60G 内存；
<span class="bullet">2. </span>底层存储包括：HDFS、HBase、Hive、MySQL；
<span class="bullet">3. </span>作业类型，包括：ETL、SparkSQL、Machine Learning、Graph Compute、Streaming；
<span class="bullet">4. </span>每天任务数，10K+；
<span class="bullet">5. </span>腾讯从2013年开始引入Spark 0.6，已经使用2年了；
</code></pre><p>Spark的典型应用：</p>
<pre><code><span class="bullet">1. </span>预测用户的广告点击概率；
<span class="bullet">2. </span>计算两个好友间的共同好友数；
<span class="bullet">3. </span>用于ETL的SparkSQL和DAG任务；
</code></pre><p>Case 1: 预测用户的广告点击概率<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_4.JPG" alt=""></p>
<pre><code><span class="number">1</span>. 数据是通过<span class="function"><span class="title">DCT</span><span class="params">(Data Collect Tool)</span></span>推送到HDFS上，然后Spark直接将HDFS数据导入到 RDD&amp;Cache；
<span class="number">2</span>. <span class="number">60</span>次迭代计算的时间为<span class="number">10</span>～<span class="number">15</span>分钟，即每次迭代<span class="number">10</span>～<span class="number">15</span>秒；
</code></pre><p>Case 2: 计算两个好友间的共同好友数</p>
<pre><code>1. 根据shuffle数量来确定partition数量；
2. 尽量使用sort-based shuffle，减少reduce的内存使用；
3. 当连接超时后选择重试来减少executor丢失的概率；
4. 避免executor被YARN给<span class="operator"><span class="keyword">kill</span>掉，设置 spark.yarn.executor.memoryoverhead
<span class="number">5.</span> 执行语句 <span class="keyword">INSERT</span> <span class="keyword">TABLE</span> test_result <span class="keyword">SELECT</span> t3.d, <span class="keyword">COUNT</span>(*) FROＭ( <span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> a, b <span class="keyword">FROM</span> join_1 ) t1 <span class="keyword">JOIN</span> （<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> b, c <span class="keyword">FROM</span> join_2 ) t2 <span class="keyword">ON</span> (t1.a = t2.c) <span class="keyword">JOIN</span> (<span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c, d <span class="keyword">FROM</span> c, d <span class="keyword">FROM</span> join_3 ) t3 <span class="keyword">ON</span> (t2.b = t3.d) <span class="keyword">GROUP</span> <span class="keyword">BY</span> t3.d 使用Hive需要<span class="number">30</span>分钟，使用SparkSQL需要<span class="number">5</span>分钟；
<span class="number">6.</span> 当有小表时使用broadcase <span class="keyword">join</span>代替Common <span class="keyword">join</span>；
<span class="number">7.</span> 尽量使用ReduceByKey代替GroupByKey；
<span class="number">8.</span> 设置spark.serializer = org.apache.spark.serializer.KryoSerializer；
<span class="number">9.</span> 使用YARN时，设置spark.shuffle.service.enabled = <span class="literal">true</span>；
<span class="number">10.</span> 在早期版本中Spark通过启动参数固定executor的数量，当前支持动态资源扩缩容特性

    * spark.dynamicAllocation.enabled = <span class="literal">true</span>
    * spark.dynamicAllocation.executorIdleTimeout = <span class="number">120</span>
    * spark.dynamicAllocation.schedulerBacklogTimeout = <span class="number">10</span>
    * spark.dynamicAllocation.minExecutors/maxExecutors

<span class="number">11.</span> 当申请固定的executors时且task数大于executor数时，存在着资源的空闲状态。</span>
</code></pre><p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/TDW_SPARK_5.JPG" alt=""><br>&lt;完&gt;</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/学习《腾讯在Spark上的应用与实践优化》/" data-id="ciepsypq3000ch4lmnf9vn4tb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BigData/">BigData</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/13-14年收集的大数据的一些技术架构图" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/13-14年收集的大数据的一些技术架构图/" class="article-date">
  <time datetime="2015-08-05T05:16:53.000Z" itemprop="datePublished">2015-08-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/13-14年收集的大数据的一些技术架构图/">13~14年收集的大数据的一些技术架构图</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1 Big Data Solution</p>
<p>1.1 HP</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_01.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_02.png" alt=""></p>
<p>1.2 Oracle</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_03.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_04.JPG" alt=""></p>
<p>1.3 IBM</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_05.jpg" alt=""></p>
<p>1.4 Microsoft</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_06.png" alt=""></p>
<p>1.5 Huawei</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_07.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_08.jpg" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_09.jpg" alt=""></p>
<p>2 Big Data on Cloud</p>
<p>2.1 Amazon AWS</p>
<p>2.1.1 Netflix BigData on AWS<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_10.png" alt=""></p>
<p>2.2 Microsoft Azure</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_11.png" alt=""></p>
<p>2.3 Facebook</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_12.png" alt=""></p>
<p>2.4 Linkedin</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_13.png" alt=""></p>
<p>2.5 Twitter</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_14.png" alt=""></p>
<p>2.6 Alibaba/Taobao</p>
<p>2.6.1 淘宝数据魔方<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_15.png" alt=""></p>
<p>2.6.2 阿里大数据应用平台<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_16.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_17.png" alt=""></p>
<p>2.6.3 阿里搜索实时流计算<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_18.png" alt=""></p>
<p>2.7 Tencent</p>
<p>2.7.1 腾讯大规模Hadoop集群TDW<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_19.png" alt=""></p>
<p>2.7.2 腾讯实时计算平台 广点通<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_20.png" alt=""></p>
<p>2.8 JD(京东)</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_21.png" alt=""></p>
<p>2.9 CMCC(中国移动)</p>
<p>2.9.1 大云PaaS 2.5<br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_22.png" alt=""></p>
<p>3 Hadoop Distribution</p>
<p>3.1 Apache Hadoop</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_23.png" alt=""></p>
<p>3.2 Cloudera</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_24.png" alt=""></p>
<p>3.3 Hortonworks</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_25.png" alt=""></p>
<p>3.4 MapR</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_26.png" alt=""></p>
<p>3.5 Intel</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_27.png" alt=""></p>
<p>3.6 EMC Pivotal HD</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_28.jpg" alt=""></p>
<p>3.7 IBM</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_29.jpg" alt=""></p>
<p>3.8 Huawei</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_30.jpg" alt=""></p>
<p>4 Landscape</p>
<p><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_31.jpg" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_32.png" alt=""><br><img src="https://raw.githubusercontent.com/stevenxu/tuku/master/navigating.github.io/2015/H13_33.png" alt=""></p>
<p>5 参考</p>
<ul>
<li><a href="http://www-01.ibm.com/software/data/bigdata/platform/resources.html" target="_blank" rel="external">IBM Report</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1E7OTT7&amp;ct=130225&amp;st=sb" target="_blank" rel="external">Gartner - Hadoop Is Not a Data Integration Solution</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1DBWMQY&amp;ct=121220&amp;st=sb" target="_blank" rel="external">Gartner - Magic Quadrant for Data Masking Technology 2012</a></li>
<li><a href="http://www.gartner.com/technology/reprints.do?id=1-1IMDMZ5&amp;ct=130819&amp;st=sb" target="_blank" rel="external"> Magic Quadrant for Cloud Infrastructure as a Service 2013</a></li>
<li><a href="http://wenku.it168.com/d_000048434.shtml" target="_blank" rel="external">Facebook Hadoop</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/13-14年收集的大数据的一些技术架构图/" data-id="ciepsypvs003kh4lm2vscbab9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BigData/">BigData</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CDH/">CDH</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDP/">HDP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapR/">MapR</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2015/读《微软研发制胜策略》" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/读《微软研发制胜策略》/" class="article-date">
  <time datetime="2015-08-04T14:30:45.000Z" itemprop="datePublished">2015-08-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/读书/">读书</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/读《微软研发制胜策略》/">读《微软研发制胜策略》</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>软件开发的核心就是：达成项目目标，提高生产率，提高软件的质量。除此之外，都不要重要。<br>管理上、复用上，一切的核心就是人的问题，提高人的能力是第一生产力。</p>
<p>1.项目中一个现象就是紧紧的去控制进度，调整进度，进度的跟踪只是一种日常的事务工作。<br>2.观念的改变是第一位的，什么是观念改变的原则：规则不是法律，是可以触碰的。什么是我们要改变的规则，就是要有主动、计划、灵活。<br>3.紧密的进度计划，是一般的管理人员的通常做法，他的好处就是看到不断的工作，会有不断的压力；如果运用不当，就可能让人觉得厌烦和沮丧。<br>4.为了日程进度，牺牲质量往往是不值得的，除非你要一笑而过的做法。再不管这个项目的后续开发和维护了。对于产品或者项目的期限，要谨慎，要反思可能为了进度而牺牲质量。这叫着草率的期限。<br>5.一个好的日程表会兼顾公司和员工的利益的。<br>6.没有期限的目标不过是梦想而已。<br>7.把一个大项目，切分成n个小项目来做，每一个项目的周期大约是2个月。叫着阶段式的日程控制法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://navigating.github.io/2015/读《微软研发制胜策略》/" data-id="ciepsypph0003h4lmtxk76a33" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a><span class="category-list-count">70</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/生活/">生活</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书/">读书</a><span class="category-list-count">25</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Ambari/" style="font-size: 10px;">Ambari</a> <a href="/tags/Android/" style="font-size: 15px;">Android</a> <a href="/tags/Annotation/" style="font-size: 10px;">Annotation</a> <a href="/tags/Apple/" style="font-size: 10px;">Apple</a> <a href="/tags/Architecture/" style="font-size: 15px;">Architecture</a> <a href="/tags/BigData/" style="font-size: 20px;">BigData</a> <a href="/tags/CDH/" style="font-size: 16.25px;">CDH</a> <a href="/tags/Cassandra/" style="font-size: 10px;">Cassandra</a> <a href="/tags/Cloudera/" style="font-size: 10px;">Cloudera</a> <a href="/tags/Eclipse/" style="font-size: 10px;">Eclipse</a> <a href="/tags/Flume/" style="font-size: 10px;">Flume</a> <a href="/tags/Google/" style="font-size: 11.25px;">Google</a> <a href="/tags/HBase/" style="font-size: 13.75px;">HBase</a> <a href="/tags/HDP/" style="font-size: 16.25px;">HDP</a> <a href="/tags/HQueue/" style="font-size: 10px;">HQueue</a> <a href="/tags/Hadoop/" style="font-size: 20px;">Hadoop</a> <a href="/tags/Hive/" style="font-size: 10px;">Hive</a> <a href="/tags/JNI/" style="font-size: 10px;">JNI</a> <a href="/tags/JUnit/" style="font-size: 11.25px;">JUnit</a> <a href="/tags/Java/" style="font-size: 18.75px;">Java</a> <a href="/tags/Kafka/" style="font-size: 11.25px;">Kafka</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/MapR/" style="font-size: 10px;">MapR</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/NIO/" style="font-size: 12.5px;">NIO</a> <a href="/tags/Netty/" style="font-size: 11.25px;">Netty</a> <a href="/tags/NoSQL/" style="font-size: 10px;">NoSQL</a> <a href="/tags/Oozie/" style="font-size: 10px;">Oozie</a> <a href="/tags/Performance/" style="font-size: 10px;">Performance</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Review/" style="font-size: 10px;">Review</a> <a href="/tags/Ruby/" style="font-size: 10px;">Ruby</a> <a href="/tags/SOA/" style="font-size: 10px;">SOA</a> <a href="/tags/SQL-on-Hadoop/" style="font-size: 11.25px;">SQL on Hadoop</a> <a href="/tags/Spark/" style="font-size: 17.5px;">Spark</a> <a href="/tags/Storm/" style="font-size: 10px;">Storm</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/XP/" style="font-size: 11.25px;">XP</a> <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/github/" style="font-size: 10px;">github</a> <a href="/tags/hexo/" style="font-size: 11.25px;">hexo</a> <a href="/tags/iOS/" style="font-size: 10px;">iOS</a> <a href="/tags/iStream/" style="font-size: 10px;">iStream</a> <a href="/tags/mongoDB/" style="font-size: 11.25px;">mongoDB</a>
    </div>
  </div>

  
    <div class="widget-wrap">
  <input type="text" class="st-default-search-input">
</div>
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/Oozie：入门概述/">Oozie：入门概述</a>
          </li>
        
          <li>
            <a href="/2015/大数据技术百度指数201508/">大数据技术百度指数201508</a>
          </li>
        
          <li>
            <a href="/2015/大数据动态之201508/">大数据动态之201508</a>
          </li>
        
          <li>
            <a href="/2015/SparkOnHBase-Cloudera/">SparkOnHBase(Cloudera)</a>
          </li>
        
          <li>
            <a href="/2015/学习《Hadoop生态技术在阿里全网商品搜索实战》/">学习《Hadoop生态技术在阿里全网商品搜索实战》</a>
          </li>
        
          <li>
            <a href="/2015/Hadoop发行版(2015第二季)/">Hadoop发行版(2015第二季)</a>
          </li>
        
          <li>
            <a href="/2015/学习《七牛是如何搞定每天500亿条日志的》/">学习《七牛是如何搞定每天500亿条日志的》</a>
          </li>
        
          <li>
            <a href="/2015/学习《腾讯在Spark上的应用与实践优化》/">学习《腾讯在Spark上的应用与实践优化》</a>
          </li>
        
          <li>
            <a href="/2015/13-14年收集的大数据的一些技术架构图/">13~14年收集的大数据的一些技术架构图</a>
          </li>
        
          <li>
            <a href="/2015/读《微软研发制胜策略》/">读《微软研发制胜策略》</a>
          </li>
        
          <li>
            <a href="/2015/大数据动态之201507/">大数据动态之201507</a>
          </li>
        
          <li>
            <a href="/2015/使用Hexo搭建Github静态博客/">使用Hexo搭建Github静态博客</a>
          </li>
        
          <li>
            <a href="/2015/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2015/Hadoop-2-7-1-发布/">Hadoop 2.7.1 发布</a>
          </li>
        
          <li>
            <a href="/2015/读《Deploying-Apache-Kafka-A-Practical-FAQ》/">读《Deploying Apache Kafka: A Practical FAQ》</a>
          </li>
        
          <li>
            <a href="/2015/大数据动态之201506/">大数据动态之201506</a>
          </li>
        
          <li>
            <a href="/2015/大数据动态之201505/">大数据动态之201505</a>
          </li>
        
          <li>
            <a href="/2015/大数据动态之201502/">大数据动态之201502</a>
          </li>
        
          <li>
            <a href="/2015/Hadoop发行版(2015第一季)/">Hadoop发行版(2015第一季)</a>
          </li>
        
          <li>
            <a href="/2013/2013年年终总结/">2013年年终总结</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">九月 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">八月 2015</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">七月 2015</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">六月 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">五月 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">三月 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">一月 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/12/">十二月 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/08/">八月 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2012/12/">十二月 2012</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/11/">十一月 2011</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/10/">十月 2011</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2011/03/">三月 2011</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/11/">十一月 2010</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/09/">九月 2010</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/08/">八月 2010</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/06/">六月 2010</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/05/">五月 2010</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/04/">四月 2010</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/03/">三月 2010</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2010/01/">一月 2010</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2009/12/">十二月 2009</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2009/10/">十月 2009</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2008/04/">四月 2008</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2008/03/">三月 2008</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2007/11/">十一月 2007</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2007/08/">八月 2007</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2007/07/">七月 2007</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2007/06/">六月 2007</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2007/05/">五月 2007</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2007/03/">三月 2007</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/12/">十二月 2006</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/11/">十一月 2006</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/10/">十月 2006</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/08/">八月 2006</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2006/04/">四月 2006</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2005/09/">九月 2005</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2005/08/">八月 2005</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2005/05/">五月 2005</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2005/02/">二月 2005</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2005/01/">一月 2005</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2004/12/">十二月 2004</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2004/11/">十一月 2004</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2004/10/">十月 2004</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2004/09/">九月 2004</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Steven Xu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/sitemap.xml" class="mobile-nav-link">Sitemap</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>